{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading functions\n",
    "import os\n",
    "import time\n",
    "from monai.data import DataLoader, decollate_batch\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "\n",
    "from src.get_data import CustomDataset, CustomDatasetSeg\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from types import SimpleNamespace\n",
    "import wandb\n",
    "import logging\n",
    "\n",
    "#####\n",
    "import json\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai import transforms\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    Activations,\n",
    "    MapTransform,\n",
    "    Transform,\n",
    ")\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.utils.enums import MetricReduction\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai import data\n",
    "\n",
    "# from monai.data import decollate_batch\n",
    "from functools import partial\n",
    "from src.custom_transforms import ConvertToMultiChannelBasedOnN_Froi, ConvertToMultiChannelBasedOnAnotatedInfiltration, masked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformaciones Swin UNETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minigo/anaconda3/envs/monai_env/lib/python3.11/site-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.croppad.dictionary CropForegroundd.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    }
   ],
   "source": [
    "roi = (128, 128, 128) # (220, 220, 155) (128, 128, 64)\n",
    "source_k=\"image\"\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        # ConvertToMultiChannelBasedOnN_Froi(keys=\"label\"),\n",
    "        # masked(keys=\"image\"),\n",
    "        ConvertToMultiChannelBasedOnAnotatedInfiltration(keys=\"label\"),\n",
    "        transforms.CropForegroundd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            source_key=source_k,\n",
    "            k_divisible=[roi[0], roi[1], roi[2]],\n",
    "        ),\n",
    "        transforms.RandSpatialCropd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            roi_size=[roi[0], roi[1], roi[2]],\n",
    "            random_size=False,\n",
    "        ),\n",
    "        transforms.NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "        \n",
    "    ]\n",
    ")\n",
    "val_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        # ConvertToMultiChannelBasedOnN_Froi(keys=\"label\"),\n",
    "        # masked(keys=\"image\"),\n",
    "        ConvertToMultiChannelBasedOnAnotatedInfiltration(keys=\"label\"),\n",
    "        transforms.RandSpatialCropd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            roi_size=[-1, -1, -1], #[224, 224, 128],\n",
    "            random_size=False,\n",
    "        ),\n",
    "        transforms.NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minigo/anaconda3/envs/monai_env/lib/python3.11/site-packages/monai/utils/deprecate_utils.py:221: FutureWarning: monai.networks.nets.swin_unetr SwinUNETR.__init__:img_size: Argument `img_size` has been deprecated since version 1.3. It will be removed in version 1.5. The img_size argument is not required anymore and checks on the input size are run during forward().\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SwinUNETR(\n",
       "  (swinViT): SwinTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv3d(11, 48, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (layers1): ModuleList(\n",
       "      (0): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=48, out_features=144, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=48, out_features=48, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (linear1): Linear(in_features=48, out_features=192, bias=True)\n",
       "              (linear2): Linear(in_features=192, out_features=48, bias=True)\n",
       "              (fn): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=384, out_features=96, bias=False)\n",
       "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layers2): ModuleList(\n",
       "      (0): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (linear1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (linear2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (fn): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=768, out_features=192, bias=False)\n",
       "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layers3): ModuleList(\n",
       "      (0): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (fn): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=1536, out_features=384, bias=False)\n",
       "          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layers4): ModuleList(\n",
       "      (0): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (linear1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (linear2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (fn): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder1): UnetrBasicBlock(\n",
       "    (layer): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(11, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(11, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder2): UnetrBasicBlock(\n",
       "    (layer): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder3): UnetrBasicBlock(\n",
       "    (layer): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder4): UnetrBasicBlock(\n",
       "    (layer): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder10): UnetrBasicBlock(\n",
       "    (layer): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(768, 768, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(768, 768, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder5): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(768, 384, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(768, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(768, 384, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder4): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(384, 192, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(384, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(384, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder3): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(192, 96, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(192, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(192, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder2): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(96, 48, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(96, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(96, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder1): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(48, 48, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(96, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(96, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (out): UnetOutBlock(\n",
       "    (conv): Convolution(\n",
       "      (conv): Conv3d(48, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################\n",
    "# Crear el modelo\n",
    "######################\n",
    "\n",
    "### Hyperparameter\n",
    "roi = (128, 128, 128)  # (128, 128, 128)\n",
    "\n",
    "# Create Swin transformer\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SwinUNETR(\n",
    "    img_size=roi,\n",
    "    in_channels=11,\n",
    "    out_channels=2,  # mdificar con edema\n",
    "    feature_size=48, #48\n",
    "    drop_rate=0.0,\n",
    "    attn_drop_rate=0.0,\n",
    "    dropout_path_rate=0.0,\n",
    "    use_checkpoint=True,\n",
    ")\n",
    "\n",
    "# Load the best model\n",
    "model_path = \"artifacts/ddfkhvym_best_model:v0/model.pt\"\n",
    "\n",
    "# Load the model on CPU\n",
    "loaded_model = torch.load(model_path, map_location=torch.device('cuda:0'))[\"state_dict\"]\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(loaded_model)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36 images and 36 labels.\n"
     ]
    }
   ],
   "source": [
    "# Create dataset data loader\n",
    "dataset_path='./Dataset/Dataset_recurrence'\n",
    "train_set=CustomDataset(dataset_path, section=\"train\", transform=train_transform) # v_transform\n",
    "train_loader = DataLoader(train_set, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 0\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 1\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 2\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 3\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 4\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 5\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 6\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 7\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 8\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 9\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 10\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 11\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 12\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 13\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 14\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 15\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 16\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 17\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 18\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 19\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 20\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 21\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 22\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 23\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 24\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 25\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 26\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 27\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 28\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 29\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 30\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 31\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 32\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 33\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 34\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 35\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embedding_dir = \"Dataset/contrastive_voxel_wise/embeddings\"\n",
    "label_output_dir = \"Dataset/contrastive_voxel_wise/labels\"\n",
    "\n",
    "# Crear carpetas si no existen\n",
    "os.makedirs(embedding_dir, exist_ok=True)\n",
    "os.makedirs(label_output_dir, exist_ok=True)\n",
    "\n",
    "# Variable para las características del decoder\n",
    "decoder_features = None\n",
    "\n",
    "# Función hook\n",
    "def decoder_hook_fn(module, input, output):\n",
    "    global decoder_features\n",
    "    decoder_features = output\n",
    "\n",
    "# Registrar el hook en decoder1.conv_block\n",
    "hook_handle_decoder = model.decoder1.conv_block.register_forward_hook(decoder_hook_fn)\n",
    "\n",
    "# Extraer y guardar\n",
    "with torch.no_grad():\n",
    "    for idx, batch_data in enumerate(train_loader):\n",
    "        image, label = batch_data[\"image\"], batch_data[\"label\"]\n",
    "        print(\"Image\", image.shape)  # [1, 11, 128, 128, 128]\n",
    "        print(\"label before squeeze\", label.shape)  # [1, 2, 128, 128, 128]\n",
    "        \n",
    "        image = image.to(device)\n",
    "        label = label.squeeze(0)  # [2, 128, 128, 128]\n",
    "        \n",
    "        # Convertir one-hot a etiquetas únicas\n",
    "        label_sum = label.sum(dim=0)  # [128, 128, 128], suma de canales\n",
    "        label_class = torch.zeros_like(label_sum, dtype=torch.long)  # [128, 128, 128]\n",
    "        \n",
    "        # Asignar clases:\n",
    "        # - Fondo (0, 0) -> 0\n",
    "        # - Vasogénico (1, 0) -> 1\n",
    "        # - Infiltrado (0, 1) -> 2\n",
    "        label_class[label[1] == 1] = 2  # Infiltrado\n",
    "        label_class[(label[0] == 1) & (label[1] == 0)] = 1  # Vasogénico\n",
    "        # Donde label_sum == 0, ya es fondo (0)\n",
    "        \n",
    "        label = label_class.cpu().numpy()  # [128, 128, 128]\n",
    "        print(\"label\", label.shape)\n",
    "        \n",
    "        # Obtener embeddings\n",
    "        _ = model(image)  # Ejecuta el forward para activar el hook\n",
    "        \n",
    "        print(\"decoder_features:\", decoder_features.shape)  # [1, 48, 128, 128, 128]\n",
    "        \n",
    "        # Guardar embeddings y etiquetas\n",
    "        np.save(f\"{embedding_dir}/case_{idx}.npy\", decoder_features.cpu().numpy())\n",
    "        np.save(f\"{label_output_dir}/case_{idx}.npy\", label)\n",
    "        \n",
    "        print(f\"Guardado embeddings y etiquetas para caso {idx}\")\n",
    "\n",
    "# Remover el hook\n",
    "hook_handle_decoder.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 2546, Unique labels: [0, 1, 2]\n",
      "Epoch 1/10, Batch 0/36, Loss: 0.8071\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 1/10, Batch 1/36, Loss: 0.8541\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 1/10, Batch 2/36, Loss: 0.8532\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 1/10, Batch 3/36, Loss: 0.7374\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 1/10, Batch 4/36, Loss: 0.6246\n",
      "Batch size: 2111, Unique labels: [0, 1, 2]\n",
      "Epoch 1/10, Batch 5/36, Loss: 0.3689\n",
      "Batch size: 2896, Unique labels: [0, 1, 2]\n",
      "Epoch 1/10, Batch 6/36, Loss: 0.5647\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 1/10, Batch 7/36, Loss: 0.5819\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 1/10, Batch 8/36, Loss: 0.5266\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 1/10, Batch 10/36, Loss: 0.5104\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 1/10, Batch 11/36, Loss: 0.5714\n",
      "Batch size: 2559, Unique labels: [0, 1, 2]\n",
      "Epoch 1/10, Batch 12/36, Loss: 0.4940\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 1/10, Batch 15/36, Loss: 0.5289\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 1/10, Batch 16/36, Loss: 0.7108\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 2048, Unique labels: [0, 2]\n",
      "Epoch 1/10, Batch 18/36, Loss: 0.0656\n",
      "Batch size: 2048, Unique labels: [0, 2]\n",
      "Epoch 1/10, Batch 19/36, Loss: 0.2110\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 1/10, Batch 21/36, Loss: 0.5812\n",
      "Batch size: 2816, Unique labels: [0, 1, 2]\n",
      "Epoch 1/10, Batch 22/36, Loss: 0.4615\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 1/10, Batch 24/36, Loss: 0.5185\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 1/10, Batch 25/36, Loss: 0.4552\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 1/10, Batch 27/36, Loss: 0.4639\n",
      "Batch size: 2048, Unique labels: [0, 2]\n",
      "Epoch 1/10, Batch 28/36, Loss: 0.2819\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 1/10, Batch 29/36, Loss: 0.5186\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 1/10, Batch 30/36, Loss: 0.4795\n",
      "Batch size: 2067, Unique labels: [0, 1, 2]\n",
      "Epoch 1/10, Batch 31/36, Loss: 0.1258\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 1/10, Batch 32/36, Loss: 0.4897\n",
      "Batch size: 2063, Unique labels: [0, 1, 2]\n",
      "Epoch 1/10, Batch 33/36, Loss: 0.0926\n",
      "Batch size: 1903, Unique labels: [0, 2]\n",
      "Epoch 1/10, Batch 34/36, Loss: 0.0572\n",
      "Batch size: 2051, Unique labels: [0, 1, 2]\n",
      "Epoch 1/10, Batch 35/36, Loss: 0.0746\n",
      "Epoch 1/10, Average Loss: 0.4693, Valid Batches: 29/36\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 2/10, Batch 0/36, Loss: 0.3729\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 2/10, Batch 1/36, Loss: 0.5200\n",
      "Batch size: 2546, Unique labels: [0, 1, 2]\n",
      "Epoch 2/10, Batch 2/36, Loss: 0.4108\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 2/10, Batch 3/36, Loss: 0.5032\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 2111, Unique labels: [0, 1, 2]\n",
      "Epoch 2/10, Batch 5/36, Loss: 0.2303\n",
      "Batch size: 2048, Unique labels: [0, 2]\n",
      "Epoch 2/10, Batch 6/36, Loss: 0.2793\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 2/10, Batch 7/36, Loss: 0.4154\n",
      "Batch size: 2896, Unique labels: [0, 1, 2]\n",
      "Epoch 2/10, Batch 8/36, Loss: 0.4799\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 2/10, Batch 9/36, Loss: 0.5076\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 2/10, Batch 10/36, Loss: 0.5216\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 2067, Unique labels: [0, 1, 2]\n",
      "Epoch 2/10, Batch 13/36, Loss: 0.1099\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 2/10, Batch 14/36, Loss: 0.4050\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 2/10, Batch 15/36, Loss: 0.4632\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 2/10, Batch 17/36, Loss: 0.4949\n",
      "Batch size: 2048, Unique labels: [0, 2]\n",
      "Epoch 2/10, Batch 18/36, Loss: 0.0541\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 2/10, Batch 19/36, Loss: 0.6457\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 2/10, Batch 21/36, Loss: 0.5977\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 2/10, Batch 22/36, Loss: 0.4424\n",
      "Batch size: 2559, Unique labels: [0, 1, 2]\n",
      "Epoch 2/10, Batch 23/36, Loss: 0.4271\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 2/10, Batch 25/36, Loss: 0.4277\n",
      "Batch size: 1903, Unique labels: [0, 2]\n",
      "Epoch 2/10, Batch 26/36, Loss: 0.0482\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 2/10, Batch 27/36, Loss: 0.5371\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 2/10, Batch 28/36, Loss: 0.3359\n",
      "Batch size: 2048, Unique labels: [0, 2]\n",
      "Epoch 2/10, Batch 29/36, Loss: 0.1526\n",
      "Batch size: 2051, Unique labels: [0, 1, 2]\n",
      "Epoch 2/10, Batch 30/36, Loss: 0.0819\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 2816, Unique labels: [0, 1, 2]\n",
      "Epoch 2/10, Batch 32/36, Loss: 0.3833\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 2/10, Batch 33/36, Loss: 0.5156\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 2/10, Batch 34/36, Loss: 0.4967\n",
      "Batch size: 2063, Unique labels: [0, 1, 2]\n",
      "Epoch 2/10, Batch 35/36, Loss: 0.0791\n",
      "Epoch 2/10, Average Loss: 0.3772, Valid Batches: 29/36\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 3/10, Batch 2/36, Loss: 0.5118\n",
      "Batch size: 2063, Unique labels: [0, 1, 2]\n",
      "Epoch 3/10, Batch 3/36, Loss: 0.0800\n",
      "Batch size: 2048, Unique labels: [0, 2]\n",
      "Epoch 3/10, Batch 4/36, Loss: 0.0646\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 3/10, Batch 5/36, Loss: 0.4952\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 3/10, Batch 6/36, Loss: 0.3793\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 3/10, Batch 7/36, Loss: 0.4479\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 3/10, Batch 8/36, Loss: 0.5059\n",
      "Batch size: 2546, Unique labels: [0, 1, 2]\n",
      "Epoch 3/10, Batch 9/36, Loss: 0.3959\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 3/10, Batch 11/36, Loss: 0.5221\n",
      "Batch size: 2051, Unique labels: [0, 1, 2]\n",
      "Epoch 3/10, Batch 12/36, Loss: 0.0850\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 3/10, Batch 13/36, Loss: 0.3138\n",
      "Batch size: 1903, Unique labels: [0, 2]\n",
      "Epoch 3/10, Batch 14/36, Loss: 0.0394\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 2067, Unique labels: [0, 1, 2]\n",
      "Epoch 3/10, Batch 16/36, Loss: 0.1079\n",
      "Batch size: 2111, Unique labels: [0, 1, 2]\n",
      "Epoch 3/10, Batch 17/36, Loss: 0.2663\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 2048, Unique labels: [0, 2]\n",
      "Epoch 3/10, Batch 19/36, Loss: 0.2548\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 3/10, Batch 21/36, Loss: 0.5000\n",
      "Batch size: 2048, Unique labels: [0, 2]\n",
      "Epoch 3/10, Batch 22/36, Loss: 0.1385\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 3/10, Batch 23/36, Loss: 0.6377\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 3/10, Batch 24/36, Loss: 0.3690\n",
      "Batch size: 2896, Unique labels: [0, 1, 2]\n",
      "Epoch 3/10, Batch 25/36, Loss: 0.4806\n",
      "Batch size: 2559, Unique labels: [0, 1, 2]\n",
      "Epoch 3/10, Batch 26/36, Loss: 0.4541\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 3/10, Batch 27/36, Loss: 0.5588\n",
      "Batch size: 2816, Unique labels: [0, 1, 2]\n",
      "Epoch 3/10, Batch 28/36, Loss: 0.3979\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 3/10, Batch 29/36, Loss: 0.5068\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 3/10, Batch 30/36, Loss: 0.2693\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 3/10, Batch 31/36, Loss: 0.4930\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 3/10, Batch 32/36, Loss: 0.4304\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 3/10, Batch 33/36, Loss: 0.3964\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 3/10, Batch 35/36, Loss: 0.5219\n",
      "Epoch 3/10, Average Loss: 0.3664, Valid Batches: 29/36\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 4/10, Batch 0/36, Loss: 0.5743\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 4/10, Batch 1/36, Loss: 0.5485\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 4/10, Batch 2/36, Loss: 0.4813\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 4/10, Batch 5/36, Loss: 0.3854\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 4/10, Batch 7/36, Loss: 0.4451\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 4/10, Batch 8/36, Loss: 0.5187\n",
      "Batch size: 2559, Unique labels: [0, 1, 2]\n",
      "Epoch 4/10, Batch 9/36, Loss: 0.4165\n",
      "Batch size: 2063, Unique labels: [0, 1, 2]\n",
      "Epoch 4/10, Batch 10/36, Loss: 0.0851\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 4/10, Batch 11/36, Loss: 0.5001\n",
      "Batch size: 2546, Unique labels: [0, 1, 2]\n",
      "Epoch 4/10, Batch 12/36, Loss: 0.3972\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 4/10, Batch 13/36, Loss: 0.4903\n",
      "Batch size: 2896, Unique labels: [0, 1, 2]\n",
      "Epoch 4/10, Batch 14/36, Loss: 0.4749\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 2067, Unique labels: [0, 1, 2]\n",
      "Epoch 4/10, Batch 16/36, Loss: 0.1010\n",
      "Batch size: 2048, Unique labels: [0, 2]\n",
      "Epoch 4/10, Batch 17/36, Loss: 0.2882\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 4/10, Batch 19/36, Loss: 0.2593\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 4/10, Batch 20/36, Loss: 0.6807\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 4/10, Batch 21/36, Loss: 0.4276\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 4/10, Batch 22/36, Loss: 0.4845\n",
      "Batch size: 2048, Unique labels: [0, 2]\n",
      "Epoch 4/10, Batch 23/36, Loss: 0.0633\n",
      "Batch size: 2048, Unique labels: [0, 2]\n",
      "Epoch 4/10, Batch 24/36, Loss: 0.1352\n",
      "Batch size: 1903, Unique labels: [0, 2]\n",
      "Epoch 4/10, Batch 25/36, Loss: 0.0520\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 4/10, Batch 26/36, Loss: 0.4756\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 4/10, Batch 27/36, Loss: 0.5199\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 4/10, Batch 29/36, Loss: 0.4087\n",
      "Batch size: 2111, Unique labels: [0, 1, 2]\n",
      "Epoch 4/10, Batch 30/36, Loss: 0.2166\n",
      "Batch size: 2051, Unique labels: [0, 1, 2]\n",
      "Epoch 4/10, Batch 31/36, Loss: 0.0868\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 4/10, Batch 33/36, Loss: 0.3339\n",
      "Batch size: 2816, Unique labels: [0, 1, 2]\n",
      "Epoch 4/10, Batch 34/36, Loss: 0.3816\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 4/10, Batch 35/36, Loss: 0.3997\n",
      "Epoch 4/10, Average Loss: 0.3666, Valid Batches: 29/36\n",
      "Batch size: 2048, Unique labels: [0, 2]\n",
      "Epoch 5/10, Batch 0/36, Loss: 0.1342\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 5/10, Batch 1/36, Loss: 0.3798\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 5/10, Batch 2/36, Loss: 0.5227\n",
      "Batch size: 2546, Unique labels: [0, 1, 2]\n",
      "Epoch 5/10, Batch 3/36, Loss: 0.4133\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 5/10, Batch 5/36, Loss: 0.2431\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 2111, Unique labels: [0, 1, 2]\n",
      "Epoch 5/10, Batch 8/36, Loss: 0.2436\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 5/10, Batch 9/36, Loss: 0.3678\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 5/10, Batch 10/36, Loss: 0.3720\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 5/10, Batch 11/36, Loss: 0.4599\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 5/10, Batch 12/36, Loss: 0.4933\n",
      "Batch size: 2048, Unique labels: [0, 2]\n",
      "Epoch 5/10, Batch 13/36, Loss: 0.2072\n",
      "Batch size: 2896, Unique labels: [0, 1, 2]\n",
      "Epoch 5/10, Batch 14/36, Loss: 0.4629\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 5/10, Batch 15/36, Loss: 0.4864\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 5/10, Batch 16/36, Loss: 0.4204\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 5/10, Batch 17/36, Loss: 0.4293\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 1903, Unique labels: [0, 2]\n",
      "Epoch 5/10, Batch 21/36, Loss: 0.0572\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 5/10, Batch 22/36, Loss: 0.6263\n",
      "Batch size: 2063, Unique labels: [0, 1, 2]\n",
      "Epoch 5/10, Batch 23/36, Loss: 0.0954\n",
      "Batch size: 2067, Unique labels: [0, 1, 2]\n",
      "Epoch 5/10, Batch 24/36, Loss: 0.1070\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 5/10, Batch 25/36, Loss: 0.5082\n",
      "Batch size: 2816, Unique labels: [0, 1, 2]\n",
      "Epoch 5/10, Batch 26/36, Loss: 0.3610\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 5/10, Batch 27/36, Loss: 0.5774\n",
      "Batch size: 2051, Unique labels: [0, 1, 2]\n",
      "Epoch 5/10, Batch 28/36, Loss: 0.0723\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 5/10, Batch 29/36, Loss: 0.5066\n",
      "Batch size: 2559, Unique labels: [0, 1, 2]\n",
      "Epoch 5/10, Batch 30/36, Loss: 0.4315\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 5/10, Batch 31/36, Loss: 0.3100\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 5/10, Batch 32/36, Loss: 0.4818\n",
      "Batch size: 2048, Unique labels: [0, 2]\n",
      "Epoch 5/10, Batch 33/36, Loss: 0.0653\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 5/10, Batch 35/36, Loss: 0.5484\n",
      "Epoch 5/10, Average Loss: 0.3581, Valid Batches: 29/36\n",
      "Batch size: 2067, Unique labels: [0, 1, 2]\n",
      "Epoch 6/10, Batch 0/36, Loss: 0.1256\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 6/10, Batch 2/36, Loss: 0.3503\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 6/10, Batch 3/36, Loss: 0.4331\n",
      "Batch size: 2546, Unique labels: [0, 1, 2]\n",
      "Epoch 6/10, Batch 4/36, Loss: 0.4114\n",
      "Batch size: 2048, Unique labels: [0, 2]\n",
      "Epoch 6/10, Batch 5/36, Loss: 0.0634\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 6/10, Batch 6/36, Loss: 0.4807\n",
      "Batch size: 1903, Unique labels: [0, 2]\n",
      "Epoch 6/10, Batch 7/36, Loss: 0.0444\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 6/10, Batch 8/36, Loss: 0.3713\n",
      "Batch size: 2559, Unique labels: [0, 1, 2]\n",
      "Epoch 6/10, Batch 9/36, Loss: 0.4046\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 6/10, Batch 10/36, Loss: 0.2495\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 6/10, Batch 11/36, Loss: 0.5132\n",
      "Batch size: 2063, Unique labels: [0, 1, 2]\n",
      "Epoch 6/10, Batch 12/36, Loss: 0.0752\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 6/10, Batch 13/36, Loss: 0.4743\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 2048, Unique labels: [0, 2]\n",
      "Epoch 6/10, Batch 15/36, Loss: 0.1616\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 6/10, Batch 16/36, Loss: 0.6361\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 2048, Unique labels: [0, 2]\n",
      "Epoch 6/10, Batch 18/36, Loss: 0.2199\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 6/10, Batch 19/36, Loss: 0.3422\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 6/10, Batch 20/36, Loss: 0.4550\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 6/10, Batch 21/36, Loss: 0.5128\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 6/10, Batch 22/36, Loss: 0.5068\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 6/10, Batch 23/36, Loss: 0.4833\n",
      "Batch size: 2051, Unique labels: [0, 1, 2]\n",
      "Epoch 6/10, Batch 24/36, Loss: 0.0747\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 6/10, Batch 26/36, Loss: 0.3037\n",
      "Batch size: 2816, Unique labels: [0, 1, 2]\n",
      "Epoch 6/10, Batch 27/36, Loss: 0.3750\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 6/10, Batch 28/36, Loss: 0.3893\n",
      "Batch size: 2896, Unique labels: [0, 1, 2]\n",
      "Epoch 6/10, Batch 29/36, Loss: 0.4729\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 6/10, Batch 30/36, Loss: 0.5473\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 6/10, Batch 32/36, Loss: 0.5684\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 2111, Unique labels: [0, 1, 2]\n",
      "Epoch 6/10, Batch 34/36, Loss: 0.2180\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Epoch 6/10, Average Loss: 0.3539, Valid Batches: 29/36\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 7/10, Batch 0/36, Loss: 0.4041\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 7/10, Batch 1/36, Loss: 0.4780\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 7/10, Batch 2/36, Loss: 0.3621\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 2816, Unique labels: [0, 1, 2]\n",
      "Epoch 7/10, Batch 4/36, Loss: 0.3700\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 7/10, Batch 5/36, Loss: 0.5414\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 7/10, Batch 7/36, Loss: 0.4919\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 7/10, Batch 9/36, Loss: 0.5550\n",
      "Batch size: 2896, Unique labels: [0, 1, 2]\n",
      "Epoch 7/10, Batch 10/36, Loss: 0.4758\n",
      "Batch size: 2111, Unique labels: [0, 1, 2]\n",
      "Epoch 7/10, Batch 11/36, Loss: 0.2125\n",
      "Batch size: 2067, Unique labels: [0, 1, 2]\n",
      "Epoch 7/10, Batch 12/36, Loss: 0.1293\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 7/10, Batch 14/36, Loss: 0.3170\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 7/10, Batch 15/36, Loss: 0.4498\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 7/10, Batch 16/36, Loss: 0.4658\n",
      "Batch size: 2048, Unique labels: [0, 2]\n",
      "Epoch 7/10, Batch 17/36, Loss: 0.1299\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 7/10, Batch 18/36, Loss: 0.2483\n",
      "Batch size: 2063, Unique labels: [0, 1, 2]\n",
      "Epoch 7/10, Batch 19/36, Loss: 0.0985\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 7/10, Batch 21/36, Loss: 0.5365\n",
      "Batch size: 1903, Unique labels: [0, 2]\n",
      "Epoch 7/10, Batch 22/36, Loss: 0.0499\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 7/10, Batch 23/36, Loss: 0.5119\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 7/10, Batch 24/36, Loss: 0.5029\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 7/10, Batch 25/36, Loss: 0.4082\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 7/10, Batch 26/36, Loss: 0.3522\n",
      "Batch size: 2051, Unique labels: [0, 1, 2]\n",
      "Epoch 7/10, Batch 27/36, Loss: 0.0742\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 7/10, Batch 28/36, Loss: 0.4726\n",
      "Batch size: 2559, Unique labels: [0, 1, 2]\n",
      "Epoch 7/10, Batch 29/36, Loss: 0.4067\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 7/10, Batch 30/36, Loss: 0.6084\n",
      "Batch size: 2048, Unique labels: [0, 2]\n",
      "Epoch 7/10, Batch 31/36, Loss: 0.1916\n",
      "Batch size: 2546, Unique labels: [0, 1, 2]\n",
      "Epoch 7/10, Batch 32/36, Loss: 0.4000\n",
      "Batch size: 2048, Unique labels: [0, 2]\n",
      "Epoch 7/10, Batch 33/36, Loss: 0.0648\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Epoch 7/10, Average Loss: 0.3555, Valid Batches: 29/36\n",
      "Batch size: 1903, Unique labels: [0, 2]\n",
      "Epoch 8/10, Batch 0/36, Loss: 0.0552\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 8/10, Batch 1/36, Loss: 0.4671\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 8/10, Batch 2/36, Loss: 0.2972\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 8/10, Batch 3/36, Loss: 0.3618\n",
      "Batch size: 2546, Unique labels: [0, 1, 2]\n",
      "Epoch 8/10, Batch 4/36, Loss: 0.4077\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 8/10, Batch 5/36, Loss: 0.4526\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 8/10, Batch 6/36, Loss: 0.5267\n",
      "Batch size: 2067, Unique labels: [0, 1, 2]\n",
      "Epoch 8/10, Batch 7/36, Loss: 0.1114\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 8/10, Batch 9/36, Loss: 0.5005\n",
      "Batch size: 2896, Unique labels: [0, 1, 2]\n",
      "Epoch 8/10, Batch 10/36, Loss: 0.4729\n",
      "Batch size: 2559, Unique labels: [0, 1, 2]\n",
      "Epoch 8/10, Batch 11/36, Loss: 0.4153\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 8/10, Batch 12/36, Loss: 0.3696\n",
      "Batch size: 2063, Unique labels: [0, 1, 2]\n",
      "Epoch 8/10, Batch 13/36, Loss: 0.0913\n",
      "Batch size: 2816, Unique labels: [0, 1, 2]\n",
      "Epoch 8/10, Batch 14/36, Loss: 0.3770\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 8/10, Batch 15/36, Loss: 0.2400\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 8/10, Batch 16/36, Loss: 0.5197\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 8/10, Batch 17/36, Loss: 0.4878\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 2051, Unique labels: [0, 1, 2]\n",
      "Epoch 8/10, Batch 19/36, Loss: 0.0730\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 8/10, Batch 20/36, Loss: 0.3935\n",
      "Batch size: 2048, Unique labels: [0, 2]\n",
      "Epoch 8/10, Batch 21/36, Loss: 0.1435\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 8/10, Batch 22/36, Loss: 0.4098\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 8/10, Batch 23/36, Loss: 0.6099\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 8/10, Batch 28/36, Loss: 0.5869\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 8/10, Batch 29/36, Loss: 0.4763\n",
      "Batch size: 2048, Unique labels: [0, 2]\n",
      "Epoch 8/10, Batch 30/36, Loss: 0.1569\n",
      "Batch size: 2048, Unique labels: [0, 2]\n",
      "Epoch 8/10, Batch 31/36, Loss: 0.0666\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 8/10, Batch 32/36, Loss: 0.5227\n",
      "Batch size: 2111, Unique labels: [0, 1, 2]\n",
      "Epoch 8/10, Batch 33/36, Loss: 0.1879\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 8/10, Batch 34/36, Loss: 0.4682\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Epoch 8/10, Average Loss: 0.3534, Valid Batches: 29/36\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 9/10, Batch 0/36, Loss: 0.2606\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 9/10, Batch 1/36, Loss: 0.5586\n",
      "Batch size: 2051, Unique labels: [0, 1, 2]\n",
      "Epoch 9/10, Batch 2/36, Loss: 0.0900\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 9/10, Batch 3/36, Loss: 0.3919\n",
      "Batch size: 1903, Unique labels: [0, 2]\n",
      "Epoch 9/10, Batch 4/36, Loss: 0.0798\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 9/10, Batch 5/36, Loss: 0.3084\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 9/10, Batch 6/36, Loss: 0.5292\n",
      "Batch size: 2111, Unique labels: [0, 1, 2]\n",
      "Epoch 9/10, Batch 7/36, Loss: 0.2153\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 9/10, Batch 8/36, Loss: 0.4802\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 9/10, Batch 9/36, Loss: 0.3778\n",
      "Batch size: 2067, Unique labels: [0, 1, 2]\n",
      "Epoch 9/10, Batch 10/36, Loss: 0.1197\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 9/10, Batch 11/36, Loss: 0.5111\n",
      "Batch size: 2559, Unique labels: [0, 1, 2]\n",
      "Epoch 9/10, Batch 12/36, Loss: 0.4072\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 9/10, Batch 13/36, Loss: 0.4685\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 2048, Unique labels: [0, 2]\n",
      "Epoch 9/10, Batch 15/36, Loss: 0.0646\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 9/10, Batch 16/36, Loss: 0.3752\n",
      "Batch size: 2816, Unique labels: [0, 1, 2]\n",
      "Epoch 9/10, Batch 17/36, Loss: 0.3766\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 9/10, Batch 18/36, Loss: 0.6233\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 9/10, Batch 21/36, Loss: 0.4526\n",
      "Batch size: 2896, Unique labels: [0, 1, 2]\n",
      "Epoch 9/10, Batch 22/36, Loss: 0.4603\n",
      "Batch size: 2048, Unique labels: [0, 2]\n",
      "Epoch 9/10, Batch 23/36, Loss: 0.2017\n",
      "Batch size: 2546, Unique labels: [0, 1, 2]\n",
      "Epoch 9/10, Batch 24/36, Loss: 0.4024\n",
      "Batch size: 2063, Unique labels: [0, 1, 2]\n",
      "Epoch 9/10, Batch 25/36, Loss: 0.0855\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 9/10, Batch 26/36, Loss: 0.5087\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 9/10, Batch 29/36, Loss: 0.4905\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 9/10, Batch 31/36, Loss: 0.5138\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 9/10, Batch 32/36, Loss: 0.4471\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 9/10, Batch 33/36, Loss: 0.4653\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 2048, Unique labels: [0, 2]\n",
      "Epoch 9/10, Batch 35/36, Loss: 0.1146\n",
      "Epoch 9/10, Average Loss: 0.3579, Valid Batches: 29/36\n",
      "Batch size: 2559, Unique labels: [0, 1, 2]\n",
      "Epoch 10/10, Batch 0/36, Loss: 0.4003\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 10/10, Batch 1/36, Loss: 0.5081\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 10/10, Batch 2/36, Loss: 0.4609\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 10/10, Batch 3/36, Loss: 0.5337\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 2048, Unique labels: [0, 2]\n",
      "Epoch 10/10, Batch 6/36, Loss: 0.1080\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 10/10, Batch 7/36, Loss: 0.3614\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 10/10, Batch 8/36, Loss: 0.5777\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 10/10, Batch 9/36, Loss: 0.4668\n",
      "Batch size: 1903, Unique labels: [0, 2]\n",
      "Epoch 10/10, Batch 10/36, Loss: 0.0523\n",
      "Batch size: 2896, Unique labels: [0, 1, 2]\n",
      "Epoch 10/10, Batch 11/36, Loss: 0.4684\n",
      "Batch size: 2051, Unique labels: [0, 1, 2]\n",
      "Epoch 10/10, Batch 12/36, Loss: 0.0591\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 10/10, Batch 13/36, Loss: 0.5762\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 10/10, Batch 14/36, Loss: 0.4841\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 10/10, Batch 15/36, Loss: 0.4410\n",
      "Batch size: 2048, Unique labels: [0, 2]\n",
      "Epoch 10/10, Batch 16/36, Loss: 0.0575\n",
      "Batch size: 2048, Unique labels: [0, 2]\n",
      "Epoch 10/10, Batch 17/36, Loss: 0.1559\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 10/10, Batch 18/36, Loss: 0.3187\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 2111, Unique labels: [0, 1, 2]\n",
      "Epoch 10/10, Batch 21/36, Loss: 0.1993\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 10/10, Batch 23/36, Loss: 0.2295\n",
      "Batch size: 2067, Unique labels: [0, 1, 2]\n",
      "Epoch 10/10, Batch 24/36, Loss: 0.1189\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 10/10, Batch 25/36, Loss: 0.5146\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 10/10, Batch 26/36, Loss: 0.5234\n",
      "Batch size: 2546, Unique labels: [0, 1, 2]\n",
      "Epoch 10/10, Batch 27/36, Loss: 0.4098\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 10/10, Batch 28/36, Loss: 0.3895\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 10/10, Batch 29/36, Loss: 0.3541\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 10/10, Batch 30/36, Loss: 0.4077\n",
      "Batch size: 2816, Unique labels: [0, 1, 2]\n",
      "Epoch 10/10, Batch 31/36, Loss: 0.3764\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Advertencia: Solo una clase presente ([0]), devolviendo pérdida 0\n",
      "Batch size: 3072, Unique labels: [0, 1, 2]\n",
      "Epoch 10/10, Batch 34/36, Loss: 0.4747\n",
      "Batch size: 2063, Unique labels: [0, 1, 2]\n",
      "Epoch 10/10, Batch 35/36, Loss: 0.0803\n",
      "Epoch 10/10, Average Loss: 0.3486, Valid Batches: 29/36\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, embedding_dir, label_dir):\n",
    "        self.embedding_dir = embedding_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.case_files = [f for f in os.listdir(embedding_dir) if f.endswith(\".npy\")]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.case_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        embedding_path = os.path.join(self.embedding_dir, f\"case_{idx}.npy\")\n",
    "        label_path = os.path.join(self.label_dir, f\"case_{idx}.npy\")\n",
    "        \n",
    "        embeddings = np.load(embedding_path)\n",
    "        labels = np.load(label_path)\n",
    "        \n",
    "        embeddings = torch.tensor(embeddings, dtype=torch.float32).squeeze(0)\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "        return embeddings, labels\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, input_dim=48, hidden_dim=128, output_dim=128):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def contrastive_loss(z, labels, temperature=0.5, sample_size_per_class=1024):\n",
    "    N_total = z.shape[0]\n",
    "    z = F.normalize(z, dim=1)\n",
    "    \n",
    "    classes = torch.unique(labels)\n",
    "    if len(classes) < 2:\n",
    "        print(f\"Advertencia: Solo una clase presente ({classes.tolist()}), devolviendo pérdida 0\")\n",
    "        return torch.tensor(0.0, device=device, requires_grad=True)\n",
    "    \n",
    "    sampled_z = []\n",
    "    sampled_labels = []\n",
    "    \n",
    "    for cls in classes:\n",
    "        cls_indices = (labels == cls).nonzero(as_tuple=True)[0]\n",
    "        cls_size = cls_indices.shape[0]\n",
    "        if cls_size > sample_size_per_class:\n",
    "            indices = torch.randperm(cls_size)[:sample_size_per_class]\n",
    "            cls_indices = cls_indices[indices]\n",
    "        sampled_z.append(z[cls_indices])\n",
    "        sampled_labels.append(labels[cls_indices])\n",
    "    \n",
    "    z = torch.cat(sampled_z, dim=0)\n",
    "    labels = torch.cat(sampled_labels, dim=0)\n",
    "    N = z.shape[0]\n",
    "    \n",
    "    print(f\"Batch size: {N}, Unique labels: {torch.unique(labels).tolist()}\")\n",
    "    \n",
    "    if N < 2:\n",
    "        print(\"Advertencia: Batch con menos de 2 vóxeles, devolviendo pérdida 0\")\n",
    "        return torch.tensor(0.0, device=device, requires_grad=True)\n",
    "    \n",
    "    similarity = torch.mm(z, z.T) / temperature\n",
    "    labels_eq = labels.unsqueeze(1) == labels.unsqueeze(0)\n",
    "    labels_eq = labels_eq.float()\n",
    "    eye = torch.eye(N, device=device)\n",
    "    labels_eq = labels_eq * (1 - eye)\n",
    "    \n",
    "    exp_sim = torch.exp(similarity)\n",
    "    pos_sum = (exp_sim * labels_eq).sum(dim=1)\n",
    "    neg_sum = exp_sim.sum(dim=1) - exp_sim.diag()\n",
    "    \n",
    "    if pos_sum.sum() == 0:\n",
    "        print(\"Advertencia: No hay pares positivos, pérdida será 0\")\n",
    "    \n",
    "    loss = -torch.log((pos_sum + 1e-6) / (neg_sum + 1e-6))\n",
    "    return loss.mean()\n",
    "\n",
    "# Configuración\n",
    "embedding_dir = \"Dataset/contrastive_voxel_wise/embeddings\"\n",
    "label_dir = \"Dataset/contrastive_voxel_wise/labels\"\n",
    "batch_size = 1\n",
    "sample_size_per_class = 1024\n",
    "temperature = 0.5\n",
    "num_epochs = 10\n",
    "\n",
    "dataset = EmbeddingDataset(embedding_dir, label_dir)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "model = ProjectionHead(input_dim=48).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    valid_batches = 0\n",
    "    for batch_idx, (embeddings, labels) in enumerate(loader):\n",
    "        embeddings = embeddings.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        embeddings = embeddings.squeeze(0).permute(1, 2, 3, 0)\n",
    "        labels = labels.squeeze(0)\n",
    "        \n",
    "        embeddings_flat = embeddings.reshape(-1, 48)\n",
    "        labels_flat = labels.reshape(-1)\n",
    "        \n",
    "        valid_mask = labels_flat >= 0\n",
    "        embeddings_valid = embeddings_flat[valid_mask]\n",
    "        labels_valid = labels_flat[valid_mask]\n",
    "        \n",
    "        if embeddings_valid.shape[0] < 2:\n",
    "            print(f\"Batch {batch_idx}: Insuficientes vóxeles válidos\")\n",
    "            continue\n",
    "        \n",
    "        z = model(embeddings_valid)\n",
    "        loss = contrastive_loss(z, labels_valid, temperature, sample_size_per_class)\n",
    "        \n",
    "        if loss.item() == 0:\n",
    "            continue  # No contar batches con pérdida 0 en el promedio\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        valid_batches += 1\n",
    "        \n",
    "        if batch_idx % 1 == 0:  # Reducir frecuencia de impresión\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(loader)}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    avg_loss = total_loss / max(valid_batches, 1)  # Evitar división por 0\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}, Valid Batches: {valid_batches}/{len(loader)}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"contrastive_projection_head.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (embeddings, labels) in enumerate(loader):\n",
    "    print(f\"Unique labels: {torch.unique(labels).tolist()}\")\n",
    "    labels_flat = labels.reshape(-1)\n",
    "    class_counts = torch.bincount(labels_flat)\n",
    "    print(f\"Case {idx}: Fondo: {class_counts[0]}, Vasogénico: {class_counts[1] if len(class_counts) > 1 else 0}, Infiltrado: {class_counts[2] if len(class_counts) > 2 else 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "label_dir = \"Dataset/contrastive_voxel_wise/labels\"\n",
    "for idx in range(36):  # Ajusta según el número de casos\n",
    "    label_path = os.path.join(label_dir, f\"case_{idx}.npy\")\n",
    "    if os.path.exists(label_path):\n",
    "        label = np.load(label_path)\n",
    "        unique, counts = np.unique(label, return_counts=True)\n",
    "        print(f\"Case {idx}: {dict(zip(unique, counts))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
