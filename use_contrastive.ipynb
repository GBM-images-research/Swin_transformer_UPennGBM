{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading functions\n",
    "import os\n",
    "import time\n",
    "from monai.data import DataLoader, decollate_batch\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "\n",
    "from src.get_data import CustomDataset, CustomDatasetSeg\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from types import SimpleNamespace\n",
    "import wandb\n",
    "import logging\n",
    "\n",
    "#####\n",
    "import json\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai import transforms\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    Activations,\n",
    "    MapTransform,\n",
    "    Transform,\n",
    ")\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.utils.enums import MetricReduction\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai import data\n",
    "\n",
    "# from monai.data import decollate_batch\n",
    "from functools import partial\n",
    "from src.custom_transforms import ConvertToMultiChannelBasedOnN_Froi, ConvertToMultiChannelBasedOnAnotatedInfiltration, masked, ConvertToMultiChannelBasedOnBratsClassesdI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformaciones Swin UNETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/training2/miniconda3/envs/monai_env/lib/python3.11/site-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.croppad.dictionary CropForegroundd.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    }
   ],
   "source": [
    "roi = (128, 128, 128) # (220, 220, 155) (128, 128, 64)\n",
    "source_k=\"label\"\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        # ConvertToMultiChannelBasedOnN_Froi(keys=\"label\"),\n",
    "        # masked(keys=\"image\"),\n",
    "        ConvertToMultiChannelBasedOnAnotatedInfiltration(keys=\"label\"),\n",
    "        # ConvertToMultiChannelBasedOnBratsClassesdI(keys=\"label\"),\n",
    "        transforms.CropForegroundd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            source_key=source_k,\n",
    "            k_divisible=[roi[0], roi[1], roi[2]],\n",
    "        ),\n",
    "        transforms.RandSpatialCropd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            roi_size=[roi[0], roi[1], roi[2]],\n",
    "            random_size=False,\n",
    "        ),\n",
    "        transforms.NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "        \n",
    "    ]\n",
    ")\n",
    "val_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        # ConvertToMultiChannelBasedOnN_Froi(keys=\"label\"),\n",
    "        # masked(keys=\"image\"),\n",
    "        ConvertToMultiChannelBasedOnAnotatedInfiltration(keys=\"label\"),\n",
    "        # ConvertToMultiChannelBasedOnBratsClassesdI(keys=\"label\"),\n",
    "        transforms.RandSpatialCropd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            roi_size=[-1, -1, -1], #[224, 224, 128],\n",
    "            random_size=False,\n",
    "        ),\n",
    "        transforms.NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "run = wandb.init()\n",
    "artifact = run.use_artifact('mlops-team89/Swin_UPENN_10cases/fhosddxt_best_model:v0', type='model')\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/training2/miniconda3/envs/monai_env/lib/python3.11/site-packages/monai/utils/deprecate_utils.py:221: FutureWarning: monai.networks.nets.swin_unetr SwinUNETR.__init__:img_size: Argument `img_size` has been deprecated since version 1.3. It will be removed in version 1.5. The img_size argument is not required anymore and checks on the input size are run during forward().\n",
      "  warn_deprecated(argname, msg, warning_category)\n",
      "/tmp/ipykernel_1378320/730760453.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model = torch.load(model_path, map_location=torch.device('cuda:0'))[\"state_dict\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SwinUNETR(\n",
       "  (swinViT): SwinTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv3d(11, 48, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (layers1): ModuleList(\n",
       "      (0): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=48, out_features=144, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=48, out_features=48, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (linear1): Linear(in_features=48, out_features=192, bias=True)\n",
       "              (linear2): Linear(in_features=192, out_features=48, bias=True)\n",
       "              (fn): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=384, out_features=96, bias=False)\n",
       "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layers2): ModuleList(\n",
       "      (0): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (linear1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (linear2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (fn): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=768, out_features=192, bias=False)\n",
       "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layers3): ModuleList(\n",
       "      (0): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (fn): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=1536, out_features=384, bias=False)\n",
       "          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layers4): ModuleList(\n",
       "      (0): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (linear1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (linear2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (fn): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder1): UnetrBasicBlock(\n",
       "    (layer): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(11, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(11, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder2): UnetrBasicBlock(\n",
       "    (layer): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder3): UnetrBasicBlock(\n",
       "    (layer): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder4): UnetrBasicBlock(\n",
       "    (layer): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder10): UnetrBasicBlock(\n",
       "    (layer): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(768, 768, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(768, 768, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder5): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(768, 384, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(768, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(768, 384, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder4): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(384, 192, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(384, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(384, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder3): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(192, 96, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(192, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(192, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder2): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(96, 48, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(96, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(96, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder1): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(48, 48, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(96, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(96, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (out): UnetOutBlock(\n",
       "    (conv): Convolution(\n",
       "      (conv): Conv3d(48, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################\n",
    "# Crear el modelo\n",
    "######################\n",
    "\n",
    "### Hyperparameter\n",
    "roi = (128, 128, 128)  # (128, 128, 128)\n",
    "\n",
    "# Create Swin transformer\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SwinUNETR(\n",
    "    img_size=roi,\n",
    "    in_channels=11,\n",
    "    out_channels=2,  # mdificar con edema\n",
    "    feature_size=48, #48\n",
    "    drop_rate=0.0,\n",
    "    attn_drop_rate=0.0,\n",
    "    dropout_path_rate=0.0,\n",
    "    use_checkpoint=True,\n",
    ")\n",
    "\n",
    "# Load the best model\n",
    "model_path = \"artifacts/fhosddxt_best_model:v0/model.pt\"\n",
    "\n",
    "# Load the model on CPU\n",
    "loaded_model = torch.load(model_path, map_location=torch.device('cuda:0'))[\"state_dict\"]\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(loaded_model)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30 images and 30 labels.\n"
     ]
    }
   ],
   "source": [
    "# Create dataset data loader\n",
    "# dataset_path='./Dataset/Dataset_recurrence'\n",
    "dataset_path='./Dataset/Dataset_30_6'\n",
    "train_set=CustomDataset(dataset_path, section=\"train_30\", transform=train_transform) # v_transform\n",
    "train_loader = DataLoader(train_set, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 0\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 1\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 2\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 3\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 4\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 5\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 6\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 7\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 8\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 9\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 10\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 11\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 12\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 13\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 14\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 15\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 16\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 17\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 18\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 19\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 20\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 21\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 22\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 23\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 24\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 25\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 26\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 27\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 28\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 29\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embedding_dir = \"Dataset/contrastive_voxel_wise/embeddings\"\n",
    "label_output_dir = \"Dataset/contrastive_voxel_wise/labels\"\n",
    "\n",
    "# Crear carpetas si no existen\n",
    "os.makedirs(embedding_dir, exist_ok=True)\n",
    "os.makedirs(label_output_dir, exist_ok=True)\n",
    "\n",
    "# Variable para las características del decoder\n",
    "decoder_features = None\n",
    "\n",
    "# Función hook\n",
    "def decoder_hook_fn(module, input, output):\n",
    "    global decoder_features\n",
    "    decoder_features = output\n",
    "\n",
    "# Registrar el hook en decoder1.conv_block\n",
    "hook_handle_decoder = model.decoder1.conv_block.register_forward_hook(decoder_hook_fn)\n",
    "\n",
    "# Extraer y guardar\n",
    "with torch.no_grad():\n",
    "    for idx, batch_data in enumerate(train_loader):\n",
    "        image, label = batch_data[\"image\"], batch_data[\"label\"]\n",
    "        print(\"Image\", image.shape)  # [1, 11, 128, 128, 128]\n",
    "        print(\"label before squeeze\", label.shape)  # [1, 2, 128, 128, 128]\n",
    "        \n",
    "        image = image.to(device)\n",
    "        label = label.squeeze(0)  # [2, 128, 128, 128]\n",
    "        \n",
    "        # Convertir one-hot a etiquetas únicas\n",
    "        label_sum = label.sum(dim=0)  # [128, 128, 128], suma de canales\n",
    "        label_class = torch.zeros_like(label_sum, dtype=torch.long)  # [128, 128, 128]\n",
    "        \n",
    "        # Asignar clases:\n",
    "        # - Fondo (0, 0) -> 0\n",
    "        # - Vasogénico (1, 0) -> 1\n",
    "        # - Infiltrado (0, 1) -> 2\n",
    "        label_class[label[1] == 1] = 2  # Infiltrado\n",
    "        label_class[(label[0] == 1) & (label[1] == 0)] = 1  # Vasogénico\n",
    "        # Donde label_sum == 0, ya es fondo (0)\n",
    "        \n",
    "        label = label_class.cpu().numpy()  # [128, 128, 128]\n",
    "        print(\"label\", label.shape)\n",
    "        \n",
    "        # Obtener embeddings\n",
    "        _ = model(image)  # Ejecuta el forward para activar el hook\n",
    "        \n",
    "        print(\"decoder_features:\", decoder_features.shape)  # [1, 48, 128, 128, 128]\n",
    "        \n",
    "        # Guardar embeddings y etiquetas\n",
    "        np.save(f\"{embedding_dir}/case_{idx}.npy\", decoder_features.cpu().numpy())\n",
    "        np.save(f\"{label_output_dir}/case_{idx}.npy\", label)\n",
    "        \n",
    "        print(f\"Guardado embeddings y etiquetas para caso {idx}\")\n",
    "\n",
    "# Remover el hook\n",
    "hook_handle_decoder.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar modelo contrastivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/training2/miniconda3/envs/monai_env/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, embedding_dir, label_dir):\n",
    "        self.embedding_dir = embedding_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.case_files = [f for f in os.listdir(embedding_dir) if f.endswith(\".npy\")]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.case_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        embedding_path = os.path.join(self.embedding_dir, f\"case_{idx}.npy\")\n",
    "        label_path = os.path.join(self.label_dir, f\"case_{idx}.npy\")\n",
    "        \n",
    "        embeddings = np.load(embedding_path)\n",
    "        labels = np.load(label_path)\n",
    "        \n",
    "        embeddings = torch.tensor(embeddings, dtype=torch.float32).squeeze(0)\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "        return embeddings, labels\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, input_dim=48, hidden_dim=128, output_dim=128):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def contrastive_loss(z, labels, temperature=0.5, sample_size_per_class=1024):\n",
    "    N_total = z.shape[0]\n",
    "    z = F.normalize(z, dim=1)\n",
    "    \n",
    "    classes = torch.unique(labels)\n",
    "    if len(classes) < 2:\n",
    "        print(f\"Advertencia: Solo una clase presente ({classes.tolist()}), devolviendo pérdida 0\")\n",
    "        return torch.tensor(0.0, device=device, requires_grad=True)\n",
    "    \n",
    "    sampled_z = []\n",
    "    sampled_labels = []\n",
    "    \n",
    "    for cls in classes:\n",
    "        cls_indices = (labels == cls).nonzero(as_tuple=True)[0]\n",
    "        cls_size = cls_indices.shape[0]\n",
    "        if cls_size > sample_size_per_class:\n",
    "            indices = torch.randperm(cls_size)[:sample_size_per_class]\n",
    "            cls_indices = cls_indices[indices]\n",
    "        sampled_z.append(z[cls_indices])\n",
    "        sampled_labels.append(labels[cls_indices])\n",
    "    \n",
    "    z = torch.cat(sampled_z, dim=0)\n",
    "    labels = torch.cat(sampled_labels, dim=0)\n",
    "    N = z.shape[0]\n",
    "    \n",
    "    # print(f\"Batch size: {N}, Unique labels: {torch.unique(labels).tolist()}\")\n",
    "    \n",
    "    if N < 2:\n",
    "        print(\"Advertencia: Batch con menos de 2 vóxeles, devolviendo pérdida 0\")\n",
    "        return torch.tensor(0.0, device=device, requires_grad=True)\n",
    "    \n",
    "    similarity = torch.mm(z, z.T) / temperature\n",
    "    labels_eq = labels.unsqueeze(1) == labels.unsqueeze(0)\n",
    "    labels_eq = labels_eq.float()\n",
    "    eye = torch.eye(N, device=device)\n",
    "    labels_eq = labels_eq * (1 - eye)\n",
    "    \n",
    "    exp_sim = torch.exp(similarity)\n",
    "    pos_sum = (exp_sim * labels_eq).sum(dim=1)\n",
    "    neg_sum = exp_sim.sum(dim=1) - exp_sim.diag()\n",
    "    \n",
    "    if pos_sum.sum() == 0:\n",
    "        print(\"Advertencia: No hay pares positivos, pérdida será 0\")\n",
    "    \n",
    "    loss = -torch.log((pos_sum + 1e-6) / (neg_sum + 1e-6))\n",
    "    return loss.mean()\n",
    "\n",
    "# Configuración\n",
    "embedding_dir = \"Dataset/contrastive_voxel_wise/embeddings\"\n",
    "label_dir = \"Dataset/contrastive_voxel_wise/labels\"\n",
    "batch_size = 1\n",
    "sample_size_per_class = 4096\n",
    "temperature = 0.5\n",
    "num_epochs = 100\n",
    "patience = 10  # Early stopping\n",
    "\n",
    "dataset = EmbeddingDataset(embedding_dir, label_dir)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "model = ProjectionHead(input_dim=48).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Batch 0/30, Loss: 0.6172\n",
      "Epoch 1/100, Batch 5/30, Loss: 0.5817\n",
      "Epoch 1/100, Batch 10/30, Loss: 0.5196\n",
      "Epoch 1/100, Batch 15/30, Loss: 0.5190\n",
      "Epoch 1/100, Batch 20/30, Loss: 0.5328\n",
      "Epoch 1/100, Batch 25/30, Loss: 0.5632\n",
      "Epoch 1/100, Average Loss: 0.4744, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4744\n",
      "Epoch 2/100, Batch 0/30, Loss: 0.4523\n",
      "Epoch 2/100, Batch 5/30, Loss: 0.3414\n",
      "Epoch 2/100, Batch 10/30, Loss: 0.5353\n",
      "Epoch 2/100, Batch 15/30, Loss: 0.5268\n",
      "Epoch 2/100, Batch 20/30, Loss: 0.5174\n",
      "Epoch 2/100, Batch 25/30, Loss: 0.5479\n",
      "Epoch 2/100, Average Loss: 0.4327, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4327\n",
      "Epoch 3/100, Batch 0/30, Loss: 0.5190\n",
      "Epoch 3/100, Batch 5/30, Loss: 0.4881\n",
      "Epoch 3/100, Batch 10/30, Loss: 0.5118\n",
      "Epoch 3/100, Batch 15/30, Loss: 0.3847\n",
      "Epoch 3/100, Batch 20/30, Loss: 0.5156\n",
      "Epoch 3/100, Batch 25/30, Loss: 0.4373\n",
      "Epoch 3/100, Average Loss: 0.4244, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4244\n",
      "Epoch 4/100, Batch 0/30, Loss: 0.5304\n",
      "Epoch 4/100, Batch 5/30, Loss: 0.0639\n",
      "Epoch 4/100, Batch 10/30, Loss: 0.4985\n",
      "Epoch 4/100, Batch 15/30, Loss: 0.2047\n",
      "Epoch 4/100, Batch 20/30, Loss: 0.4839\n",
      "Epoch 4/100, Batch 25/30, Loss: 0.4946\n",
      "Epoch 4/100, Average Loss: 0.4188, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4188\n",
      "Epoch 5/100, Batch 0/30, Loss: 0.4948\n",
      "Epoch 5/100, Batch 5/30, Loss: 0.0632\n",
      "Epoch 5/100, Batch 10/30, Loss: 0.4851\n",
      "Epoch 5/100, Batch 15/30, Loss: 0.5337\n",
      "Epoch 5/100, Batch 20/30, Loss: 0.2072\n",
      "Epoch 5/100, Batch 25/30, Loss: 0.5240\n",
      "Epoch 5/100, Average Loss: 0.4165, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4165\n",
      "Epoch 6/100, Batch 0/30, Loss: 0.4959\n",
      "Epoch 6/100, Batch 5/30, Loss: 0.2133\n",
      "Epoch 6/100, Batch 10/30, Loss: 0.4271\n",
      "Epoch 6/100, Batch 15/30, Loss: 0.4375\n",
      "Epoch 6/100, Batch 20/30, Loss: 0.0899\n",
      "Epoch 6/100, Batch 25/30, Loss: 0.4785\n",
      "Epoch 6/100, Average Loss: 0.4148, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4148\n",
      "Epoch 7/100, Batch 0/30, Loss: 0.5084\n",
      "Epoch 7/100, Batch 5/30, Loss: 0.5239\n",
      "Epoch 7/100, Batch 10/30, Loss: 0.2303\n",
      "Epoch 7/100, Batch 15/30, Loss: 0.4964\n",
      "Epoch 7/100, Batch 20/30, Loss: 0.0922\n",
      "Epoch 7/100, Batch 25/30, Loss: 0.5254\n",
      "Epoch 7/100, Average Loss: 0.4136, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4136\n",
      "Epoch 8/100, Batch 0/30, Loss: 0.3543\n",
      "Epoch 8/100, Batch 5/30, Loss: 0.4828\n",
      "Epoch 8/100, Batch 10/30, Loss: 0.5256\n",
      "Epoch 8/100, Batch 15/30, Loss: 0.3112\n",
      "Epoch 8/100, Batch 20/30, Loss: 0.5270\n",
      "Epoch 8/100, Batch 25/30, Loss: 0.2199\n",
      "Epoch 8/100, Average Loss: 0.4130, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4130\n",
      "Epoch 9/100, Batch 0/30, Loss: 0.4597\n",
      "Epoch 9/100, Batch 5/30, Loss: 0.3647\n",
      "Epoch 9/100, Batch 10/30, Loss: 0.4846\n",
      "Epoch 9/100, Batch 15/30, Loss: 0.4786\n",
      "Epoch 9/100, Batch 20/30, Loss: 0.0702\n",
      "Epoch 9/100, Batch 25/30, Loss: 0.4789\n",
      "Epoch 9/100, Average Loss: 0.4137, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Épocas sin mejora: 1/10\n",
      "Epoch 10/100, Batch 0/30, Loss: 0.5234\n",
      "Epoch 10/100, Batch 5/30, Loss: 0.4784\n",
      "Epoch 10/100, Batch 10/30, Loss: 0.4684\n",
      "Epoch 10/100, Batch 15/30, Loss: 0.0875\n",
      "Epoch 10/100, Batch 20/30, Loss: 0.5060\n",
      "Epoch 10/100, Batch 25/30, Loss: 0.3506\n",
      "Epoch 10/100, Average Loss: 0.4122, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4122\n",
      "Epoch 11/100, Batch 0/30, Loss: 0.4930\n",
      "Epoch 11/100, Batch 5/30, Loss: 0.4204\n",
      "Epoch 11/100, Batch 10/30, Loss: 0.4544\n",
      "Epoch 11/100, Batch 15/30, Loss: 0.4576\n",
      "Epoch 11/100, Batch 20/30, Loss: 0.1964\n",
      "Epoch 11/100, Batch 25/30, Loss: 0.5157\n",
      "Epoch 11/100, Average Loss: 0.4106, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4106\n",
      "Epoch 12/100, Batch 0/30, Loss: 0.4696\n",
      "Epoch 12/100, Batch 5/30, Loss: 0.2014\n",
      "Epoch 12/100, Batch 10/30, Loss: 0.5264\n",
      "Epoch 12/100, Batch 15/30, Loss: 0.0929\n",
      "Epoch 12/100, Batch 20/30, Loss: 0.4892\n",
      "Epoch 12/100, Batch 25/30, Loss: 0.4510\n",
      "Epoch 12/100, Average Loss: 0.4109, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Épocas sin mejora: 1/10\n",
      "Epoch 13/100, Batch 0/30, Loss: 0.0897\n",
      "Epoch 13/100, Batch 5/30, Loss: 0.4558\n",
      "Epoch 13/100, Batch 10/30, Loss: 0.4506\n",
      "Epoch 13/100, Batch 15/30, Loss: 0.1016\n",
      "Epoch 13/100, Batch 20/30, Loss: 0.4806\n",
      "Epoch 13/100, Batch 25/30, Loss: 0.4332\n",
      "Epoch 13/100, Average Loss: 0.4111, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Épocas sin mejora: 2/10\n",
      "Epoch 14/100, Batch 0/30, Loss: 0.3496\n",
      "Epoch 14/100, Batch 5/30, Loss: 0.4772\n",
      "Epoch 14/100, Batch 10/30, Loss: 0.5150\n",
      "Epoch 14/100, Batch 15/30, Loss: 0.4583\n",
      "Epoch 14/100, Batch 20/30, Loss: 0.0847\n",
      "Epoch 14/100, Batch 25/30, Loss: 0.1919\n",
      "Epoch 14/100, Average Loss: 0.4098, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4098\n",
      "Epoch 15/100, Batch 0/30, Loss: 0.5073\n",
      "Epoch 15/100, Batch 5/30, Loss: 0.4844\n",
      "Epoch 15/100, Batch 10/30, Loss: 0.5231\n",
      "Epoch 15/100, Batch 15/30, Loss: 0.4517\n",
      "Epoch 15/100, Batch 20/30, Loss: 0.4216\n",
      "Epoch 15/100, Batch 25/30, Loss: 0.5202\n",
      "Epoch 15/100, Average Loss: 0.4092, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4092\n",
      "Epoch 16/100, Batch 0/30, Loss: 0.5092\n",
      "Epoch 16/100, Batch 5/30, Loss: 0.0988\n",
      "Epoch 16/100, Batch 10/30, Loss: 0.4277\n",
      "Epoch 16/100, Batch 15/30, Loss: 0.0893\n",
      "Epoch 16/100, Batch 20/30, Loss: 0.4726\n",
      "Epoch 16/100, Batch 25/30, Loss: 0.4998\n",
      "Epoch 16/100, Average Loss: 0.4102, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Épocas sin mejora: 1/10\n",
      "Epoch 17/100, Batch 0/30, Loss: 0.4256\n",
      "Epoch 17/100, Batch 5/30, Loss: 0.3111\n",
      "Epoch 17/100, Batch 10/30, Loss: 0.2131\n",
      "Epoch 17/100, Batch 15/30, Loss: 0.4491\n",
      "Epoch 17/100, Batch 20/30, Loss: 0.5094\n",
      "Epoch 17/100, Batch 25/30, Loss: 0.5225\n",
      "Epoch 17/100, Average Loss: 0.4104, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Épocas sin mejora: 2/10\n",
      "Epoch 18/100, Batch 0/30, Loss: 0.4525\n",
      "Epoch 18/100, Batch 5/30, Loss: 0.4109\n",
      "Epoch 18/100, Batch 10/30, Loss: 0.2053\n",
      "Epoch 18/100, Batch 15/30, Loss: 0.3512\n",
      "Epoch 18/100, Batch 20/30, Loss: 0.5099\n",
      "Epoch 18/100, Batch 25/30, Loss: 0.2328\n",
      "Epoch 18/100, Average Loss: 0.4083, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4083\n",
      "Epoch 19/100, Batch 0/30, Loss: 0.2251\n",
      "Epoch 19/100, Batch 5/30, Loss: 0.5112\n",
      "Epoch 19/100, Batch 10/30, Loss: 0.4853\n",
      "Epoch 19/100, Batch 15/30, Loss: 0.5199\n",
      "Epoch 19/100, Batch 20/30, Loss: 0.4778\n",
      "Epoch 19/100, Batch 25/30, Loss: 0.4666\n",
      "Epoch 19/100, Average Loss: 0.4090, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Épocas sin mejora: 1/10\n",
      "Epoch 20/100, Batch 0/30, Loss: 0.4616\n",
      "Epoch 20/100, Batch 5/30, Loss: 0.4538\n",
      "Epoch 20/100, Batch 10/30, Loss: 0.5080\n",
      "Epoch 20/100, Batch 15/30, Loss: 0.1957\n",
      "Epoch 20/100, Batch 20/30, Loss: 0.4191\n",
      "Epoch 20/100, Batch 25/30, Loss: 0.2292\n",
      "Epoch 20/100, Average Loss: 0.4072, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4072\n",
      "Epoch 21/100, Batch 0/30, Loss: 0.5253\n",
      "Epoch 21/100, Batch 5/30, Loss: 0.4465\n",
      "Epoch 21/100, Batch 10/30, Loss: 0.0951\n",
      "Epoch 21/100, Batch 15/30, Loss: 0.5354\n",
      "Epoch 21/100, Batch 20/30, Loss: 0.5143\n",
      "Epoch 21/100, Batch 25/30, Loss: 0.4997\n",
      "Epoch 21/100, Average Loss: 0.4091, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Épocas sin mejora: 1/10\n",
      "Epoch 22/100, Batch 0/30, Loss: 0.4681\n",
      "Epoch 22/100, Batch 5/30, Loss: 0.5207\n",
      "Epoch 22/100, Batch 10/30, Loss: 0.4738\n",
      "Epoch 22/100, Batch 15/30, Loss: 0.3558\n",
      "Epoch 22/100, Batch 20/30, Loss: 0.4844\n",
      "Epoch 22/100, Batch 25/30, Loss: 0.2313\n",
      "Epoch 22/100, Average Loss: 0.4088, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Épocas sin mejora: 2/10\n",
      "Epoch 23/100, Batch 0/30, Loss: 0.5289\n",
      "Epoch 23/100, Batch 5/30, Loss: 0.4683\n",
      "Epoch 23/100, Batch 10/30, Loss: 0.0902\n",
      "Epoch 23/100, Batch 15/30, Loss: 0.0654\n",
      "Epoch 23/100, Batch 20/30, Loss: 0.4978\n",
      "Epoch 23/100, Batch 25/30, Loss: 0.4295\n",
      "Epoch 23/100, Average Loss: 0.4076, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Épocas sin mejora: 3/10\n",
      "Epoch 24/100, Batch 0/30, Loss: 0.4281\n",
      "Epoch 24/100, Batch 5/30, Loss: 0.0897\n",
      "Epoch 24/100, Batch 10/30, Loss: 0.4751\n",
      "Epoch 24/100, Batch 15/30, Loss: 0.1988\n",
      "Epoch 24/100, Batch 20/30, Loss: 0.5175\n",
      "Epoch 24/100, Batch 25/30, Loss: 0.4461\n",
      "Epoch 24/100, Average Loss: 0.4070, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4070\n",
      "Epoch 25/100, Batch 0/30, Loss: 0.4825\n",
      "Epoch 25/100, Batch 5/30, Loss: 0.4456\n",
      "Epoch 25/100, Batch 10/30, Loss: 0.4231\n",
      "Epoch 25/100, Batch 15/30, Loss: 0.2103\n",
      "Epoch 25/100, Batch 20/30, Loss: 0.3154\n",
      "Epoch 25/100, Batch 25/30, Loss: 0.4480\n",
      "Epoch 25/100, Average Loss: 0.4059, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4059\n",
      "Epoch 26/100, Batch 0/30, Loss: 0.5060\n",
      "Epoch 26/100, Batch 5/30, Loss: 0.4266\n",
      "Epoch 26/100, Batch 10/30, Loss: 0.4482\n",
      "Epoch 26/100, Batch 15/30, Loss: 0.0671\n",
      "Epoch 26/100, Batch 20/30, Loss: 0.4890\n",
      "Epoch 26/100, Batch 25/30, Loss: 0.5284\n",
      "Epoch 26/100, Average Loss: 0.4068, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Épocas sin mejora: 1/10\n",
      "Epoch 27/100, Batch 0/30, Loss: 0.1943\n",
      "Epoch 27/100, Batch 5/30, Loss: 0.5037\n",
      "Epoch 27/100, Batch 10/30, Loss: 0.5146\n",
      "Epoch 27/100, Batch 15/30, Loss: 0.5059\n",
      "Epoch 27/100, Batch 20/30, Loss: 0.4825\n",
      "Epoch 27/100, Batch 25/30, Loss: 0.4547\n",
      "Epoch 27/100, Average Loss: 0.4058, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4058\n",
      "Epoch 28/100, Batch 0/30, Loss: 0.4943\n",
      "Epoch 28/100, Batch 5/30, Loss: 0.4808\n",
      "Epoch 28/100, Batch 10/30, Loss: 0.0665\n",
      "Epoch 28/100, Batch 15/30, Loss: 0.4476\n",
      "Epoch 28/100, Batch 20/30, Loss: 0.3083\n",
      "Epoch 28/100, Batch 25/30, Loss: 0.5040\n",
      "Epoch 28/100, Average Loss: 0.4072, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Épocas sin mejora: 1/10\n",
      "Epoch 29/100, Batch 0/30, Loss: 0.4958\n",
      "Epoch 29/100, Batch 5/30, Loss: 0.4719\n",
      "Epoch 29/100, Batch 10/30, Loss: 0.5119\n",
      "Epoch 29/100, Batch 15/30, Loss: 0.4507\n",
      "Epoch 29/100, Batch 20/30, Loss: 0.3528\n",
      "Epoch 29/100, Batch 25/30, Loss: 0.4648\n",
      "Epoch 29/100, Average Loss: 0.4071, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Épocas sin mejora: 2/10\n",
      "Epoch 30/100, Batch 0/30, Loss: 0.3429\n",
      "Epoch 30/100, Batch 5/30, Loss: 0.4776\n",
      "Epoch 30/100, Batch 10/30, Loss: 0.4642\n",
      "Epoch 30/100, Batch 15/30, Loss: 0.4562\n",
      "Epoch 30/100, Batch 20/30, Loss: 0.5296\n",
      "Epoch 30/100, Batch 25/30, Loss: 0.5022\n",
      "Epoch 30/100, Average Loss: 0.4072, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Épocas sin mejora: 3/10\n",
      "Epoch 31/100, Batch 0/30, Loss: 0.4834\n",
      "Epoch 31/100, Batch 5/30, Loss: 0.5045\n",
      "Epoch 31/100, Batch 10/30, Loss: 0.4856\n",
      "Epoch 31/100, Batch 15/30, Loss: 0.5111\n",
      "Epoch 31/100, Batch 20/30, Loss: 0.4730\n",
      "Epoch 31/100, Batch 25/30, Loss: 0.5241\n",
      "Epoch 31/100, Average Loss: 0.4064, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Épocas sin mejora: 4/10\n",
      "Epoch 32/100, Batch 0/30, Loss: 0.0946\n",
      "Epoch 32/100, Batch 5/30, Loss: 0.5036\n",
      "Epoch 32/100, Batch 10/30, Loss: 0.2252\n",
      "Epoch 32/100, Batch 15/30, Loss: 0.4546\n",
      "Epoch 32/100, Batch 20/30, Loss: 0.4438\n",
      "Epoch 32/100, Batch 25/30, Loss: 0.5059\n",
      "Epoch 32/100, Average Loss: 0.4041, Valid Batches: 30/30, Learning Rate: 0.000500\n",
      "Guardado checkpoint con mejor pérdida: 0.4041\n",
      "Epoch 33/100, Batch 0/30, Loss: 0.4710\n",
      "Epoch 33/100, Batch 5/30, Loss: 0.5173\n",
      "Epoch 33/100, Batch 10/30, Loss: 0.5260\n",
      "Epoch 33/100, Batch 15/30, Loss: 0.5090\n",
      "Epoch 33/100, Batch 20/30, Loss: 0.4699\n",
      "Epoch 33/100, Batch 25/30, Loss: 0.0885\n",
      "Epoch 33/100, Average Loss: 0.4047, Valid Batches: 30/30, Learning Rate: 0.000500\n",
      "Épocas sin mejora: 1/10\n",
      "Epoch 34/100, Batch 0/30, Loss: 0.3003\n",
      "Epoch 34/100, Batch 5/30, Loss: 0.5250\n",
      "Epoch 34/100, Batch 10/30, Loss: 0.5146\n",
      "Epoch 34/100, Batch 15/30, Loss: 0.4281\n",
      "Epoch 34/100, Batch 20/30, Loss: 0.2049\n",
      "Epoch 34/100, Batch 25/30, Loss: 0.5042\n",
      "Epoch 34/100, Average Loss: 0.4042, Valid Batches: 30/30, Learning Rate: 0.000500\n",
      "Épocas sin mejora: 2/10\n",
      "Epoch 35/100, Batch 0/30, Loss: 0.5026\n",
      "Epoch 35/100, Batch 5/30, Loss: 0.4089\n",
      "Epoch 35/100, Batch 10/30, Loss: 0.4904\n",
      "Epoch 35/100, Batch 15/30, Loss: 0.4651\n",
      "Epoch 35/100, Batch 20/30, Loss: 0.0624\n",
      "Epoch 35/100, Batch 25/30, Loss: 0.3507\n",
      "Epoch 35/100, Average Loss: 0.4047, Valid Batches: 30/30, Learning Rate: 0.000500\n",
      "Épocas sin mejora: 3/10\n",
      "Epoch 36/100, Batch 0/30, Loss: 0.0815\n",
      "Epoch 36/100, Batch 5/30, Loss: 0.0623\n",
      "Epoch 36/100, Batch 10/30, Loss: 0.4549\n",
      "Epoch 36/100, Batch 15/30, Loss: 0.4775\n",
      "Epoch 36/100, Batch 20/30, Loss: 0.5231\n",
      "Epoch 36/100, Batch 25/30, Loss: 0.4200\n",
      "Epoch 36/100, Average Loss: 0.4037, Valid Batches: 30/30, Learning Rate: 0.000500\n",
      "Guardado checkpoint con mejor pérdida: 0.4037\n",
      "Epoch 37/100, Batch 0/30, Loss: 0.2237\n",
      "Epoch 37/100, Batch 5/30, Loss: 0.5004\n",
      "Epoch 37/100, Batch 10/30, Loss: 0.4590\n",
      "Epoch 37/100, Batch 15/30, Loss: 0.5126\n",
      "Epoch 37/100, Batch 20/30, Loss: 0.0818\n",
      "Epoch 37/100, Batch 25/30, Loss: 0.5031\n",
      "Epoch 37/100, Average Loss: 0.4030, Valid Batches: 30/30, Learning Rate: 0.000500\n",
      "Guardado checkpoint con mejor pérdida: 0.4030\n",
      "Epoch 38/100, Batch 0/30, Loss: 0.4915\n",
      "Epoch 38/100, Batch 5/30, Loss: 0.5045\n",
      "Epoch 38/100, Batch 10/30, Loss: 0.5315\n",
      "Epoch 38/100, Batch 15/30, Loss: 0.4611\n",
      "Epoch 38/100, Batch 20/30, Loss: 0.4708\n",
      "Epoch 38/100, Batch 25/30, Loss: 0.2271\n",
      "Epoch 38/100, Average Loss: 0.4036, Valid Batches: 30/30, Learning Rate: 0.000500\n",
      "Épocas sin mejora: 1/10\n",
      "Epoch 39/100, Batch 0/30, Loss: 0.5314\n",
      "Epoch 39/100, Batch 5/30, Loss: 0.5035\n",
      "Epoch 39/100, Batch 10/30, Loss: 0.5108\n",
      "Epoch 39/100, Batch 15/30, Loss: 0.4745\n",
      "Epoch 39/100, Batch 20/30, Loss: 0.3455\n",
      "Epoch 39/100, Batch 25/30, Loss: 0.3087\n",
      "Epoch 39/100, Average Loss: 0.4043, Valid Batches: 30/30, Learning Rate: 0.000500\n",
      "Épocas sin mejora: 2/10\n",
      "Epoch 40/100, Batch 0/30, Loss: 0.4761\n",
      "Epoch 40/100, Batch 5/30, Loss: 0.4570\n",
      "Epoch 40/100, Batch 10/30, Loss: 0.5046\n",
      "Epoch 40/100, Batch 15/30, Loss: 0.5026\n",
      "Epoch 40/100, Batch 20/30, Loss: 0.5268\n",
      "Epoch 40/100, Batch 25/30, Loss: 0.5152\n",
      "Epoch 40/100, Average Loss: 0.4027, Valid Batches: 30/30, Learning Rate: 0.000500\n",
      "Guardado checkpoint con mejor pérdida: 0.4027\n",
      "Epoch 41/100, Batch 0/30, Loss: 0.0885\n",
      "Epoch 41/100, Batch 5/30, Loss: 0.4691\n",
      "Epoch 41/100, Batch 10/30, Loss: 0.4912\n",
      "Epoch 41/100, Batch 15/30, Loss: 0.4995\n",
      "Epoch 41/100, Batch 20/30, Loss: 0.5041\n",
      "Epoch 41/100, Batch 25/30, Loss: 0.2059\n",
      "Epoch 41/100, Average Loss: 0.4037, Valid Batches: 30/30, Learning Rate: 0.000500\n",
      "Épocas sin mejora: 1/10\n",
      "Epoch 42/100, Batch 0/30, Loss: 0.3987\n",
      "Epoch 42/100, Batch 5/30, Loss: 0.5085\n",
      "Epoch 42/100, Batch 10/30, Loss: 0.2298\n",
      "Epoch 42/100, Batch 15/30, Loss: 0.4484\n",
      "Epoch 42/100, Batch 20/30, Loss: 0.5114\n",
      "Epoch 42/100, Batch 25/30, Loss: 0.5055\n",
      "Epoch 42/100, Average Loss: 0.4027, Valid Batches: 30/30, Learning Rate: 0.000500\n",
      "Guardado checkpoint con mejor pérdida: 0.4027\n",
      "Epoch 43/100, Batch 0/30, Loss: 0.4440\n",
      "Epoch 43/100, Batch 5/30, Loss: 0.5063\n",
      "Epoch 43/100, Batch 10/30, Loss: 0.4203\n",
      "Epoch 43/100, Batch 15/30, Loss: 0.5260\n",
      "Epoch 43/100, Batch 20/30, Loss: 0.5065\n",
      "Epoch 43/100, Batch 25/30, Loss: 0.3017\n",
      "Epoch 43/100, Average Loss: 0.4025, Valid Batches: 30/30, Learning Rate: 0.000500\n",
      "Guardado checkpoint con mejor pérdida: 0.4025\n",
      "Epoch 44/100, Batch 0/30, Loss: 0.4457\n",
      "Epoch 44/100, Batch 5/30, Loss: 0.4221\n",
      "Epoch 44/100, Batch 10/30, Loss: 0.5029\n",
      "Epoch 44/100, Batch 15/30, Loss: 0.4889\n",
      "Epoch 44/100, Batch 20/30, Loss: 0.5140\n",
      "Epoch 44/100, Batch 25/30, Loss: 0.1990\n",
      "Epoch 44/100, Average Loss: 0.4029, Valid Batches: 30/30, Learning Rate: 0.000500\n",
      "Épocas sin mejora: 1/10\n",
      "Epoch 45/100, Batch 0/30, Loss: 0.5220\n",
      "Epoch 45/100, Batch 5/30, Loss: 0.5278\n",
      "Epoch 45/100, Batch 10/30, Loss: 0.0870\n",
      "Epoch 45/100, Batch 15/30, Loss: 0.4736\n",
      "Epoch 45/100, Batch 20/30, Loss: 0.0662\n",
      "Epoch 45/100, Batch 25/30, Loss: 0.0788\n",
      "Epoch 45/100, Average Loss: 0.4024, Valid Batches: 30/30, Learning Rate: 0.000500\n",
      "Guardado checkpoint con mejor pérdida: 0.4024\n",
      "Epoch 46/100, Batch 0/30, Loss: 0.4444\n",
      "Epoch 46/100, Batch 5/30, Loss: 0.0779\n",
      "Epoch 46/100, Batch 10/30, Loss: 0.4398\n",
      "Epoch 46/100, Batch 15/30, Loss: 0.5176\n",
      "Epoch 46/100, Batch 20/30, Loss: 0.4858\n",
      "Epoch 46/100, Batch 25/30, Loss: 0.5144\n",
      "Epoch 46/100, Average Loss: 0.4028, Valid Batches: 30/30, Learning Rate: 0.000500\n",
      "Épocas sin mejora: 1/10\n",
      "Epoch 47/100, Batch 0/30, Loss: 0.1936\n",
      "Epoch 47/100, Batch 5/30, Loss: 0.0793\n",
      "Epoch 47/100, Batch 10/30, Loss: 0.5123\n",
      "Epoch 47/100, Batch 15/30, Loss: 0.2272\n",
      "Epoch 47/100, Batch 20/30, Loss: 0.3462\n",
      "Epoch 47/100, Batch 25/30, Loss: 0.4460\n",
      "Epoch 47/100, Average Loss: 0.4021, Valid Batches: 30/30, Learning Rate: 0.000500\n",
      "Guardado checkpoint con mejor pérdida: 0.4021\n",
      "Epoch 48/100, Batch 0/30, Loss: 0.4338\n",
      "Epoch 48/100, Batch 5/30, Loss: 0.5027\n",
      "Epoch 48/100, Batch 10/30, Loss: 0.2009\n",
      "Epoch 48/100, Batch 15/30, Loss: 0.2022\n",
      "Epoch 48/100, Batch 20/30, Loss: 0.0869\n",
      "Epoch 48/100, Batch 25/30, Loss: 0.5205\n",
      "Epoch 48/100, Average Loss: 0.4019, Valid Batches: 30/30, Learning Rate: 0.000500\n",
      "Guardado checkpoint con mejor pérdida: 0.4019\n",
      "Epoch 49/100, Batch 0/30, Loss: 0.1932\n",
      "Epoch 49/100, Batch 5/30, Loss: 0.4168\n",
      "Epoch 49/100, Batch 10/30, Loss: 0.4418\n",
      "Epoch 49/100, Batch 15/30, Loss: 0.0857\n",
      "Epoch 49/100, Batch 20/30, Loss: 0.0805\n",
      "Epoch 49/100, Batch 25/30, Loss: 0.4978\n",
      "Epoch 49/100, Average Loss: 0.4022, Valid Batches: 30/30, Learning Rate: 0.000500\n",
      "Épocas sin mejora: 1/10\n",
      "Epoch 50/100, Batch 0/30, Loss: 0.5006\n",
      "Epoch 50/100, Batch 5/30, Loss: 0.5218\n",
      "Epoch 50/100, Batch 10/30, Loss: 0.5111\n",
      "Epoch 50/100, Batch 15/30, Loss: 0.0653\n",
      "Epoch 50/100, Batch 20/30, Loss: 0.5125\n",
      "Epoch 50/100, Batch 25/30, Loss: 0.5257\n",
      "Epoch 50/100, Average Loss: 0.4041, Valid Batches: 30/30, Learning Rate: 0.000500\n",
      "Épocas sin mejora: 2/10\n",
      "Epoch 51/100, Batch 0/30, Loss: 0.5035\n",
      "Epoch 51/100, Batch 5/30, Loss: 0.5240\n",
      "Epoch 51/100, Batch 10/30, Loss: 0.5284\n",
      "Epoch 51/100, Batch 15/30, Loss: 0.4542\n",
      "Epoch 51/100, Batch 20/30, Loss: 0.4819\n",
      "Epoch 51/100, Batch 25/30, Loss: 0.3381\n",
      "Epoch 51/100, Average Loss: 0.4028, Valid Batches: 30/30, Learning Rate: 0.000500\n",
      "Épocas sin mejora: 3/10\n",
      "Epoch 52/100, Batch 0/30, Loss: 0.4204\n",
      "Epoch 52/100, Batch 5/30, Loss: 0.4777\n",
      "Epoch 52/100, Batch 10/30, Loss: 0.2260\n",
      "Epoch 52/100, Batch 15/30, Loss: 0.4369\n",
      "Epoch 52/100, Batch 20/30, Loss: 0.5194\n",
      "Epoch 52/100, Batch 25/30, Loss: 0.4956\n",
      "Epoch 52/100, Average Loss: 0.4011, Valid Batches: 30/30, Learning Rate: 0.000500\n",
      "Guardado checkpoint con mejor pérdida: 0.4011\n",
      "Epoch 53/100, Batch 0/30, Loss: 0.4458\n",
      "Epoch 53/100, Batch 5/30, Loss: 0.5227\n",
      "Epoch 53/100, Batch 10/30, Loss: 0.4421\n",
      "Epoch 53/100, Batch 15/30, Loss: 0.4829\n",
      "Epoch 53/100, Batch 20/30, Loss: 0.5241\n",
      "Epoch 53/100, Batch 25/30, Loss: 0.4400\n",
      "Epoch 53/100, Average Loss: 0.4020, Valid Batches: 30/30, Learning Rate: 0.000500\n",
      "Épocas sin mejora: 1/10\n",
      "Epoch 54/100, Batch 0/30, Loss: 0.2254\n",
      "Epoch 54/100, Batch 5/30, Loss: 0.4396\n",
      "Epoch 54/100, Batch 10/30, Loss: 0.4656\n",
      "Epoch 54/100, Batch 15/30, Loss: 0.4120\n",
      "Epoch 54/100, Batch 20/30, Loss: 0.5206\n",
      "Epoch 54/100, Batch 25/30, Loss: 0.5016\n",
      "Epoch 54/100, Average Loss: 0.4017, Valid Batches: 30/30, Learning Rate: 0.000500\n",
      "Épocas sin mejora: 2/10\n",
      "Epoch 55/100, Batch 0/30, Loss: 0.4678\n",
      "Epoch 55/100, Batch 5/30, Loss: 0.5016\n",
      "Epoch 55/100, Batch 10/30, Loss: 0.4749\n",
      "Epoch 55/100, Batch 15/30, Loss: 0.5152\n",
      "Epoch 55/100, Batch 20/30, Loss: 0.5090\n",
      "Epoch 55/100, Batch 25/30, Loss: 0.5270\n",
      "Epoch 55/100, Average Loss: 0.4020, Valid Batches: 30/30, Learning Rate: 0.000500\n",
      "Épocas sin mejora: 3/10\n",
      "Epoch 56/100, Batch 0/30, Loss: 0.4729\n",
      "Epoch 56/100, Batch 5/30, Loss: 0.4514\n",
      "Epoch 56/100, Batch 10/30, Loss: 0.4830\n",
      "Epoch 56/100, Batch 15/30, Loss: 0.4940\n",
      "Epoch 56/100, Batch 20/30, Loss: 0.5032\n",
      "Epoch 56/100, Batch 25/30, Loss: 0.1989\n",
      "Epoch 56/100, Average Loss: 0.4002, Valid Batches: 30/30, Learning Rate: 0.000500\n",
      "Guardado checkpoint con mejor pérdida: 0.4002\n",
      "Epoch 57/100, Batch 0/30, Loss: 0.5274\n",
      "Epoch 57/100, Batch 5/30, Loss: 0.3858\n",
      "Epoch 57/100, Batch 10/30, Loss: 0.0836\n",
      "Epoch 57/100, Batch 15/30, Loss: 0.4686\n",
      "Epoch 57/100, Batch 20/30, Loss: 0.5390\n",
      "Epoch 57/100, Batch 25/30, Loss: 0.5118\n",
      "Epoch 57/100, Average Loss: 0.4020, Valid Batches: 30/30, Learning Rate: 0.000500\n",
      "Épocas sin mejora: 1/10\n",
      "Epoch 58/100, Batch 0/30, Loss: 0.3987\n",
      "Epoch 58/100, Batch 5/30, Loss: 0.0786\n",
      "Epoch 58/100, Batch 10/30, Loss: 0.4474\n",
      "Epoch 58/100, Batch 15/30, Loss: 0.5003\n",
      "Epoch 58/100, Batch 20/30, Loss: 0.4640\n",
      "Epoch 58/100, Batch 25/30, Loss: 0.2222\n",
      "Epoch 58/100, Average Loss: 0.4009, Valid Batches: 30/30, Learning Rate: 0.000500\n",
      "Épocas sin mejora: 2/10\n",
      "Epoch 59/100, Batch 0/30, Loss: 0.1974\n",
      "Epoch 59/100, Batch 5/30, Loss: 0.5174\n",
      "Epoch 59/100, Batch 10/30, Loss: 0.5010\n",
      "Epoch 59/100, Batch 15/30, Loss: 0.2031\n",
      "Epoch 59/100, Batch 20/30, Loss: 0.2266\n",
      "Epoch 59/100, Batch 25/30, Loss: 0.4153\n",
      "Epoch 59/100, Average Loss: 0.4018, Valid Batches: 30/30, Learning Rate: 0.000500\n",
      "Épocas sin mejora: 3/10\n",
      "Epoch 60/100, Batch 0/30, Loss: 0.4409\n",
      "Epoch 60/100, Batch 5/30, Loss: 0.4516\n",
      "Epoch 60/100, Batch 10/30, Loss: 0.4777\n",
      "Epoch 60/100, Batch 15/30, Loss: 0.2938\n",
      "Epoch 60/100, Batch 20/30, Loss: 0.2021\n",
      "Epoch 60/100, Batch 25/30, Loss: 0.2039\n",
      "Epoch 60/100, Average Loss: 0.4015, Valid Batches: 30/30, Learning Rate: 0.000500\n",
      "Épocas sin mejora: 4/10\n",
      "Epoch 61/100, Batch 0/30, Loss: 0.5273\n",
      "Epoch 61/100, Batch 5/30, Loss: 0.5178\n",
      "Epoch 61/100, Batch 10/30, Loss: 0.4658\n",
      "Epoch 61/100, Batch 15/30, Loss: 0.4428\n",
      "Epoch 61/100, Batch 20/30, Loss: 0.4091\n",
      "Epoch 61/100, Batch 25/30, Loss: 0.4978\n",
      "Epoch 61/100, Average Loss: 0.4002, Valid Batches: 30/30, Learning Rate: 0.000250\n",
      "Épocas sin mejora: 5/10\n",
      "Epoch 62/100, Batch 0/30, Loss: 0.1996\n",
      "Epoch 62/100, Batch 5/30, Loss: 0.5017\n",
      "Epoch 62/100, Batch 10/30, Loss: 0.4841\n",
      "Epoch 62/100, Batch 15/30, Loss: 0.5288\n",
      "Epoch 62/100, Batch 20/30, Loss: 0.0820\n",
      "Epoch 62/100, Batch 25/30, Loss: 0.4968\n",
      "Epoch 62/100, Average Loss: 0.3996, Valid Batches: 30/30, Learning Rate: 0.000250\n",
      "Guardado checkpoint con mejor pérdida: 0.3996\n",
      "Epoch 63/100, Batch 0/30, Loss: 0.4740\n",
      "Epoch 63/100, Batch 5/30, Loss: 0.4308\n",
      "Epoch 63/100, Batch 10/30, Loss: 0.5157\n",
      "Epoch 63/100, Batch 15/30, Loss: 0.5037\n",
      "Epoch 63/100, Batch 20/30, Loss: 0.5075\n",
      "Epoch 63/100, Batch 25/30, Loss: 0.0783\n",
      "Epoch 63/100, Average Loss: 0.4005, Valid Batches: 30/30, Learning Rate: 0.000250\n",
      "Épocas sin mejora: 1/10\n",
      "Epoch 64/100, Batch 0/30, Loss: 0.5154\n",
      "Epoch 64/100, Batch 5/30, Loss: 0.1912\n",
      "Epoch 64/100, Batch 10/30, Loss: 0.4519\n",
      "Epoch 64/100, Batch 15/30, Loss: 0.4641\n",
      "Epoch 64/100, Batch 20/30, Loss: 0.5024\n",
      "Epoch 64/100, Batch 25/30, Loss: 0.4476\n",
      "Epoch 64/100, Average Loss: 0.3995, Valid Batches: 30/30, Learning Rate: 0.000250\n",
      "Guardado checkpoint con mejor pérdida: 0.3995\n",
      "Epoch 65/100, Batch 0/30, Loss: 0.4990\n",
      "Epoch 65/100, Batch 5/30, Loss: 0.4619\n",
      "Epoch 65/100, Batch 10/30, Loss: 0.4438\n",
      "Epoch 65/100, Batch 15/30, Loss: 0.2268\n",
      "Epoch 65/100, Batch 20/30, Loss: 0.3384\n",
      "Epoch 65/100, Batch 25/30, Loss: 0.5211\n",
      "Epoch 65/100, Average Loss: 0.3998, Valid Batches: 30/30, Learning Rate: 0.000250\n",
      "Épocas sin mejora: 1/10\n",
      "Epoch 66/100, Batch 0/30, Loss: 0.4988\n",
      "Epoch 66/100, Batch 5/30, Loss: 0.4688\n",
      "Epoch 66/100, Batch 10/30, Loss: 0.5228\n",
      "Epoch 66/100, Batch 15/30, Loss: 0.4387\n",
      "Epoch 66/100, Batch 20/30, Loss: 0.3417\n",
      "Epoch 66/100, Batch 25/30, Loss: 0.5030\n",
      "Epoch 66/100, Average Loss: 0.4001, Valid Batches: 30/30, Learning Rate: 0.000250\n",
      "Épocas sin mejora: 2/10\n",
      "Epoch 67/100, Batch 0/30, Loss: 0.5099\n",
      "Epoch 67/100, Batch 5/30, Loss: 0.4088\n",
      "Epoch 67/100, Batch 10/30, Loss: 0.2302\n",
      "Epoch 67/100, Batch 15/30, Loss: 0.2096\n",
      "Epoch 67/100, Batch 20/30, Loss: 0.5041\n",
      "Epoch 67/100, Batch 25/30, Loss: 0.4684\n",
      "Epoch 67/100, Average Loss: 0.3992, Valid Batches: 30/30, Learning Rate: 0.000250\n",
      "Guardado checkpoint con mejor pérdida: 0.3992\n",
      "Epoch 68/100, Batch 0/30, Loss: 0.0660\n",
      "Epoch 68/100, Batch 5/30, Loss: 0.5112\n",
      "Epoch 68/100, Batch 10/30, Loss: 0.4569\n",
      "Epoch 68/100, Batch 15/30, Loss: 0.2257\n",
      "Epoch 68/100, Batch 20/30, Loss: 0.2002\n",
      "Epoch 68/100, Batch 25/30, Loss: 0.4977\n",
      "Epoch 68/100, Average Loss: 0.3993, Valid Batches: 30/30, Learning Rate: 0.000250\n",
      "Épocas sin mejora: 1/10\n",
      "Epoch 69/100, Batch 0/30, Loss: 0.2963\n",
      "Epoch 69/100, Batch 5/30, Loss: 0.5223\n",
      "Epoch 69/100, Batch 10/30, Loss: 0.5213\n",
      "Epoch 69/100, Batch 15/30, Loss: 0.5079\n",
      "Epoch 69/100, Batch 20/30, Loss: 0.4173\n",
      "Epoch 69/100, Batch 25/30, Loss: 0.0844\n",
      "Epoch 69/100, Average Loss: 0.4007, Valid Batches: 30/30, Learning Rate: 0.000250\n",
      "Épocas sin mejora: 2/10\n",
      "Epoch 70/100, Batch 0/30, Loss: 0.2283\n",
      "Epoch 70/100, Batch 5/30, Loss: 0.5092\n",
      "Epoch 70/100, Batch 10/30, Loss: 0.4705\n",
      "Epoch 70/100, Batch 15/30, Loss: 0.4495\n",
      "Epoch 70/100, Batch 20/30, Loss: 0.4416\n",
      "Epoch 70/100, Batch 25/30, Loss: 0.5261\n",
      "Epoch 70/100, Average Loss: 0.3994, Valid Batches: 30/30, Learning Rate: 0.000250\n",
      "Épocas sin mejora: 3/10\n",
      "Epoch 71/100, Batch 0/30, Loss: 0.5109\n",
      "Epoch 71/100, Batch 5/30, Loss: 0.4149\n",
      "Epoch 71/100, Batch 10/30, Loss: 0.1971\n",
      "Epoch 71/100, Batch 15/30, Loss: 0.0842\n",
      "Epoch 71/100, Batch 20/30, Loss: 0.5056\n",
      "Epoch 71/100, Batch 25/30, Loss: 0.2268\n",
      "Epoch 71/100, Average Loss: 0.3993, Valid Batches: 30/30, Learning Rate: 0.000250\n",
      "Épocas sin mejora: 4/10\n",
      "Epoch 72/100, Batch 0/30, Loss: 0.1906\n",
      "Epoch 72/100, Batch 5/30, Loss: 0.4777\n",
      "Epoch 72/100, Batch 10/30, Loss: 0.5035\n",
      "Epoch 72/100, Batch 15/30, Loss: 0.3350\n",
      "Epoch 72/100, Batch 20/30, Loss: 0.5261\n",
      "Epoch 72/100, Batch 25/30, Loss: 0.2958\n",
      "Epoch 72/100, Average Loss: 0.3988, Valid Batches: 30/30, Learning Rate: 0.000125\n",
      "Guardado checkpoint con mejor pérdida: 0.3988\n",
      "Epoch 73/100, Batch 0/30, Loss: 0.5188\n",
      "Epoch 73/100, Batch 5/30, Loss: 0.5105\n",
      "Epoch 73/100, Batch 10/30, Loss: 0.5074\n",
      "Epoch 73/100, Batch 15/30, Loss: 0.4499\n",
      "Epoch 73/100, Batch 20/30, Loss: 0.5100\n",
      "Epoch 73/100, Batch 25/30, Loss: 0.5203\n",
      "Epoch 73/100, Average Loss: 0.3985, Valid Batches: 30/30, Learning Rate: 0.000125\n",
      "Guardado checkpoint con mejor pérdida: 0.3985\n",
      "Epoch 74/100, Batch 0/30, Loss: 0.4161\n",
      "Epoch 74/100, Batch 5/30, Loss: 0.4598\n",
      "Epoch 74/100, Batch 10/30, Loss: 0.2924\n",
      "Epoch 74/100, Batch 15/30, Loss: 0.4391\n",
      "Epoch 74/100, Batch 20/30, Loss: 0.5228\n",
      "Epoch 74/100, Batch 25/30, Loss: 0.5151\n",
      "Epoch 74/100, Average Loss: 0.3985, Valid Batches: 30/30, Learning Rate: 0.000125\n",
      "Épocas sin mejora: 1/10\n",
      "Epoch 75/100, Batch 0/30, Loss: 0.2235\n",
      "Epoch 75/100, Batch 5/30, Loss: 0.4372\n",
      "Epoch 75/100, Batch 10/30, Loss: 0.5034\n",
      "Epoch 75/100, Batch 15/30, Loss: 0.5200\n",
      "Epoch 75/100, Batch 20/30, Loss: 0.5012\n",
      "Epoch 75/100, Batch 25/30, Loss: 0.4362\n",
      "Epoch 75/100, Average Loss: 0.3983, Valid Batches: 30/30, Learning Rate: 0.000125\n",
      "Guardado checkpoint con mejor pérdida: 0.3983\n",
      "Epoch 76/100, Batch 0/30, Loss: 0.5022\n",
      "Epoch 76/100, Batch 5/30, Loss: 0.5189\n",
      "Epoch 76/100, Batch 10/30, Loss: 0.5105\n",
      "Epoch 76/100, Batch 15/30, Loss: 0.0821\n",
      "Epoch 76/100, Batch 20/30, Loss: 0.5217\n",
      "Epoch 76/100, Batch 25/30, Loss: 0.4653\n",
      "Epoch 76/100, Average Loss: 0.3988, Valid Batches: 30/30, Learning Rate: 0.000125\n",
      "Épocas sin mejora: 1/10\n",
      "Epoch 77/100, Batch 0/30, Loss: 0.2260\n",
      "Epoch 77/100, Batch 5/30, Loss: 0.4673\n",
      "Epoch 77/100, Batch 10/30, Loss: 0.4350\n",
      "Epoch 77/100, Batch 15/30, Loss: 0.5226\n",
      "Epoch 77/100, Batch 20/30, Loss: 0.4660\n",
      "Epoch 77/100, Batch 25/30, Loss: 0.2055\n",
      "Epoch 77/100, Average Loss: 0.3982, Valid Batches: 30/30, Learning Rate: 0.000125\n",
      "Guardado checkpoint con mejor pérdida: 0.3982\n",
      "Epoch 78/100, Batch 0/30, Loss: 0.4369\n",
      "Epoch 78/100, Batch 5/30, Loss: 0.4682\n",
      "Epoch 78/100, Batch 10/30, Loss: 0.1979\n",
      "Epoch 78/100, Batch 15/30, Loss: 0.4705\n",
      "Epoch 78/100, Batch 20/30, Loss: 0.5154\n",
      "Epoch 78/100, Batch 25/30, Loss: 0.4853\n",
      "Epoch 78/100, Average Loss: 0.3987, Valid Batches: 30/30, Learning Rate: 0.000125\n",
      "Épocas sin mejora: 1/10\n",
      "Epoch 79/100, Batch 0/30, Loss: 0.3359\n",
      "Epoch 79/100, Batch 5/30, Loss: 0.5223\n",
      "Epoch 79/100, Batch 10/30, Loss: 0.4670\n",
      "Epoch 79/100, Batch 15/30, Loss: 0.4989\n",
      "Epoch 79/100, Batch 20/30, Loss: 0.5163\n",
      "Epoch 79/100, Batch 25/30, Loss: 0.4315\n",
      "Epoch 79/100, Average Loss: 0.3989, Valid Batches: 30/30, Learning Rate: 0.000125\n",
      "Épocas sin mejora: 2/10\n",
      "Epoch 80/100, Batch 0/30, Loss: 0.1938\n",
      "Epoch 80/100, Batch 5/30, Loss: 0.4363\n",
      "Epoch 80/100, Batch 10/30, Loss: 0.0774\n",
      "Epoch 80/100, Batch 15/30, Loss: 0.5001\n",
      "Epoch 80/100, Batch 20/30, Loss: 0.4462\n",
      "Epoch 80/100, Batch 25/30, Loss: 0.5053\n",
      "Epoch 80/100, Average Loss: 0.3977, Valid Batches: 30/30, Learning Rate: 0.000063\n",
      "Guardado checkpoint con mejor pérdida: 0.3977\n",
      "Epoch 81/100, Batch 0/30, Loss: 0.4383\n",
      "Epoch 81/100, Batch 5/30, Loss: 0.5018\n",
      "Epoch 81/100, Batch 10/30, Loss: 0.5128\n",
      "Epoch 81/100, Batch 15/30, Loss: 0.5015\n",
      "Epoch 81/100, Batch 20/30, Loss: 0.3370\n",
      "Epoch 81/100, Batch 25/30, Loss: 0.0880\n",
      "Epoch 81/100, Average Loss: 0.3979, Valid Batches: 30/30, Learning Rate: 0.000063\n",
      "Épocas sin mejora: 1/10\n",
      "Epoch 82/100, Batch 0/30, Loss: 0.5010\n",
      "Epoch 82/100, Batch 5/30, Loss: 0.0799\n",
      "Epoch 82/100, Batch 10/30, Loss: 0.4665\n",
      "Epoch 82/100, Batch 15/30, Loss: 0.5063\n",
      "Epoch 82/100, Batch 20/30, Loss: 0.0661\n",
      "Epoch 82/100, Batch 25/30, Loss: 0.5177\n",
      "Epoch 82/100, Average Loss: 0.3978, Valid Batches: 30/30, Learning Rate: 0.000063\n",
      "Épocas sin mejora: 2/10\n",
      "Epoch 83/100, Batch 0/30, Loss: 0.4488\n",
      "Epoch 83/100, Batch 5/30, Loss: 0.2274\n",
      "Epoch 83/100, Batch 10/30, Loss: 0.3354\n",
      "Epoch 83/100, Batch 15/30, Loss: 0.5212\n",
      "Epoch 83/100, Batch 20/30, Loss: 0.2953\n",
      "Epoch 83/100, Batch 25/30, Loss: 0.5075\n",
      "Epoch 83/100, Average Loss: 0.3979, Valid Batches: 30/30, Learning Rate: 0.000063\n",
      "Épocas sin mejora: 3/10\n",
      "Epoch 84/100, Batch 0/30, Loss: 0.1905\n",
      "Epoch 84/100, Batch 5/30, Loss: 0.4074\n",
      "Epoch 84/100, Batch 10/30, Loss: 0.4985\n",
      "Epoch 84/100, Batch 15/30, Loss: 0.0805\n",
      "Epoch 84/100, Batch 20/30, Loss: 0.5035\n",
      "Epoch 84/100, Batch 25/30, Loss: 0.5100\n",
      "Epoch 84/100, Average Loss: 0.3978, Valid Batches: 30/30, Learning Rate: 0.000063\n",
      "Épocas sin mejora: 4/10\n",
      "Epoch 85/100, Batch 0/30, Loss: 0.4954\n",
      "Epoch 85/100, Batch 5/30, Loss: 0.4708\n",
      "Epoch 85/100, Batch 10/30, Loss: 0.0614\n",
      "Epoch 85/100, Batch 15/30, Loss: 0.5025\n",
      "Epoch 85/100, Batch 20/30, Loss: 0.4683\n",
      "Epoch 85/100, Batch 25/30, Loss: 0.4105\n",
      "Epoch 85/100, Average Loss: 0.3980, Valid Batches: 30/30, Learning Rate: 0.000031\n",
      "Épocas sin mejora: 5/10\n",
      "Epoch 86/100, Batch 0/30, Loss: 0.1970\n",
      "Epoch 86/100, Batch 5/30, Loss: 0.5036\n",
      "Epoch 86/100, Batch 10/30, Loss: 0.5162\n",
      "Epoch 86/100, Batch 15/30, Loss: 0.5108\n",
      "Epoch 86/100, Batch 20/30, Loss: 0.0827\n",
      "Epoch 86/100, Batch 25/30, Loss: 0.4525\n",
      "Epoch 86/100, Average Loss: 0.3984, Valid Batches: 30/30, Learning Rate: 0.000031\n",
      "Épocas sin mejora: 6/10\n",
      "Epoch 87/100, Batch 0/30, Loss: 0.5223\n",
      "Epoch 87/100, Batch 5/30, Loss: 0.4364\n",
      "Epoch 87/100, Batch 10/30, Loss: 0.2243\n",
      "Epoch 87/100, Batch 15/30, Loss: 0.0781\n",
      "Epoch 87/100, Batch 20/30, Loss: 0.2924\n",
      "Epoch 87/100, Batch 25/30, Loss: 0.4720\n",
      "Epoch 87/100, Average Loss: 0.3981, Valid Batches: 30/30, Learning Rate: 0.000031\n",
      "Épocas sin mejora: 7/10\n",
      "Epoch 88/100, Batch 0/30, Loss: 0.4090\n",
      "Epoch 88/100, Batch 5/30, Loss: 0.5048\n",
      "Epoch 88/100, Batch 10/30, Loss: 0.0819\n",
      "Epoch 88/100, Batch 15/30, Loss: 0.2969\n",
      "Epoch 88/100, Batch 20/30, Loss: 0.1954\n",
      "Epoch 88/100, Batch 25/30, Loss: 0.5002\n",
      "Epoch 88/100, Average Loss: 0.3979, Valid Batches: 30/30, Learning Rate: 0.000031\n",
      "Épocas sin mejora: 8/10\n",
      "Epoch 89/100, Batch 0/30, Loss: 0.4610\n",
      "Epoch 89/100, Batch 5/30, Loss: 0.5138\n",
      "Epoch 89/100, Batch 10/30, Loss: 0.4372\n",
      "Epoch 89/100, Batch 15/30, Loss: 0.5169\n",
      "Epoch 89/100, Batch 20/30, Loss: 0.4992\n",
      "Epoch 89/100, Batch 25/30, Loss: 0.5146\n",
      "Epoch 89/100, Average Loss: 0.3978, Valid Batches: 30/30, Learning Rate: 0.000016\n",
      "Épocas sin mejora: 9/10\n",
      "Epoch 90/100, Batch 0/30, Loss: 0.4247\n",
      "Epoch 90/100, Batch 5/30, Loss: 0.5114\n",
      "Epoch 90/100, Batch 10/30, Loss: 0.5137\n",
      "Epoch 90/100, Batch 15/30, Loss: 0.5198\n",
      "Epoch 90/100, Batch 20/30, Loss: 0.0801\n",
      "Epoch 90/100, Batch 25/30, Loss: 0.4472\n",
      "Epoch 90/100, Average Loss: 0.3974, Valid Batches: 30/30, Learning Rate: 0.000016\n",
      "Guardado checkpoint con mejor pérdida: 0.3974\n",
      "Epoch 91/100, Batch 0/30, Loss: 0.4978\n",
      "Epoch 91/100, Batch 5/30, Loss: 0.1996\n",
      "Epoch 91/100, Batch 10/30, Loss: 0.1969\n",
      "Epoch 91/100, Batch 15/30, Loss: 0.2910\n",
      "Epoch 91/100, Batch 20/30, Loss: 0.0820\n",
      "Epoch 91/100, Batch 25/30, Loss: 0.5007\n",
      "Epoch 91/100, Average Loss: 0.3972, Valid Batches: 30/30, Learning Rate: 0.000016\n",
      "Guardado checkpoint con mejor pérdida: 0.3972\n",
      "Epoch 92/100, Batch 0/30, Loss: 0.4335\n",
      "Epoch 92/100, Batch 5/30, Loss: 0.4990\n",
      "Epoch 92/100, Batch 10/30, Loss: 0.0864\n",
      "Epoch 92/100, Batch 15/30, Loss: 0.5125\n",
      "Epoch 92/100, Batch 20/30, Loss: 0.5207\n",
      "Epoch 92/100, Batch 25/30, Loss: 0.5020\n",
      "Epoch 92/100, Average Loss: 0.3981, Valid Batches: 30/30, Learning Rate: 0.000016\n",
      "Épocas sin mejora: 1/10\n",
      "Epoch 93/100, Batch 0/30, Loss: 0.4670\n",
      "Epoch 93/100, Batch 5/30, Loss: 0.0625\n",
      "Epoch 93/100, Batch 10/30, Loss: 0.4717\n",
      "Epoch 93/100, Batch 15/30, Loss: 0.2956\n",
      "Epoch 93/100, Batch 20/30, Loss: 0.4640\n",
      "Epoch 93/100, Batch 25/30, Loss: 0.5122\n",
      "Epoch 93/100, Average Loss: 0.3969, Valid Batches: 30/30, Learning Rate: 0.000016\n",
      "Guardado checkpoint con mejor pérdida: 0.3969\n",
      "Epoch 94/100, Batch 0/30, Loss: 0.5116\n",
      "Epoch 94/100, Batch 5/30, Loss: 0.5043\n",
      "Epoch 94/100, Batch 10/30, Loss: 0.3935\n",
      "Epoch 94/100, Batch 15/30, Loss: 0.2947\n",
      "Epoch 94/100, Batch 20/30, Loss: 0.4661\n",
      "Epoch 94/100, Batch 25/30, Loss: 0.4988\n",
      "Epoch 94/100, Average Loss: 0.3973, Valid Batches: 30/30, Learning Rate: 0.000016\n",
      "Épocas sin mejora: 1/10\n",
      "Epoch 95/100, Batch 0/30, Loss: 0.4537\n",
      "Epoch 95/100, Batch 5/30, Loss: 0.3309\n",
      "Epoch 95/100, Batch 10/30, Loss: 0.4283\n",
      "Epoch 95/100, Batch 15/30, Loss: 0.4980\n",
      "Epoch 95/100, Batch 20/30, Loss: 0.4321\n",
      "Epoch 95/100, Batch 25/30, Loss: 0.4619\n",
      "Epoch 95/100, Average Loss: 0.3969, Valid Batches: 30/30, Learning Rate: 0.000016\n",
      "Épocas sin mejora: 2/10\n",
      "Epoch 96/100, Batch 0/30, Loss: 0.5196\n",
      "Epoch 96/100, Batch 5/30, Loss: 0.4647\n",
      "Epoch 96/100, Batch 10/30, Loss: 0.0806\n",
      "Epoch 96/100, Batch 15/30, Loss: 0.4485\n",
      "Epoch 96/100, Batch 20/30, Loss: 0.0776\n",
      "Epoch 96/100, Batch 25/30, Loss: 0.5150\n",
      "Epoch 96/100, Average Loss: 0.3974, Valid Batches: 30/30, Learning Rate: 0.000016\n",
      "Épocas sin mejora: 3/10\n",
      "Epoch 97/100, Batch 0/30, Loss: 0.2265\n",
      "Epoch 97/100, Batch 5/30, Loss: 0.2048\n",
      "Epoch 97/100, Batch 10/30, Loss: 0.2943\n",
      "Epoch 97/100, Batch 15/30, Loss: 0.5000\n",
      "Epoch 97/100, Batch 20/30, Loss: 0.3962\n",
      "Epoch 97/100, Batch 25/30, Loss: 0.4867\n",
      "Epoch 97/100, Average Loss: 0.3974, Valid Batches: 30/30, Learning Rate: 0.000016\n",
      "Épocas sin mejora: 4/10\n",
      "Epoch 98/100, Batch 0/30, Loss: 0.2930\n",
      "Epoch 98/100, Batch 5/30, Loss: 0.4348\n",
      "Epoch 98/100, Batch 10/30, Loss: 0.4688\n",
      "Epoch 98/100, Batch 15/30, Loss: 0.4992\n",
      "Epoch 98/100, Batch 20/30, Loss: 0.5155\n",
      "Epoch 98/100, Batch 25/30, Loss: 0.4659\n",
      "Epoch 98/100, Average Loss: 0.3971, Valid Batches: 30/30, Learning Rate: 0.000008\n",
      "Épocas sin mejora: 5/10\n",
      "Epoch 99/100, Batch 0/30, Loss: 0.5088\n",
      "Epoch 99/100, Batch 5/30, Loss: 0.4839\n",
      "Epoch 99/100, Batch 10/30, Loss: 0.5016\n",
      "Epoch 99/100, Batch 15/30, Loss: 0.0912\n",
      "Epoch 99/100, Batch 20/30, Loss: 0.4720\n",
      "Epoch 99/100, Batch 25/30, Loss: 0.4601\n",
      "Epoch 99/100, Average Loss: 0.3970, Valid Batches: 30/30, Learning Rate: 0.000008\n",
      "Épocas sin mejora: 6/10\n",
      "Epoch 100/100, Batch 0/30, Loss: 0.4702\n",
      "Epoch 100/100, Batch 5/30, Loss: 0.2257\n",
      "Epoch 100/100, Batch 10/30, Loss: 0.4840\n",
      "Epoch 100/100, Batch 15/30, Loss: 0.5009\n",
      "Epoch 100/100, Batch 20/30, Loss: 0.5009\n",
      "Epoch 100/100, Batch 25/30, Loss: 0.5137\n",
      "Epoch 100/100, Average Loss: 0.3974, Valid Batches: 30/30, Learning Rate: 0.000008\n",
      "Épocas sin mejora: 7/10\n",
      "Cargado el mejor modelo desde trained_models/checkpoints_contrastive/best_contrastive_projection_head.pth con pérdida: 0.3969\n",
      "Modelo final guardado en 'trained_models/checkpoints_contrastive/contrastive_projection_head_final.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1384671/1500752681.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(best_model_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "# Directorio para checkpoints\n",
    "output_dir = \"trained_models/checkpoints_contrastive\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Variables para early stopping y checkpoints\n",
    "best_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "best_model_path = os.path.join(output_dir, \"best_contrastive_projection_head.pth\")\n",
    "\n",
    "# Entrenamiento con scheduler, checkpoints y early stopping\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    valid_batches = 0\n",
    "    \n",
    "    model.train()  # Modo entrenamiento\n",
    "    \n",
    "    for batch_idx, (embeddings, labels) in enumerate(loader):\n",
    "        embeddings = embeddings.to(device)  # [1, 48, 128, 128, 128]\n",
    "        labels = labels.to(device)  # [1, 128, 128, 128]\n",
    "        \n",
    "        embeddings = embeddings.squeeze(0).permute(1, 2, 3, 0)  # [128, 128, 128, 48]\n",
    "        labels = labels.squeeze(0)  # [128, 128, 128]\n",
    "        \n",
    "        embeddings_flat = embeddings.reshape(-1, 48)  # [2097152, 48]\n",
    "        labels_flat = labels.reshape(-1)  # [2097152]\n",
    "        \n",
    "        valid_mask = labels_flat >= 0\n",
    "        embeddings_valid = embeddings_flat[valid_mask]\n",
    "        labels_valid = labels_flat[valid_mask]\n",
    "        \n",
    "        if embeddings_valid.shape[0] < 2:\n",
    "            print(f\"Batch {batch_idx}: Insuficientes vóxeles válidos\")\n",
    "            continue\n",
    "        \n",
    "        # Forward\n",
    "        z = model(embeddings_valid)\n",
    "        loss = contrastive_loss(z, labels_valid, temperature, sample_size_per_class)\n",
    "        \n",
    "        if loss.item() == 0:\n",
    "            continue  # No contar batches con pérdida 0\n",
    "        \n",
    "        # Optimización\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        valid_batches += 1\n",
    "        \n",
    "        if batch_idx % 5 == 0:  # Imprimir cada 5 batches\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(loader)}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Calcular pérdida promedio\n",
    "    avg_loss = total_loss / max(valid_batches, 1)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}, Valid Batches: {valid_batches}/{len(loader)}, \"\n",
    "          f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Scheduler: ajustar tasa de aprendizaje\n",
    "    scheduler.step(avg_loss)\n",
    "    \n",
    "    # Checkpoint: guardar el mejor modelo\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': best_loss,\n",
    "        }, best_model_path)\n",
    "        print(f\"Guardado checkpoint con mejor pérdida: {best_loss:.4f}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"Épocas sin mejora: {epochs_no_improve}/{patience}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f\"Early stopping activado tras {epoch+1} épocas. Mejor pérdida: {best_loss:.4f}\")\n",
    "        break\n",
    "\n",
    "# Cargar el mejor modelo al final (opcional)\n",
    "checkpoint = torch.load(best_model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Cargado el mejor modelo desde {best_model_path} con pérdida: {checkpoint['loss']:.4f}\")\n",
    "\n",
    "# Guardar el modelo final (opcional)\n",
    "torch.save(model.state_dict(), os.path.join(output_dir, \"contrastive_projection_head_final.pth\"))\n",
    "print(\"Modelo final guardado en 'trained_models/checkpoints_contrastive/contrastive_projection_head_final.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    valid_batches = 0\n",
    "    for batch_idx, (embeddings, labels) in enumerate(loader):\n",
    "        embeddings = embeddings.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        embeddings = embeddings.squeeze(0).permute(1, 2, 3, 0)\n",
    "        labels = labels.squeeze(0)\n",
    "        \n",
    "        embeddings_flat = embeddings.reshape(-1, 48)\n",
    "        labels_flat = labels.reshape(-1)\n",
    "        \n",
    "        valid_mask = labels_flat >= 0\n",
    "        embeddings_valid = embeddings_flat[valid_mask]\n",
    "        labels_valid = labels_flat[valid_mask]\n",
    "        \n",
    "        if embeddings_valid.shape[0] < 2:\n",
    "            print(f\"Batch {batch_idx}: Insuficientes vóxeles válidos\")\n",
    "            continue\n",
    "        \n",
    "        z = model(embeddings_valid)\n",
    "        loss = contrastive_loss(z, labels_valid, temperature, sample_size_per_class)\n",
    "        \n",
    "        if loss.item() == 0:\n",
    "            continue  # No contar batches con pérdida 0 en el promedio\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        valid_batches += 1\n",
    "        \n",
    "        if batch_idx % 1 == 0:  # Reducir frecuencia de impresión\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(loader)}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    avg_loss = total_loss / max(valid_batches, 1)  # Evitar división por 0\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}, Valid Batches: {valid_batches}/{len(loader)}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"trained_models/contrastive_projection_head.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (embeddings, labels) in enumerate(loader):\n",
    "    print(f\"Unique labels: {torch.unique(labels).tolist()}\")\n",
    "    labels_flat = labels.reshape(-1)\n",
    "    class_counts = torch.bincount(labels_flat)\n",
    "    print(f\"Case {idx}: Fondo: {class_counts[0]}, Vasogénico: {class_counts[1] if len(class_counts) > 1 else 0}, Infiltrado: {class_counts[2] if len(class_counts) > 2 else 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar modelo de clasificacion supervisado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1384671/4223223206.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  projection_head.load_state_dict(torch.load(\"trained_models/checkpoints_contrastive/contrastive_projection_head_final.pth\", map_location=device))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Dispositivo\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset (ya lo tienes)\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, embedding_dir, label_dir):\n",
    "        self.embedding_dir = embedding_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.case_files = [f for f in os.listdir(embedding_dir) if f.endswith(\".npy\")]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.case_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        embedding_path = os.path.join(self.embedding_dir, f\"case_{idx}.npy\")\n",
    "        label_path = os.path.join(self.label_dir, f\"case_{idx}.npy\")\n",
    "        \n",
    "        embeddings = np.load(embedding_path)  # [1, 48, 128, 128, 128]\n",
    "        labels = np.load(label_path)  # [128, 128, 128]\n",
    "        \n",
    "        embeddings = torch.tensor(embeddings, dtype=torch.float32).squeeze(0)  # [48, 128, 128, 128]\n",
    "        labels = torch.tensor(labels, dtype=torch.long)  # [128, 128, 128]\n",
    "        \n",
    "        return embeddings, labels\n",
    "\n",
    "# Modelo de proyección (ya lo tienes)\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, input_dim=48, hidden_dim=128, output_dim=128):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Clasificador supervisado\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim=128, num_classes=3):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Configuración\n",
    "embedding_dir = \"Dataset/contrastive_voxel_wise/embeddings\"\n",
    "label_dir = \"Dataset/contrastive_voxel_wise/labels\"\n",
    "batch_size = 1\n",
    "sample_size_per_class = 4096 # ~10,000 vóxeles total\n",
    "num_epochs = 100  # Máximo de épocas\n",
    "patience = 10  # Early stopping: épocas sin mejora\n",
    "\n",
    "# Cargar dataset y DataLoader\n",
    "dataset = EmbeddingDataset(embedding_dir, label_dir)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "# Cargar modelo contrastivo preentrenado\n",
    "projection_head = ProjectionHead(input_dim=48).to(device)\n",
    "projection_head.load_state_dict(torch.load(\"trained_models/checkpoints_contrastive/contrastive_projection_head_final.pth\", map_location=device))\n",
    "projection_head.eval()  # Modo evaluación, sin gradientes\n",
    "\n",
    "# Definir clasificador\n",
    "classifier = Classifier(input_dim=128, num_classes=3).to(device)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Batch 0/30, Loss: 1.0935, Sampled size: 8704, Classes: [0, 1, 2]\n",
      "Epoch 1/100, Batch 5/30, Loss: 1.0490, Sampled size: 8194, Classes: [0, 1, 2]\n",
      "Epoch 1/100, Batch 10/30, Loss: 1.0064, Sampled size: 8192, Classes: [0, 2]\n",
      "Epoch 1/100, Batch 15/30, Loss: 0.9832, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 1/100, Batch 20/30, Loss: 0.9566, Sampled size: 9559, Classes: [0, 1, 2]\n",
      "Epoch 1/100, Batch 25/30, Loss: 0.9212, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 1/100, Average Loss: 0.9859, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.9859\n",
      "Epoch 2/100, Batch 0/30, Loss: 0.8745, Sampled size: 8531, Classes: [0, 1, 2]\n",
      "Epoch 2/100, Batch 5/30, Loss: 0.8344, Sampled size: 8192, Classes: [0, 2]\n",
      "Epoch 2/100, Batch 10/30, Loss: 0.8263, Sampled size: 9825, Classes: [0, 1, 2]\n",
      "Epoch 2/100, Batch 15/30, Loss: 0.7890, Sampled size: 9224, Classes: [0, 1, 2]\n",
      "Epoch 2/100, Batch 20/30, Loss: 0.7859, Sampled size: 10122, Classes: [0, 1, 2]\n",
      "Epoch 2/100, Batch 25/30, Loss: 0.7904, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 2/100, Average Loss: 0.8167, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.8167\n",
      "Epoch 3/100, Batch 0/30, Loss: 0.7704, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 3/100, Batch 5/30, Loss: 0.7478, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 3/100, Batch 10/30, Loss: 0.7378, Sampled size: 11763, Classes: [0, 1, 2]\n",
      "Epoch 3/100, Batch 15/30, Loss: 0.6339, Sampled size: 8194, Classes: [0, 1, 2]\n",
      "Epoch 3/100, Batch 20/30, Loss: 0.6962, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 3/100, Batch 25/30, Loss: 0.7067, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 3/100, Average Loss: 0.7023, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.7023\n",
      "Epoch 4/100, Batch 0/30, Loss: 0.6994, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 4/100, Batch 5/30, Loss: 0.6517, Sampled size: 11396, Classes: [0, 1, 2]\n",
      "Epoch 4/100, Batch 10/30, Loss: 0.6673, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 4/100, Batch 15/30, Loss: 0.6815, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 4/100, Batch 20/30, Loss: 0.6450, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 4/100, Batch 25/30, Loss: 0.5442, Sampled size: 9224, Classes: [0, 1, 2]\n",
      "Epoch 4/100, Average Loss: 0.6269, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.6269\n",
      "Epoch 5/100, Batch 0/30, Loss: 0.6188, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 5/100, Batch 5/30, Loss: 0.6562, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 5/100, Batch 10/30, Loss: 0.5879, Sampled size: 11002, Classes: [0, 1, 2]\n",
      "Epoch 5/100, Batch 15/30, Loss: 0.6366, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 5/100, Batch 20/30, Loss: 0.5553, Sampled size: 10517, Classes: [0, 1, 2]\n",
      "Epoch 5/100, Batch 25/30, Loss: 0.4504, Sampled size: 8250, Classes: [0, 1, 2]\n",
      "Epoch 5/100, Average Loss: 0.5765, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.5765\n",
      "Epoch 6/100, Batch 0/30, Loss: 0.5798, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 6/100, Batch 5/30, Loss: 0.6332, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 6/100, Batch 10/30, Loss: 0.6216, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 6/100, Batch 15/30, Loss: 0.5570, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 6/100, Batch 20/30, Loss: 0.4544, Sampled size: 9224, Classes: [0, 1, 2]\n",
      "Epoch 6/100, Batch 25/30, Loss: 0.5904, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 6/100, Average Loss: 0.5416, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.5416\n",
      "Epoch 7/100, Batch 0/30, Loss: 0.5172, Sampled size: 10517, Classes: [0, 1, 2]\n",
      "Epoch 7/100, Batch 5/30, Loss: 0.4275, Sampled size: 8704, Classes: [0, 1, 2]\n",
      "Epoch 7/100, Batch 10/30, Loss: 0.5669, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 7/100, Batch 15/30, Loss: 0.4087, Sampled size: 8646, Classes: [0, 1, 2]\n",
      "Epoch 7/100, Batch 20/30, Loss: 0.4272, Sampled size: 8531, Classes: [0, 1, 2]\n",
      "Epoch 7/100, Batch 25/30, Loss: 0.5112, Sampled size: 11002, Classes: [0, 1, 2]\n",
      "Epoch 7/100, Average Loss: 0.5181, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.5181\n",
      "Epoch 8/100, Batch 0/30, Loss: 0.5174, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 8/100, Batch 5/30, Loss: 0.4237, Sampled size: 9651, Classes: [0, 1, 2]\n",
      "Epoch 8/100, Batch 10/30, Loss: 0.5420, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 8/100, Batch 15/30, Loss: 0.4036, Sampled size: 9224, Classes: [0, 1, 2]\n",
      "Epoch 8/100, Batch 20/30, Loss: 0.5654, Sampled size: 11763, Classes: [0, 1, 2]\n",
      "Epoch 8/100, Batch 25/30, Loss: 0.5842, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 8/100, Average Loss: 0.4997, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4997\n",
      "Epoch 9/100, Batch 0/30, Loss: 0.3952, Sampled size: 9224, Classes: [0, 1, 2]\n",
      "Epoch 9/100, Batch 5/30, Loss: 0.5938, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 9/100, Batch 10/30, Loss: 0.5079, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 9/100, Batch 15/30, Loss: 0.5241, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 9/100, Batch 20/30, Loss: 0.3756, Sampled size: 8704, Classes: [0, 1, 2]\n",
      "Epoch 9/100, Batch 25/30, Loss: 0.5265, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 9/100, Average Loss: 0.4870, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4870\n",
      "Epoch 10/100, Batch 0/30, Loss: 0.4862, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 10/100, Batch 5/30, Loss: 0.5513, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 10/100, Batch 10/30, Loss: 0.5634, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 10/100, Batch 15/30, Loss: 0.4901, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 10/100, Batch 20/30, Loss: 0.3927, Sampled size: 9651, Classes: [0, 1, 2]\n",
      "Epoch 10/100, Batch 25/30, Loss: 0.4905, Sampled size: 9559, Classes: [0, 1, 2]\n",
      "Epoch 10/100, Average Loss: 0.4772, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4772\n",
      "Epoch 11/100, Batch 0/30, Loss: 0.5842, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 11/100, Batch 5/30, Loss: 0.5410, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 11/100, Batch 10/30, Loss: 0.5136, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 11/100, Batch 15/30, Loss: 0.3607, Sampled size: 8704, Classes: [0, 1, 2]\n",
      "Epoch 11/100, Batch 20/30, Loss: 0.4844, Sampled size: 9559, Classes: [0, 1, 2]\n",
      "Epoch 11/100, Batch 25/30, Loss: 0.5726, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 11/100, Average Loss: 0.4689, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4689\n",
      "Epoch 12/100, Batch 0/30, Loss: 0.4650, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 12/100, Batch 5/30, Loss: 0.4597, Sampled size: 10122, Classes: [0, 1, 2]\n",
      "Epoch 12/100, Batch 10/30, Loss: 0.5621, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 12/100, Batch 15/30, Loss: 0.5469, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 12/100, Batch 20/30, Loss: 0.4552, Sampled size: 11002, Classes: [0, 1, 2]\n",
      "Epoch 12/100, Batch 25/30, Loss: 0.3659, Sampled size: 8192, Classes: [0, 2]\n",
      "Epoch 12/100, Average Loss: 0.4635, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4635\n",
      "Epoch 13/100, Batch 0/30, Loss: 0.3663, Sampled size: 9651, Classes: [0, 1, 2]\n",
      "Epoch 13/100, Batch 5/30, Loss: 0.5731, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 13/100, Batch 10/30, Loss: 0.4138, Sampled size: 9825, Classes: [0, 1, 2]\n",
      "Epoch 13/100, Batch 15/30, Loss: 0.3403, Sampled size: 8704, Classes: [0, 1, 2]\n",
      "Epoch 13/100, Batch 20/30, Loss: 0.5255, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 13/100, Batch 25/30, Loss: 0.5038, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 13/100, Average Loss: 0.4586, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4586\n",
      "Epoch 14/100, Batch 0/30, Loss: 0.5334, Sampled size: 11763, Classes: [0, 1, 2]\n",
      "Epoch 14/100, Batch 5/30, Loss: 0.4520, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 14/100, Batch 10/30, Loss: 0.4056, Sampled size: 9825, Classes: [0, 1, 2]\n",
      "Epoch 14/100, Batch 15/30, Loss: 0.5222, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 14/100, Batch 20/30, Loss: 0.3323, Sampled size: 8704, Classes: [0, 1, 2]\n",
      "Epoch 14/100, Batch 25/30, Loss: 0.3536, Sampled size: 9651, Classes: [0, 1, 2]\n",
      "Epoch 14/100, Average Loss: 0.4534, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4534\n",
      "Epoch 15/100, Batch 0/30, Loss: 0.4341, Sampled size: 10517, Classes: [0, 1, 2]\n",
      "Epoch 15/100, Batch 5/30, Loss: 0.2956, Sampled size: 8194, Classes: [0, 1, 2]\n",
      "Epoch 15/100, Batch 10/30, Loss: 0.3512, Sampled size: 9651, Classes: [0, 1, 2]\n",
      "Epoch 15/100, Batch 15/30, Loss: 0.4079, Sampled size: 9825, Classes: [0, 1, 2]\n",
      "Epoch 15/100, Batch 20/30, Loss: 0.5211, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 15/100, Batch 25/30, Loss: 0.4691, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 15/100, Average Loss: 0.4507, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4507\n",
      "Epoch 16/100, Batch 0/30, Loss: 0.4417, Sampled size: 11396, Classes: [0, 1, 2]\n",
      "Epoch 16/100, Batch 5/30, Loss: 0.4453, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 16/100, Batch 10/30, Loss: 0.4320, Sampled size: 10517, Classes: [0, 1, 2]\n",
      "Epoch 16/100, Batch 15/30, Loss: 0.4908, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 16/100, Batch 20/30, Loss: 0.5481, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 16/100, Batch 25/30, Loss: 0.4074, Sampled size: 9825, Classes: [0, 1, 2]\n",
      "Epoch 16/100, Average Loss: 0.4480, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4480\n",
      "Epoch 17/100, Batch 0/30, Loss: 0.4040, Sampled size: 9825, Classes: [0, 1, 2]\n",
      "Epoch 17/100, Batch 5/30, Loss: 0.5132, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 17/100, Batch 10/30, Loss: 0.5603, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 17/100, Batch 15/30, Loss: 0.5416, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 17/100, Batch 20/30, Loss: 0.4375, Sampled size: 11002, Classes: [0, 1, 2]\n",
      "Epoch 17/100, Batch 25/30, Loss: 0.3309, Sampled size: 9224, Classes: [0, 1, 2]\n",
      "Epoch 17/100, Average Loss: 0.4456, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4456\n",
      "Epoch 18/100, Batch 0/30, Loss: 0.5409, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 18/100, Batch 5/30, Loss: 0.5273, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 18/100, Batch 10/30, Loss: 0.3383, Sampled size: 8192, Classes: [0, 2]\n",
      "Epoch 18/100, Batch 15/30, Loss: 0.4816, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 18/100, Batch 20/30, Loss: 0.4038, Sampled size: 9825, Classes: [0, 1, 2]\n",
      "Epoch 18/100, Batch 25/30, Loss: 0.5231, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 18/100, Average Loss: 0.4434, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4434\n",
      "Epoch 19/100, Batch 0/30, Loss: 0.4506, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 19/100, Batch 5/30, Loss: 0.4815, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 19/100, Batch 10/30, Loss: 0.4631, Sampled size: 9559, Classes: [0, 1, 2]\n",
      "Epoch 19/100, Batch 15/30, Loss: 0.4912, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 19/100, Batch 20/30, Loss: 0.5697, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 19/100, Batch 25/30, Loss: 0.2901, Sampled size: 8194, Classes: [0, 1, 2]\n",
      "Epoch 19/100, Average Loss: 0.4409, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4409\n",
      "Epoch 20/100, Batch 0/30, Loss: 0.3378, Sampled size: 8192, Classes: [0, 2]\n",
      "Epoch 20/100, Batch 5/30, Loss: 0.3377, Sampled size: 9651, Classes: [0, 1, 2]\n",
      "Epoch 20/100, Batch 10/30, Loss: 0.5301, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 20/100, Batch 15/30, Loss: 0.4335, Sampled size: 11002, Classes: [0, 1, 2]\n",
      "Epoch 20/100, Batch 20/30, Loss: 0.5750, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 20/100, Batch 25/30, Loss: 0.5130, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 20/100, Average Loss: 0.4401, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4401\n",
      "Epoch 21/100, Batch 0/30, Loss: 0.5207, Sampled size: 11763, Classes: [0, 1, 2]\n",
      "Epoch 21/100, Batch 5/30, Loss: 0.5500, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 21/100, Batch 10/30, Loss: 0.4436, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 21/100, Batch 15/30, Loss: 0.2984, Sampled size: 8250, Classes: [0, 1, 2]\n",
      "Epoch 21/100, Batch 20/30, Loss: 0.3115, Sampled size: 8646, Classes: [0, 1, 2]\n",
      "Epoch 21/100, Batch 25/30, Loss: 0.4379, Sampled size: 10122, Classes: [0, 1, 2]\n",
      "Epoch 21/100, Average Loss: 0.4404, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Épocas sin mejora: 1/10\n",
      "Epoch 22/100, Batch 0/30, Loss: 0.4478, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 22/100, Batch 5/30, Loss: 0.2967, Sampled size: 8646, Classes: [0, 1, 2]\n",
      "Epoch 22/100, Batch 10/30, Loss: 0.5587, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 22/100, Batch 15/30, Loss: 0.4256, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 22/100, Batch 20/30, Loss: 0.4193, Sampled size: 10517, Classes: [0, 1, 2]\n",
      "Epoch 22/100, Batch 25/30, Loss: 0.3360, Sampled size: 8531, Classes: [0, 1, 2]\n",
      "Epoch 22/100, Average Loss: 0.4374, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4374\n",
      "Epoch 23/100, Batch 0/30, Loss: 0.2868, Sampled size: 8194, Classes: [0, 1, 2]\n",
      "Epoch 23/100, Batch 5/30, Loss: 0.4167, Sampled size: 10517, Classes: [0, 1, 2]\n",
      "Epoch 23/100, Batch 10/30, Loss: 0.5411, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 23/100, Batch 15/30, Loss: 0.4670, Sampled size: 9559, Classes: [0, 1, 2]\n",
      "Epoch 23/100, Batch 20/30, Loss: 0.2830, Sampled size: 8250, Classes: [0, 1, 2]\n",
      "Epoch 23/100, Batch 25/30, Loss: 0.5232, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 23/100, Average Loss: 0.4367, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4367\n",
      "Epoch 24/100, Batch 0/30, Loss: 0.2943, Sampled size: 8646, Classes: [0, 1, 2]\n",
      "Epoch 24/100, Batch 5/30, Loss: 0.4605, Sampled size: 9559, Classes: [0, 1, 2]\n",
      "Epoch 24/100, Batch 10/30, Loss: 0.5581, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 24/100, Batch 15/30, Loss: 0.5069, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 24/100, Batch 20/30, Loss: 0.4367, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 24/100, Batch 25/30, Loss: 0.3132, Sampled size: 9224, Classes: [0, 1, 2]\n",
      "Epoch 24/100, Average Loss: 0.4353, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4353\n",
      "Epoch 25/100, Batch 0/30, Loss: 0.4323, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 25/100, Batch 5/30, Loss: 0.5576, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 25/100, Batch 10/30, Loss: 0.3322, Sampled size: 8531, Classes: [0, 1, 2]\n",
      "Epoch 25/100, Batch 15/30, Loss: 0.4145, Sampled size: 10517, Classes: [0, 1, 2]\n",
      "Epoch 25/100, Batch 20/30, Loss: 0.3109, Sampled size: 8704, Classes: [0, 1, 2]\n",
      "Epoch 25/100, Batch 25/30, Loss: 0.5414, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 25/100, Average Loss: 0.4346, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4346\n",
      "Epoch 26/100, Batch 0/30, Loss: 0.4840, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 26/100, Batch 5/30, Loss: 0.4445, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 26/100, Batch 10/30, Loss: 0.4328, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 26/100, Batch 15/30, Loss: 0.5115, Sampled size: 11763, Classes: [0, 1, 2]\n",
      "Epoch 26/100, Batch 20/30, Loss: 0.5205, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 26/100, Batch 25/30, Loss: 0.4245, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 26/100, Average Loss: 0.4337, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4337\n",
      "Epoch 27/100, Batch 0/30, Loss: 0.3337, Sampled size: 9651, Classes: [0, 1, 2]\n",
      "Epoch 27/100, Batch 5/30, Loss: 0.5148, Sampled size: 11763, Classes: [0, 1, 2]\n",
      "Epoch 27/100, Batch 10/30, Loss: 0.3318, Sampled size: 8192, Classes: [0, 2]\n",
      "Epoch 27/100, Batch 15/30, Loss: 0.4268, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 27/100, Batch 20/30, Loss: 0.2811, Sampled size: 8646, Classes: [0, 1, 2]\n",
      "Epoch 27/100, Batch 25/30, Loss: 0.4791, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 27/100, Average Loss: 0.4361, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Épocas sin mejora: 1/10\n",
      "Epoch 28/100, Batch 0/30, Loss: 0.4477, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 28/100, Batch 5/30, Loss: 0.5151, Sampled size: 11763, Classes: [0, 1, 2]\n",
      "Epoch 28/100, Batch 10/30, Loss: 0.4359, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 28/100, Batch 15/30, Loss: 0.5013, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 28/100, Batch 20/30, Loss: 0.2861, Sampled size: 8646, Classes: [0, 1, 2]\n",
      "Epoch 28/100, Batch 25/30, Loss: 0.5308, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 28/100, Average Loss: 0.4329, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4329\n",
      "Epoch 29/100, Batch 0/30, Loss: 0.2807, Sampled size: 8194, Classes: [0, 1, 2]\n",
      "Epoch 29/100, Batch 5/30, Loss: 0.5008, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 29/100, Batch 10/30, Loss: 0.5606, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 29/100, Batch 15/30, Loss: 0.4249, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 29/100, Batch 20/30, Loss: 0.4133, Sampled size: 10517, Classes: [0, 1, 2]\n",
      "Epoch 29/100, Batch 25/30, Loss: 0.5234, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 29/100, Average Loss: 0.4324, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4324\n",
      "Epoch 30/100, Batch 0/30, Loss: 0.3467, Sampled size: 8192, Classes: [0, 2]\n",
      "Epoch 30/100, Batch 5/30, Loss: 0.4085, Sampled size: 10517, Classes: [0, 1, 2]\n",
      "Epoch 30/100, Batch 10/30, Loss: 0.4413, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 30/100, Batch 15/30, Loss: 0.4390, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 30/100, Batch 20/30, Loss: 0.2887, Sampled size: 8194, Classes: [0, 1, 2]\n",
      "Epoch 30/100, Batch 25/30, Loss: 0.4254, Sampled size: 11002, Classes: [0, 1, 2]\n",
      "Epoch 30/100, Average Loss: 0.4330, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Épocas sin mejora: 1/10\n",
      "Epoch 31/100, Batch 0/30, Loss: 0.3415, Sampled size: 8192, Classes: [0, 2]\n",
      "Epoch 31/100, Batch 5/30, Loss: 0.4274, Sampled size: 10122, Classes: [0, 1, 2]\n",
      "Epoch 31/100, Batch 10/30, Loss: 0.3101, Sampled size: 8704, Classes: [0, 1, 2]\n",
      "Epoch 31/100, Batch 15/30, Loss: 0.2877, Sampled size: 8646, Classes: [0, 1, 2]\n",
      "Epoch 31/100, Batch 20/30, Loss: 0.4316, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 31/100, Batch 25/30, Loss: 0.5121, Sampled size: 11763, Classes: [0, 1, 2]\n",
      "Epoch 31/100, Average Loss: 0.4320, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4320\n",
      "Epoch 32/100, Batch 0/30, Loss: 0.5602, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 32/100, Batch 5/30, Loss: 0.5134, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 32/100, Batch 10/30, Loss: 0.4189, Sampled size: 11396, Classes: [0, 1, 2]\n",
      "Epoch 32/100, Batch 15/30, Loss: 0.2859, Sampled size: 8194, Classes: [0, 1, 2]\n",
      "Epoch 32/100, Batch 20/30, Loss: 0.5259, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 32/100, Batch 25/30, Loss: 0.4692, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 32/100, Average Loss: 0.4317, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4317\n",
      "Epoch 33/100, Batch 0/30, Loss: 0.2870, Sampled size: 8646, Classes: [0, 1, 2]\n",
      "Epoch 33/100, Batch 5/30, Loss: 0.3116, Sampled size: 9224, Classes: [0, 1, 2]\n",
      "Epoch 33/100, Batch 10/30, Loss: 0.5507, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 33/100, Batch 15/30, Loss: 0.5552, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 33/100, Batch 20/30, Loss: 0.4239, Sampled size: 11002, Classes: [0, 1, 2]\n",
      "Epoch 33/100, Batch 25/30, Loss: 0.3303, Sampled size: 9651, Classes: [0, 1, 2]\n",
      "Epoch 33/100, Average Loss: 0.4309, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4309\n",
      "Epoch 34/100, Batch 0/30, Loss: 0.5506, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 34/100, Batch 5/30, Loss: 0.2778, Sampled size: 8250, Classes: [0, 1, 2]\n",
      "Epoch 34/100, Batch 10/30, Loss: 0.5114, Sampled size: 11763, Classes: [0, 1, 2]\n",
      "Epoch 34/100, Batch 15/30, Loss: 0.3308, Sampled size: 9651, Classes: [0, 1, 2]\n",
      "Epoch 34/100, Batch 20/30, Loss: 0.4288, Sampled size: 10122, Classes: [0, 1, 2]\n",
      "Epoch 34/100, Batch 25/30, Loss: 0.5666, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 34/100, Average Loss: 0.4309, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4309\n",
      "Epoch 35/100, Batch 0/30, Loss: 0.4238, Sampled size: 10122, Classes: [0, 1, 2]\n",
      "Epoch 35/100, Batch 5/30, Loss: 0.5304, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 35/100, Batch 10/30, Loss: 0.3066, Sampled size: 8704, Classes: [0, 1, 2]\n",
      "Epoch 35/100, Batch 15/30, Loss: 0.2901, Sampled size: 8646, Classes: [0, 1, 2]\n",
      "Epoch 35/100, Batch 20/30, Loss: 0.2880, Sampled size: 8250, Classes: [0, 1, 2]\n",
      "Epoch 35/100, Batch 25/30, Loss: 0.3315, Sampled size: 8531, Classes: [0, 1, 2]\n",
      "Epoch 35/100, Average Loss: 0.4308, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4308\n",
      "Epoch 36/100, Batch 0/30, Loss: 0.3402, Sampled size: 8192, Classes: [0, 2]\n",
      "Epoch 36/100, Batch 5/30, Loss: 0.4351, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 36/100, Batch 10/30, Loss: 0.3267, Sampled size: 9651, Classes: [0, 1, 2]\n",
      "Epoch 36/100, Batch 15/30, Loss: 0.4672, Sampled size: 9559, Classes: [0, 1, 2]\n",
      "Epoch 36/100, Batch 20/30, Loss: 0.5140, Sampled size: 11763, Classes: [0, 1, 2]\n",
      "Epoch 36/100, Batch 25/30, Loss: 0.4756, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 36/100, Average Loss: 0.4297, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4297\n",
      "Epoch 37/100, Batch 0/30, Loss: 0.3985, Sampled size: 9825, Classes: [0, 1, 2]\n",
      "Epoch 37/100, Batch 5/30, Loss: 0.4968, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 37/100, Batch 10/30, Loss: 0.4333, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 37/100, Batch 15/30, Loss: 0.3093, Sampled size: 9224, Classes: [0, 1, 2]\n",
      "Epoch 37/100, Batch 20/30, Loss: 0.5634, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 37/100, Batch 25/30, Loss: 0.4656, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 37/100, Average Loss: 0.4311, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Épocas sin mejora: 1/10\n",
      "Epoch 38/100, Batch 0/30, Loss: 0.5347, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 38/100, Batch 5/30, Loss: 0.4289, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 38/100, Batch 10/30, Loss: 0.3096, Sampled size: 9224, Classes: [0, 1, 2]\n",
      "Epoch 38/100, Batch 15/30, Loss: 0.4432, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 38/100, Batch 20/30, Loss: 0.2799, Sampled size: 8194, Classes: [0, 1, 2]\n",
      "Epoch 38/100, Batch 25/30, Loss: 0.5589, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 38/100, Average Loss: 0.4284, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Guardado checkpoint con mejor pérdida: 0.4284\n",
      "Epoch 39/100, Batch 0/30, Loss: 0.2769, Sampled size: 8250, Classes: [0, 1, 2]\n",
      "Epoch 39/100, Batch 5/30, Loss: 0.5357, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 39/100, Batch 10/30, Loss: 0.4702, Sampled size: 9559, Classes: [0, 1, 2]\n",
      "Epoch 39/100, Batch 15/30, Loss: 0.4323, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 39/100, Batch 20/30, Loss: 0.4669, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 39/100, Batch 25/30, Loss: 0.3285, Sampled size: 8192, Classes: [0, 2]\n",
      "Epoch 39/100, Average Loss: 0.4295, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Épocas sin mejora: 1/10\n",
      "Epoch 40/100, Batch 0/30, Loss: 0.4248, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 40/100, Batch 5/30, Loss: 0.2762, Sampled size: 8194, Classes: [0, 1, 2]\n",
      "Epoch 40/100, Batch 10/30, Loss: 0.4236, Sampled size: 10122, Classes: [0, 1, 2]\n",
      "Epoch 40/100, Batch 15/30, Loss: 0.5048, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 40/100, Batch 20/30, Loss: 0.3110, Sampled size: 8704, Classes: [0, 1, 2]\n",
      "Epoch 40/100, Batch 25/30, Loss: 0.4754, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 40/100, Average Loss: 0.4302, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Épocas sin mejora: 2/10\n",
      "Epoch 41/100, Batch 0/30, Loss: 0.3294, Sampled size: 8192, Classes: [0, 2]\n",
      "Epoch 41/100, Batch 5/30, Loss: 0.4768, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 41/100, Batch 10/30, Loss: 0.4189, Sampled size: 11002, Classes: [0, 1, 2]\n",
      "Epoch 41/100, Batch 15/30, Loss: 0.4298, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 41/100, Batch 20/30, Loss: 0.2772, Sampled size: 8194, Classes: [0, 1, 2]\n",
      "Epoch 41/100, Batch 25/30, Loss: 0.5090, Sampled size: 11763, Classes: [0, 1, 2]\n",
      "Epoch 41/100, Average Loss: 0.4285, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Épocas sin mejora: 3/10\n",
      "Epoch 42/100, Batch 0/30, Loss: 0.3256, Sampled size: 8531, Classes: [0, 1, 2]\n",
      "Epoch 42/100, Batch 5/30, Loss: 0.4795, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 42/100, Batch 10/30, Loss: 0.4238, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 42/100, Batch 15/30, Loss: 0.2680, Sampled size: 8250, Classes: [0, 1, 2]\n",
      "Epoch 42/100, Batch 20/30, Loss: 0.5586, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 42/100, Batch 25/30, Loss: 0.4422, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 42/100, Average Loss: 0.4293, Valid Batches: 30/30, Learning Rate: 0.001000\n",
      "Épocas sin mejora: 4/10\n",
      "Epoch 43/100, Batch 0/30, Loss: 0.5193, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 43/100, Batch 5/30, Loss: 0.5489, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 43/100, Batch 10/30, Loss: 0.5456, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 43/100, Batch 15/30, Loss: 0.4167, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 43/100, Batch 20/30, Loss: 0.4394, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 43/100, Batch 25/30, Loss: 0.3268, Sampled size: 8531, Classes: [0, 1, 2]\n",
      "Epoch 43/100, Average Loss: 0.4284, Valid Batches: 30/30, Learning Rate: 0.000500\n",
      "Guardado checkpoint con mejor pérdida: 0.4284\n",
      "Epoch 44/100, Batch 0/30, Loss: 0.2826, Sampled size: 8646, Classes: [0, 1, 2]\n",
      "Epoch 44/100, Batch 5/30, Loss: 0.2792, Sampled size: 8194, Classes: [0, 1, 2]\n",
      "Epoch 44/100, Batch 10/30, Loss: 0.4849, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 44/100, Batch 15/30, Loss: 0.4275, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 44/100, Batch 20/30, Loss: 0.3229, Sampled size: 8531, Classes: [0, 1, 2]\n",
      "Epoch 44/100, Batch 25/30, Loss: 0.4224, Sampled size: 11002, Classes: [0, 1, 2]\n",
      "Epoch 44/100, Average Loss: 0.4282, Valid Batches: 30/30, Learning Rate: 0.000500\n",
      "Guardado checkpoint con mejor pérdida: 0.4282\n",
      "Epoch 45/100, Batch 0/30, Loss: 0.4131, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 45/100, Batch 5/30, Loss: 0.5486, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 45/100, Batch 10/30, Loss: 0.5163, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 45/100, Batch 15/30, Loss: 0.5610, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 45/100, Batch 20/30, Loss: 0.2701, Sampled size: 8250, Classes: [0, 1, 2]\n",
      "Epoch 45/100, Batch 25/30, Loss: 0.4808, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 45/100, Average Loss: 0.4277, Valid Batches: 30/30, Learning Rate: 0.000500\n",
      "Guardado checkpoint con mejor pérdida: 0.4277\n",
      "Epoch 46/100, Batch 0/30, Loss: 0.5319, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 46/100, Batch 5/30, Loss: 0.2873, Sampled size: 8646, Classes: [0, 1, 2]\n",
      "Epoch 46/100, Batch 10/30, Loss: 0.3067, Sampled size: 9224, Classes: [0, 1, 2]\n",
      "Epoch 46/100, Batch 15/30, Loss: 0.4396, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 46/100, Batch 20/30, Loss: 0.4123, Sampled size: 10517, Classes: [0, 1, 2]\n",
      "Epoch 46/100, Batch 25/30, Loss: 0.4631, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 46/100, Average Loss: 0.4284, Valid Batches: 30/30, Learning Rate: 0.000500\n",
      "Épocas sin mejora: 1/10\n",
      "Epoch 47/100, Batch 0/30, Loss: 0.4301, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 47/100, Batch 5/30, Loss: 0.4383, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 47/100, Batch 10/30, Loss: 0.4606, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 47/100, Batch 15/30, Loss: 0.4277, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 47/100, Batch 20/30, Loss: 0.3317, Sampled size: 8531, Classes: [0, 1, 2]\n",
      "Epoch 47/100, Batch 25/30, Loss: 0.3288, Sampled size: 8192, Classes: [0, 2]\n",
      "Epoch 47/100, Average Loss: 0.4285, Valid Batches: 30/30, Learning Rate: 0.000500\n",
      "Épocas sin mejora: 2/10\n",
      "Epoch 48/100, Batch 0/30, Loss: 0.2781, Sampled size: 8194, Classes: [0, 1, 2]\n",
      "Epoch 48/100, Batch 5/30, Loss: 0.5265, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 48/100, Batch 10/30, Loss: 0.3115, Sampled size: 9224, Classes: [0, 1, 2]\n",
      "Epoch 48/100, Batch 15/30, Loss: 0.3237, Sampled size: 8531, Classes: [0, 1, 2]\n",
      "Epoch 48/100, Batch 20/30, Loss: 0.4311, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 48/100, Batch 25/30, Loss: 0.3057, Sampled size: 8704, Classes: [0, 1, 2]\n",
      "Epoch 48/100, Average Loss: 0.4286, Valid Batches: 30/30, Learning Rate: 0.000500\n",
      "Épocas sin mejora: 3/10\n",
      "Epoch 49/100, Batch 0/30, Loss: 0.3297, Sampled size: 8531, Classes: [0, 1, 2]\n",
      "Epoch 49/100, Batch 5/30, Loss: 0.4282, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 49/100, Batch 10/30, Loss: 0.5371, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 49/100, Batch 15/30, Loss: 0.2696, Sampled size: 8250, Classes: [0, 1, 2]\n",
      "Epoch 49/100, Batch 20/30, Loss: 0.2737, Sampled size: 8194, Classes: [0, 1, 2]\n",
      "Epoch 49/100, Batch 25/30, Loss: 0.4036, Sampled size: 10517, Classes: [0, 1, 2]\n",
      "Epoch 49/100, Average Loss: 0.4279, Valid Batches: 30/30, Learning Rate: 0.000500\n",
      "Épocas sin mejora: 4/10\n",
      "Epoch 50/100, Batch 0/30, Loss: 0.5495, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 50/100, Batch 5/30, Loss: 0.4681, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 50/100, Batch 10/30, Loss: 0.2684, Sampled size: 8194, Classes: [0, 1, 2]\n",
      "Epoch 50/100, Batch 15/30, Loss: 0.4138, Sampled size: 11396, Classes: [0, 1, 2]\n",
      "Epoch 50/100, Batch 20/30, Loss: 0.5065, Sampled size: 11763, Classes: [0, 1, 2]\n",
      "Epoch 50/100, Batch 25/30, Loss: 0.4189, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 50/100, Average Loss: 0.4282, Valid Batches: 30/30, Learning Rate: 0.000250\n",
      "Épocas sin mejora: 5/10\n",
      "Epoch 51/100, Batch 0/30, Loss: 0.4219, Sampled size: 11002, Classes: [0, 1, 2]\n",
      "Epoch 51/100, Batch 5/30, Loss: 0.5135, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 51/100, Batch 10/30, Loss: 0.5079, Sampled size: 11763, Classes: [0, 1, 2]\n",
      "Epoch 51/100, Batch 15/30, Loss: 0.4219, Sampled size: 10122, Classes: [0, 1, 2]\n",
      "Epoch 51/100, Batch 20/30, Loss: 0.4627, Sampled size: 9559, Classes: [0, 1, 2]\n",
      "Epoch 51/100, Batch 25/30, Loss: 0.4098, Sampled size: 10517, Classes: [0, 1, 2]\n",
      "Epoch 51/100, Average Loss: 0.4281, Valid Batches: 30/30, Learning Rate: 0.000250\n",
      "Épocas sin mejora: 6/10\n",
      "Epoch 52/100, Batch 0/30, Loss: 0.4605, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 52/100, Batch 5/30, Loss: 0.5159, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 52/100, Batch 10/30, Loss: 0.4206, Sampled size: 10122, Classes: [0, 1, 2]\n",
      "Epoch 52/100, Batch 15/30, Loss: 0.3276, Sampled size: 8192, Classes: [0, 2]\n",
      "Epoch 52/100, Batch 20/30, Loss: 0.5509, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 52/100, Batch 25/30, Loss: 0.3261, Sampled size: 8531, Classes: [0, 1, 2]\n",
      "Epoch 52/100, Average Loss: 0.4283, Valid Batches: 30/30, Learning Rate: 0.000250\n",
      "Épocas sin mejora: 7/10\n",
      "Epoch 53/100, Batch 0/30, Loss: 0.4689, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 53/100, Batch 5/30, Loss: 0.4244, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 53/100, Batch 10/30, Loss: 0.4344, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 53/100, Batch 15/30, Loss: 0.4164, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 53/100, Batch 20/30, Loss: 0.3193, Sampled size: 8531, Classes: [0, 1, 2]\n",
      "Epoch 53/100, Batch 25/30, Loss: 0.4467, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 53/100, Average Loss: 0.4280, Valid Batches: 30/30, Learning Rate: 0.000250\n",
      "Épocas sin mejora: 8/10\n",
      "Epoch 54/100, Batch 0/30, Loss: 0.5262, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 54/100, Batch 5/30, Loss: 0.5134, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 54/100, Batch 10/30, Loss: 0.4124, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 54/100, Batch 15/30, Loss: 0.3078, Sampled size: 8704, Classes: [0, 1, 2]\n",
      "Epoch 54/100, Batch 20/30, Loss: 0.3290, Sampled size: 9651, Classes: [0, 1, 2]\n",
      "Epoch 54/100, Batch 25/30, Loss: 0.5388, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 54/100, Average Loss: 0.4272, Valid Batches: 30/30, Learning Rate: 0.000125\n",
      "Guardado checkpoint con mejor pérdida: 0.4272\n",
      "Epoch 55/100, Batch 0/30, Loss: 0.3911, Sampled size: 9825, Classes: [0, 1, 2]\n",
      "Epoch 55/100, Batch 5/30, Loss: 0.5501, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 55/100, Batch 10/30, Loss: 0.5550, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 55/100, Batch 15/30, Loss: 0.2784, Sampled size: 8646, Classes: [0, 1, 2]\n",
      "Epoch 55/100, Batch 20/30, Loss: 0.4919, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 55/100, Batch 25/30, Loss: 0.4737, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 55/100, Average Loss: 0.4274, Valid Batches: 30/30, Learning Rate: 0.000125\n",
      "Épocas sin mejora: 1/10\n",
      "Epoch 56/100, Batch 0/30, Loss: 0.3272, Sampled size: 8192, Classes: [0, 2]\n",
      "Epoch 56/100, Batch 5/30, Loss: 0.4722, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 56/100, Batch 10/30, Loss: 0.4273, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 56/100, Batch 15/30, Loss: 0.2702, Sampled size: 8194, Classes: [0, 1, 2]\n",
      "Epoch 56/100, Batch 20/30, Loss: 0.4297, Sampled size: 10122, Classes: [0, 1, 2]\n",
      "Epoch 56/100, Batch 25/30, Loss: 0.5509, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 56/100, Average Loss: 0.4277, Valid Batches: 30/30, Learning Rate: 0.000125\n",
      "Épocas sin mejora: 2/10\n",
      "Epoch 57/100, Batch 0/30, Loss: 0.4670, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 57/100, Batch 5/30, Loss: 0.4132, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 57/100, Batch 10/30, Loss: 0.4810, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 57/100, Batch 15/30, Loss: 0.3237, Sampled size: 9651, Classes: [0, 1, 2]\n",
      "Epoch 57/100, Batch 20/30, Loss: 0.2800, Sampled size: 8646, Classes: [0, 1, 2]\n",
      "Epoch 57/100, Batch 25/30, Loss: 0.2821, Sampled size: 8194, Classes: [0, 1, 2]\n",
      "Epoch 57/100, Average Loss: 0.4279, Valid Batches: 30/30, Learning Rate: 0.000125\n",
      "Épocas sin mejora: 3/10\n",
      "Epoch 58/100, Batch 0/30, Loss: 0.3215, Sampled size: 8531, Classes: [0, 1, 2]\n",
      "Epoch 58/100, Batch 5/30, Loss: 0.3359, Sampled size: 8192, Classes: [0, 2]\n",
      "Epoch 58/100, Batch 10/30, Loss: 0.5163, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 58/100, Batch 15/30, Loss: 0.4955, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 58/100, Batch 20/30, Loss: 0.3060, Sampled size: 9224, Classes: [0, 1, 2]\n",
      "Epoch 58/100, Batch 25/30, Loss: 0.4182, Sampled size: 10122, Classes: [0, 1, 2]\n",
      "Epoch 58/100, Average Loss: 0.4282, Valid Batches: 30/30, Learning Rate: 0.000125\n",
      "Épocas sin mejora: 4/10\n",
      "Epoch 59/100, Batch 0/30, Loss: 0.4098, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 59/100, Batch 5/30, Loss: 0.4068, Sampled size: 10517, Classes: [0, 1, 2]\n",
      "Epoch 59/100, Batch 10/30, Loss: 0.5139, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 59/100, Batch 15/30, Loss: 0.4246, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 59/100, Batch 20/30, Loss: 0.5306, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 59/100, Batch 25/30, Loss: 0.4315, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 59/100, Average Loss: 0.4284, Valid Batches: 30/30, Learning Rate: 0.000063\n",
      "Épocas sin mejora: 5/10\n",
      "Epoch 60/100, Batch 0/30, Loss: 0.4316, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 60/100, Batch 5/30, Loss: 0.3239, Sampled size: 8531, Classes: [0, 1, 2]\n",
      "Epoch 60/100, Batch 10/30, Loss: 0.5272, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 60/100, Batch 15/30, Loss: 0.5534, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 60/100, Batch 20/30, Loss: 0.3013, Sampled size: 9224, Classes: [0, 1, 2]\n",
      "Epoch 60/100, Batch 25/30, Loss: 0.5028, Sampled size: 11763, Classes: [0, 1, 2]\n",
      "Epoch 60/100, Average Loss: 0.4273, Valid Batches: 30/30, Learning Rate: 0.000063\n",
      "Épocas sin mejora: 6/10\n",
      "Epoch 61/100, Batch 0/30, Loss: 0.5563, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 61/100, Batch 5/30, Loss: 0.4295, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 61/100, Batch 10/30, Loss: 0.4220, Sampled size: 10122, Classes: [0, 1, 2]\n",
      "Epoch 61/100, Batch 15/30, Loss: 0.5031, Sampled size: 11763, Classes: [0, 1, 2]\n",
      "Epoch 61/100, Batch 20/30, Loss: 0.5519, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 61/100, Batch 25/30, Loss: 0.3040, Sampled size: 8704, Classes: [0, 1, 2]\n",
      "Epoch 61/100, Average Loss: 0.4282, Valid Batches: 30/30, Learning Rate: 0.000063\n",
      "Épocas sin mejora: 7/10\n",
      "Epoch 62/100, Batch 0/30, Loss: 0.2747, Sampled size: 8250, Classes: [0, 1, 2]\n",
      "Epoch 62/100, Batch 5/30, Loss: 0.5178, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 62/100, Batch 10/30, Loss: 0.4223, Sampled size: 10122, Classes: [0, 1, 2]\n",
      "Epoch 62/100, Batch 15/30, Loss: 0.3222, Sampled size: 8531, Classes: [0, 1, 2]\n",
      "Epoch 62/100, Batch 20/30, Loss: 0.4078, Sampled size: 10517, Classes: [0, 1, 2]\n",
      "Epoch 62/100, Batch 25/30, Loss: 0.4664, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 62/100, Average Loss: 0.4272, Valid Batches: 30/30, Learning Rate: 0.000063\n",
      "Épocas sin mejora: 8/10\n",
      "Epoch 63/100, Batch 0/30, Loss: 0.5091, Sampled size: 11763, Classes: [0, 1, 2]\n",
      "Epoch 63/100, Batch 5/30, Loss: 0.4727, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 63/100, Batch 10/30, Loss: 0.4399, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 63/100, Batch 15/30, Loss: 0.5308, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 63/100, Batch 20/30, Loss: 0.4067, Sampled size: 10517, Classes: [0, 1, 2]\n",
      "Epoch 63/100, Batch 25/30, Loss: 0.4265, Sampled size: 11002, Classes: [0, 1, 2]\n",
      "Epoch 63/100, Average Loss: 0.4276, Valid Batches: 30/30, Learning Rate: 0.000031\n",
      "Épocas sin mejora: 9/10\n",
      "Epoch 64/100, Batch 0/30, Loss: 0.2780, Sampled size: 8194, Classes: [0, 1, 2]\n",
      "Epoch 64/100, Batch 5/30, Loss: 0.2824, Sampled size: 8646, Classes: [0, 1, 2]\n",
      "Epoch 64/100, Batch 10/30, Loss: 0.4318, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 64/100, Batch 15/30, Loss: 0.3976, Sampled size: 9825, Classes: [0, 1, 2]\n",
      "Epoch 64/100, Batch 20/30, Loss: 0.4075, Sampled size: 10517, Classes: [0, 1, 2]\n",
      "Epoch 64/100, Batch 25/30, Loss: 0.5375, Sampled size: 12288, Classes: [0, 1, 2]\n",
      "Epoch 64/100, Average Loss: 0.4282, Valid Batches: 30/30, Learning Rate: 0.000031\n",
      "Épocas sin mejora: 10/10\n",
      "Early stopping activado tras 64 épocas. Mejor pérdida: 0.4272\n",
      "Cargado el mejor modelo desde trained_models/checkpoints/best_supervised_classifier.pth con pérdida: 0.4272\n",
      "Clasificador final guardado en 'trained_models/checkpoints/supervised_classifier_final.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1384671/3338365319.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(best_model_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "# Directorio para checkpoints\n",
    "output_dir = \"trained_models/checkpoints\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Variables para early stopping y checkpoints\n",
    "best_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "best_model_path = os.path.join(output_dir, \"best_supervised_classifier.pth\")\n",
    "\n",
    "# Entrenamiento del clasificador con muestreo balanceado, scheduler y early stopping\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    valid_batches = 0\n",
    "    \n",
    "    classifier.train()  # Modo entrenamiento\n",
    "    \n",
    "    for batch_idx, (embeddings, labels) in enumerate(loader):\n",
    "        embeddings = embeddings.to(device)  # [1, 48, 128, 128, 128]\n",
    "        labels = labels.to(device)  # [1, 128, 128, 128]\n",
    "        \n",
    "        # Reorganizar para procesar vóxeles\n",
    "        embeddings = embeddings.squeeze(0).permute(1, 2, 3, 0)  # [128, 128, 128, 48]\n",
    "        labels = labels.squeeze(0)  # [128, 128, 128]\n",
    "        \n",
    "        embeddings_flat = embeddings.reshape(-1, 48)  # [2097152, 48]\n",
    "        labels_flat = labels.reshape(-1)  # [2097152]\n",
    "        \n",
    "        # Muestreo estratificado balanceado\n",
    "        classes = torch.unique(labels_flat)\n",
    "        if len(classes) < 2:\n",
    "            print(f\"Batch {batch_idx}: Solo una clase presente ({classes.tolist()}), saltando\")\n",
    "            continue\n",
    "        \n",
    "        sampled_embeddings = []\n",
    "        sampled_labels = []\n",
    "        \n",
    "        for cls in classes:\n",
    "            cls_indices = (labels_flat == cls).nonzero(as_tuple=True)[0]\n",
    "            cls_size = cls_indices.shape[0]\n",
    "            if cls_size > sample_size_per_class:\n",
    "                indices = torch.randperm(cls_size)[:sample_size_per_class]\n",
    "                cls_indices = cls_indices[indices]\n",
    "            sampled_embeddings.append(embeddings_flat[cls_indices])\n",
    "            sampled_labels.append(labels_flat[cls_indices])\n",
    "        \n",
    "        embeddings_sampled = torch.cat(sampled_embeddings, dim=0)\n",
    "        labels_sampled = torch.cat(sampled_labels, dim=0)\n",
    "        \n",
    "        # Obtener representaciones contrastivas\n",
    "        with torch.no_grad():\n",
    "            z = projection_head(embeddings_sampled)  # [N, 128]\n",
    "            z = F.normalize(z, dim=1)\n",
    "        \n",
    "        # Clasificación\n",
    "        logits = classifier(z)\n",
    "        loss = criterion(logits, labels_sampled)\n",
    "        \n",
    "        # Optimización\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        valid_batches += 1\n",
    "        \n",
    "        if batch_idx % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(loader)}, Loss: {loss.item():.4f}, \"\n",
    "                  f\"Sampled size: {embeddings_sampled.shape[0]}, Classes: {torch.unique(labels_sampled).tolist()}\")\n",
    "    \n",
    "    # Calcular pérdida promedio\n",
    "    avg_loss = total_loss / max(valid_batches, 1)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}, Valid Batches: {valid_batches}/{len(loader)}, \"\n",
    "          f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Scheduler: ajustar tasa de aprendizaje basada en la pérdida promedio\n",
    "    scheduler.step(avg_loss)\n",
    "    \n",
    "    # Checkpoint: guardar el mejor modelo\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': classifier.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': best_loss,\n",
    "        }, best_model_path)\n",
    "        print(f\"Guardado checkpoint con mejor pérdida: {best_loss:.4f}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"Épocas sin mejora: {epochs_no_improve}/{patience}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f\"Early stopping activado tras {epoch+1} épocas. Mejor pérdida: {best_loss:.4f}\")\n",
    "        break\n",
    "\n",
    "# Cargar el mejor modelo al final (opcional)\n",
    "checkpoint = torch.load(best_model_path, map_location=device)\n",
    "classifier.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Cargado el mejor modelo desde {best_model_path} con pérdida: {checkpoint['loss']:.4f}\")\n",
    "\n",
    "# Guardar el modelo final (opcional)\n",
    "torch.save(classifier.state_dict(), os.path.join(output_dir, \"supervised_classifier_final.pth\"))\n",
    "print(\"Clasificador final guardado en 'trained_models/checkpoints/supervised_classifier_final.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1384671/2256524057.py:37: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  projection_head.load_state_dict(torch.load(\"trained_models/checkpoints_contrastive/contrastive_projection_head_final.pth\", map_location=device))\n",
      "/tmp/ipykernel_1384671/2256524057.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  classifier.load_state_dict(torch.load(\"trained_models/checkpoints/supervised_classifier_final.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapas de probabilidad para caso 0, shape: torch.Size([3, 128, 128, 128])\n",
      "Guardado mapa de probabilidad como NIfTI en trained_models/mapas/probability_maps_case_0.nii.gz\n",
      "Guardadas etiquetas como NIfTI en trained_models/mapas/labels_case_0.nii.gz\n",
      "Guardada segmentación semántica como NIfTI en trained_models/mapas/segmentation_case_0.nii.gz\n",
      "Mapas de probabilidad para caso 1, shape: torch.Size([3, 128, 128, 128])\n",
      "Guardado mapa de probabilidad como NIfTI en trained_models/mapas/probability_maps_case_1.nii.gz\n",
      "Guardadas etiquetas como NIfTI en trained_models/mapas/labels_case_1.nii.gz\n",
      "Guardada segmentación semántica como NIfTI en trained_models/mapas/segmentation_case_1.nii.gz\n",
      "Mapas de probabilidad para caso 2, shape: torch.Size([3, 128, 128, 128])\n",
      "Guardado mapa de probabilidad como NIfTI en trained_models/mapas/probability_maps_case_2.nii.gz\n",
      "Guardadas etiquetas como NIfTI en trained_models/mapas/labels_case_2.nii.gz\n",
      "Guardada segmentación semántica como NIfTI en trained_models/mapas/segmentation_case_2.nii.gz\n",
      "Mapas de probabilidad para caso 3, shape: torch.Size([3, 128, 128, 128])\n",
      "Guardado mapa de probabilidad como NIfTI en trained_models/mapas/probability_maps_case_3.nii.gz\n",
      "Guardadas etiquetas como NIfTI en trained_models/mapas/labels_case_3.nii.gz\n",
      "Guardada segmentación semántica como NIfTI en trained_models/mapas/segmentation_case_3.nii.gz\n",
      "Mapas de probabilidad para caso 4, shape: torch.Size([3, 128, 128, 128])\n",
      "Guardado mapa de probabilidad como NIfTI en trained_models/mapas/probability_maps_case_4.nii.gz\n",
      "Guardadas etiquetas como NIfTI en trained_models/mapas/labels_case_4.nii.gz\n",
      "Guardada segmentación semántica como NIfTI en trained_models/mapas/segmentation_case_4.nii.gz\n",
      "Mapas de probabilidad para caso 5, shape: torch.Size([3, 128, 128, 128])\n",
      "Guardado mapa de probabilidad como NIfTI en trained_models/mapas/probability_maps_case_5.nii.gz\n",
      "Guardadas etiquetas como NIfTI en trained_models/mapas/labels_case_5.nii.gz\n",
      "Guardada segmentación semántica como NIfTI en trained_models/mapas/segmentation_case_5.nii.gz\n",
      "Mapas de probabilidad para caso 6, shape: torch.Size([3, 128, 128, 128])\n",
      "Guardado mapa de probabilidad como NIfTI en trained_models/mapas/probability_maps_case_6.nii.gz\n",
      "Guardadas etiquetas como NIfTI en trained_models/mapas/labels_case_6.nii.gz\n",
      "Guardada segmentación semántica como NIfTI en trained_models/mapas/segmentation_case_6.nii.gz\n",
      "Mapas de probabilidad para caso 7, shape: torch.Size([3, 128, 128, 128])\n",
      "Guardado mapa de probabilidad como NIfTI en trained_models/mapas/probability_maps_case_7.nii.gz\n",
      "Guardadas etiquetas como NIfTI en trained_models/mapas/labels_case_7.nii.gz\n",
      "Guardada segmentación semántica como NIfTI en trained_models/mapas/segmentation_case_7.nii.gz\n",
      "Mapas de probabilidad para caso 8, shape: torch.Size([3, 128, 128, 128])\n",
      "Guardado mapa de probabilidad como NIfTI en trained_models/mapas/probability_maps_case_8.nii.gz\n",
      "Guardadas etiquetas como NIfTI en trained_models/mapas/labels_case_8.nii.gz\n",
      "Guardada segmentación semántica como NIfTI en trained_models/mapas/segmentation_case_8.nii.gz\n",
      "Mapas de probabilidad para caso 9, shape: torch.Size([3, 128, 128, 128])\n",
      "Guardado mapa de probabilidad como NIfTI en trained_models/mapas/probability_maps_case_9.nii.gz\n",
      "Guardadas etiquetas como NIfTI en trained_models/mapas/labels_case_9.nii.gz\n",
      "Guardada segmentación semántica como NIfTI en trained_models/mapas/segmentation_case_9.nii.gz\n",
      "Mapas de probabilidad para caso 10, shape: torch.Size([3, 128, 128, 128])\n",
      "Guardado mapa de probabilidad como NIfTI en trained_models/mapas/probability_maps_case_10.nii.gz\n",
      "Guardadas etiquetas como NIfTI en trained_models/mapas/labels_case_10.nii.gz\n",
      "Guardada segmentación semántica como NIfTI en trained_models/mapas/segmentation_case_10.nii.gz\n",
      "Mapas de probabilidad para caso 11, shape: torch.Size([3, 128, 128, 128])\n",
      "Guardado mapa de probabilidad como NIfTI en trained_models/mapas/probability_maps_case_11.nii.gz\n",
      "Guardadas etiquetas como NIfTI en trained_models/mapas/labels_case_11.nii.gz\n",
      "Guardada segmentación semántica como NIfTI en trained_models/mapas/segmentation_case_11.nii.gz\n",
      "Mapas de probabilidad para caso 12, shape: torch.Size([3, 128, 128, 128])\n",
      "Guardado mapa de probabilidad como NIfTI en trained_models/mapas/probability_maps_case_12.nii.gz\n",
      "Guardadas etiquetas como NIfTI en trained_models/mapas/labels_case_12.nii.gz\n",
      "Guardada segmentación semántica como NIfTI en trained_models/mapas/segmentation_case_12.nii.gz\n",
      "Mapas de probabilidad para caso 13, shape: torch.Size([3, 128, 128, 128])\n",
      "Guardado mapa de probabilidad como NIfTI en trained_models/mapas/probability_maps_case_13.nii.gz\n",
      "Guardadas etiquetas como NIfTI en trained_models/mapas/labels_case_13.nii.gz\n",
      "Guardada segmentación semántica como NIfTI en trained_models/mapas/segmentation_case_13.nii.gz\n",
      "Mapas de probabilidad para caso 14, shape: torch.Size([3, 128, 128, 128])\n",
      "Guardado mapa de probabilidad como NIfTI en trained_models/mapas/probability_maps_case_14.nii.gz\n",
      "Guardadas etiquetas como NIfTI en trained_models/mapas/labels_case_14.nii.gz\n",
      "Guardada segmentación semántica como NIfTI en trained_models/mapas/segmentation_case_14.nii.gz\n",
      "Mapas de probabilidad para caso 15, shape: torch.Size([3, 128, 128, 128])\n",
      "Guardado mapa de probabilidad como NIfTI en trained_models/mapas/probability_maps_case_15.nii.gz\n",
      "Guardadas etiquetas como NIfTI en trained_models/mapas/labels_case_15.nii.gz\n",
      "Guardada segmentación semántica como NIfTI en trained_models/mapas/segmentation_case_15.nii.gz\n",
      "Mapas de probabilidad para caso 16, shape: torch.Size([3, 128, 128, 128])\n",
      "Guardado mapa de probabilidad como NIfTI en trained_models/mapas/probability_maps_case_16.nii.gz\n",
      "Guardadas etiquetas como NIfTI en trained_models/mapas/labels_case_16.nii.gz\n",
      "Guardada segmentación semántica como NIfTI en trained_models/mapas/segmentation_case_16.nii.gz\n",
      "Mapas de probabilidad para caso 17, shape: torch.Size([3, 128, 128, 128])\n",
      "Guardado mapa de probabilidad como NIfTI en trained_models/mapas/probability_maps_case_17.nii.gz\n",
      "Guardadas etiquetas como NIfTI en trained_models/mapas/labels_case_17.nii.gz\n",
      "Guardada segmentación semántica como NIfTI en trained_models/mapas/segmentation_case_17.nii.gz\n",
      "Mapas de probabilidad para caso 18, shape: torch.Size([3, 128, 128, 128])\n",
      "Guardado mapa de probabilidad como NIfTI en trained_models/mapas/probability_maps_case_18.nii.gz\n",
      "Guardadas etiquetas como NIfTI en trained_models/mapas/labels_case_18.nii.gz\n",
      "Guardada segmentación semántica como NIfTI en trained_models/mapas/segmentation_case_18.nii.gz\n",
      "Mapas de probabilidad para caso 19, shape: torch.Size([3, 128, 128, 128])\n",
      "Guardado mapa de probabilidad como NIfTI en trained_models/mapas/probability_maps_case_19.nii.gz\n",
      "Guardadas etiquetas como NIfTI en trained_models/mapas/labels_case_19.nii.gz\n",
      "Guardada segmentación semántica como NIfTI en trained_models/mapas/segmentation_case_19.nii.gz\n",
      "Mapas de probabilidad para caso 20, shape: torch.Size([3, 128, 128, 128])\n",
      "Guardado mapa de probabilidad como NIfTI en trained_models/mapas/probability_maps_case_20.nii.gz\n",
      "Guardadas etiquetas como NIfTI en trained_models/mapas/labels_case_20.nii.gz\n",
      "Guardada segmentación semántica como NIfTI en trained_models/mapas/segmentation_case_20.nii.gz\n",
      "Mapas de probabilidad para caso 21, shape: torch.Size([3, 128, 128, 128])\n",
      "Guardado mapa de probabilidad como NIfTI en trained_models/mapas/probability_maps_case_21.nii.gz\n",
      "Guardadas etiquetas como NIfTI en trained_models/mapas/labels_case_21.nii.gz\n",
      "Guardada segmentación semántica como NIfTI en trained_models/mapas/segmentation_case_21.nii.gz\n",
      "Mapas de probabilidad para caso 22, shape: torch.Size([3, 128, 128, 128])\n",
      "Guardado mapa de probabilidad como NIfTI en trained_models/mapas/probability_maps_case_22.nii.gz\n",
      "Guardadas etiquetas como NIfTI en trained_models/mapas/labels_case_22.nii.gz\n",
      "Guardada segmentación semántica como NIfTI en trained_models/mapas/segmentation_case_22.nii.gz\n",
      "Mapas de probabilidad para caso 23, shape: torch.Size([3, 128, 128, 128])\n",
      "Guardado mapa de probabilidad como NIfTI en trained_models/mapas/probability_maps_case_23.nii.gz\n",
      "Guardadas etiquetas como NIfTI en trained_models/mapas/labels_case_23.nii.gz\n",
      "Guardada segmentación semántica como NIfTI en trained_models/mapas/segmentation_case_23.nii.gz\n",
      "Mapas de probabilidad para caso 24, shape: torch.Size([3, 128, 128, 128])\n",
      "Guardado mapa de probabilidad como NIfTI en trained_models/mapas/probability_maps_case_24.nii.gz\n",
      "Guardadas etiquetas como NIfTI en trained_models/mapas/labels_case_24.nii.gz\n",
      "Guardada segmentación semántica como NIfTI en trained_models/mapas/segmentation_case_24.nii.gz\n",
      "Mapas de probabilidad para caso 25, shape: torch.Size([3, 128, 128, 128])\n",
      "Guardado mapa de probabilidad como NIfTI en trained_models/mapas/probability_maps_case_25.nii.gz\n",
      "Guardadas etiquetas como NIfTI en trained_models/mapas/labels_case_25.nii.gz\n",
      "Guardada segmentación semántica como NIfTI en trained_models/mapas/segmentation_case_25.nii.gz\n",
      "Mapas de probabilidad para caso 26, shape: torch.Size([3, 128, 128, 128])\n",
      "Guardado mapa de probabilidad como NIfTI en trained_models/mapas/probability_maps_case_26.nii.gz\n",
      "Guardadas etiquetas como NIfTI en trained_models/mapas/labels_case_26.nii.gz\n",
      "Guardada segmentación semántica como NIfTI en trained_models/mapas/segmentation_case_26.nii.gz\n",
      "Mapas de probabilidad para caso 27, shape: torch.Size([3, 128, 128, 128])\n",
      "Guardado mapa de probabilidad como NIfTI en trained_models/mapas/probability_maps_case_27.nii.gz\n",
      "Guardadas etiquetas como NIfTI en trained_models/mapas/labels_case_27.nii.gz\n",
      "Guardada segmentación semántica como NIfTI en trained_models/mapas/segmentation_case_27.nii.gz\n",
      "Mapas de probabilidad para caso 28, shape: torch.Size([3, 128, 128, 128])\n",
      "Guardado mapa de probabilidad como NIfTI en trained_models/mapas/probability_maps_case_28.nii.gz\n",
      "Guardadas etiquetas como NIfTI en trained_models/mapas/labels_case_28.nii.gz\n",
      "Guardada segmentación semántica como NIfTI en trained_models/mapas/segmentation_case_28.nii.gz\n",
      "Mapas de probabilidad para caso 29, shape: torch.Size([3, 128, 128, 128])\n",
      "Guardado mapa de probabilidad como NIfTI en trained_models/mapas/probability_maps_case_29.nii.gz\n",
      "Guardadas etiquetas como NIfTI en trained_models/mapas/labels_case_29.nii.gz\n",
      "Guardada segmentación semántica como NIfTI en trained_models/mapas/segmentation_case_29.nii.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import nibabel as nib\n",
    "\n",
    "# Dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Función para generar mapas de probabilidad\n",
    "def generate_probability_maps(embeddings, projection_head, classifier, device):\n",
    "    \"\"\"\n",
    "    embeddings: tensor [1, 48, 128, 128, 128] - Características de SwinUNETR\n",
    "    Retorna: mapas de probabilidad [3, 128, 128, 128]\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        embeddings = embeddings.to(device).squeeze(0).permute(1, 2, 3, 0)  # [128, 128, 128, 48]\n",
    "        embeddings_flat = embeddings.reshape(-1, 48)  # [2097152, 48]\n",
    "        \n",
    "        z = projection_head(embeddings_flat)  # [2097152, 128]\n",
    "        z = F.normalize(z, dim=1)\n",
    "        \n",
    "        logits = classifier(z)  # [2097152, 3]\n",
    "        probs = F.softmax(logits, dim=1)  # [2097152, 3]\n",
    "        \n",
    "        probs = probs.view(128, 128, 128, 3).permute(3, 0, 1, 2)  # [3, 128, 128, 128]\n",
    "        return probs\n",
    "\n",
    "dataset = EmbeddingDataset(embedding_dir=\"Dataset/contrastive_voxel_wise/embeddings\", \n",
    "                          label_dir=\"Dataset/contrastive_voxel_wise/labels\")\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "# Cargar modelos (asumiendo que ya los tienes cargados)\n",
    "projection_head = ProjectionHead(input_dim=48).to(device)\n",
    "projection_head.load_state_dict(torch.load(\"trained_models/checkpoints_contrastive/contrastive_projection_head_final.pth\", map_location=device))\n",
    "projection_head.eval()\n",
    "\n",
    "classifier = Classifier(input_dim=128, num_classes=3).to(device)\n",
    "classifier.load_state_dict(torch.load(\"trained_models/checkpoints/supervised_classifier_final.pth\", map_location=device))\n",
    "classifier.eval()\n",
    "\n",
    "# Directorio de salida\n",
    "output_dir = \"trained_models/mapas\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Procesar y guardar como NIfTI\n",
    "for idx, (embeddings, labels) in enumerate(loader):\n",
    "    # Generar mapas de probabilidad\n",
    "    prob_maps = generate_probability_maps(embeddings, projection_head, classifier, device)\n",
    "    print(f\"Mapas de probabilidad para caso {idx}, shape: {prob_maps.shape}\")\n",
    "    \n",
    "    # Convertir mapas de probabilidad a numpy y ajustar formato para NIfTI\n",
    "    prob_maps_np = prob_maps.cpu().numpy()  # [3, 128, 128, 128]\n",
    "    prob_maps_np_nifti = np.transpose(prob_maps_np, (1, 2, 3, 0))  # [128, 128, 128, 3] para NIfTI\n",
    "    \n",
    "    # Generar segmentación semántica (clase más probable por vóxel)\n",
    "    segmentation = np.argmax(prob_maps_np, axis=0)  # [128, 128, 128], valores 0, 1, 2\n",
    "    segmentation_np = segmentation.astype(np.uint8)  # Convertir a uint8 para NIfTI\n",
    "    \n",
    "    # Convertir etiquetas a numpy\n",
    "    labels = labels.squeeze(0)  # [128, 128, 128]\n",
    "    labels_np = labels.cpu().numpy().astype(np.uint8)  # Convertir a uint8\n",
    "    \n",
    "    # Crear imágenes NIfTI con matriz afín identidad\n",
    "    affine = np.eye(4)  # Ajusta si tienes una matriz afín real\n",
    "    \n",
    "    # Guardar mapas de probabilidad\n",
    "    nifti_prob_img = nib.Nifti1Image(prob_maps_np_nifti, affine)\n",
    "    prob_output_path = os.path.join(output_dir, f\"probability_maps_case_{idx}.nii.gz\")\n",
    "    nib.save(nifti_prob_img, prob_output_path)\n",
    "    print(f\"Guardado mapa de probabilidad como NIfTI en {prob_output_path}\")\n",
    "    \n",
    "    # Guardar etiquetas\n",
    "    nifti_label_img = nib.Nifti1Image(labels_np, affine)\n",
    "    label_output_path = os.path.join(output_dir, f\"labels_case_{idx}.nii.gz\")\n",
    "    nib.save(nifti_label_img, label_output_path)\n",
    "    print(f\"Guardadas etiquetas como NIfTI en {label_output_path}\")\n",
    "    \n",
    "    # Guardar segmentación semántica\n",
    "    nifti_seg_img = nib.Nifti1Image(segmentation_np, affine)\n",
    "    seg_output_path = os.path.join(output_dir, f\"segmentation_case_{idx}.nii.gz\")\n",
    "    nib.save(nifti_seg_img, seg_output_path)\n",
    "    print(f\"Guardada segmentación semántica como NIfTI en {seg_output_path}\")\n",
    "    \n",
    "    # break  # Descomenta si solo quieres procesar un caso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapas de probabilidad para caso 0, shape: torch.Size([3, 128, 128, 128])\n",
      "Caso 0 - Dice: [0.9952287704236578, 0.4777317724105649, 0.47741541993188], Sensitivity: [0.9905944306217045, 0.43054134937945443, 0.8477956166636216], Precision: [0.9999066760968015, 0.5365404588548512, 0.3322596753974467]\n",
      "Guardado mapa de probabilidad en trained_models/mapas/probability_maps_case_0.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas/labels_case_0.nii.gz\n",
      "Guardada segmentación en trained_models/mapas/segmentation_case_0.nii.gz\n",
      "Mapas de probabilidad para caso 1, shape: torch.Size([3, 128, 128, 128])\n",
      "Caso 1 - Dice: [0.990844615285207, 0.1766707244693114, 0.7934794687948444], Sensitivity: [0.9820132457643929, 0.5327560239960926, 0.8838702689183321], Precision: [0.9998362695159455, 0.10589335827480287, 0.7198613736519237]\n",
      "Guardado mapa de probabilidad en trained_models/mapas/probability_maps_case_1.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas/labels_case_1.nii.gz\n",
      "Guardada segmentación en trained_models/mapas/segmentation_case_1.nii.gz\n",
      "Mapas de probabilidad para caso 2, shape: torch.Size([3, 128, 128, 128])\n",
      "Caso 2 - Dice: [0.9939182695857535, 0.15496489467757585, 0.6293267984488483], Sensitivity: [0.9881322932828394, 0.659786476633528, 0.7302055992601594], Precision: [0.999772404176554, 0.08779240457961017, 0.5529377665292146]\n",
      "Guardado mapa de probabilidad en trained_models/mapas/probability_maps_case_2.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas/labels_case_2.nii.gz\n",
      "Guardada segmentación en trained_models/mapas/segmentation_case_2.nii.gz\n",
      "Mapas de probabilidad para caso 3, shape: torch.Size([3, 128, 128, 128])\n",
      "Caso 3 - Dice: [0.9934964737568004, 0.10076045626959991, 0.6064102232297796], Sensitivity: [0.9871401381506891, 0.5243010750433114, 0.7418354429910231], Precision: [0.9999351985744661, 0.05573590599169055, 0.5127969549585336]\n",
      "Guardado mapa de probabilidad en trained_models/mapas/probability_maps_case_3.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas/labels_case_3.nii.gz\n",
      "Guardada segmentación en trained_models/mapas/segmentation_case_3.nii.gz\n",
      "Mapas de probabilidad para caso 4, shape: torch.Size([3, 128, 128, 128])\n",
      "Caso 4 - Dice: [0.9921302084103343, 0.038445711597247495, 0.8254455548627266], Sensitivity: [0.9846009069203687, 0.6337209296184874, 0.8185396190221521], Precision: [0.9997755513047665, 0.01982418914762582, 0.8324690115679151]\n",
      "Guardado mapa de probabilidad en trained_models/mapas/probability_maps_case_4.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas/labels_case_4.nii.gz\n",
      "Guardada segmentación en trained_models/mapas/segmentation_case_4.nii.gz\n",
      "Mapas de probabilidad para caso 5, shape: torch.Size([3, 128, 128, 128])\n",
      "Caso 5 - Dice: [0.9913607084114952, 0.37536529923418493, 0.3346830494216929], Sensitivity: [0.982915213069361, 0.33455527680213093, 0.9262873151457552], Precision: [0.9999525939185329, 0.4275147927940052, 0.20423900308437024]\n",
      "Guardado mapa de probabilidad en trained_models/mapas/probability_maps_case_5.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas/labels_case_5.nii.gz\n",
      "Guardada segmentación en trained_models/mapas/segmentation_case_5.nii.gz\n",
      "Mapas de probabilidad para caso 6, shape: torch.Size([3, 128, 128, 128])\n",
      "Caso 6 - Dice: [0.9953075519759503, 0.35136544660650093, 0.7921600834556443], Sensitivity: [0.9907701653886762, 0.5955480552398306, 0.7961095743642801], Precision: [0.9998866891057646, 0.24919300604407554, 0.7882495858554583]\n",
      "Guardado mapa de probabilidad en trained_models/mapas/probability_maps_case_6.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas/labels_case_6.nii.gz\n",
      "Guardada segmentación en trained_models/mapas/segmentation_case_6.nii.gz\n",
      "Mapas de probabilidad para caso 7, shape: torch.Size([3, 128, 128, 128])\n",
      "Caso 7 - Dice: [0.9934596291062167, 0.09703711476905456, 0.864759498414102], Sensitivity: [0.987198774788809, 0.4328963793904818, 0.8775565393233554], Precision: [0.9998004034587593, 0.05464287121086385, 0.852330320915471]\n",
      "Guardado mapa de probabilidad en trained_models/mapas/probability_maps_case_7.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas/labels_case_7.nii.gz\n",
      "Guardada segmentación en trained_models/mapas/segmentation_case_7.nii.gz\n",
      "Mapas de probabilidad para caso 8, shape: torch.Size([3, 128, 128, 128])\n",
      "Caso 8 - Dice: [0.9898267190604564, 0.023467369808214438, 0.7737019315915903], Sensitivity: [0.9799702790843485, 0.980176209294766, 0.7905919278796214], Precision: [0.9998834435904824, 0.01187585065752513, 0.7575185070846505]\n",
      "Guardado mapa de probabilidad en trained_models/mapas/probability_maps_case_8.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas/labels_case_8.nii.gz\n",
      "Guardada segmentación en trained_models/mapas/segmentation_case_8.nii.gz\n",
      "Mapas de probabilidad para caso 9, shape: torch.Size([3, 128, 128, 128])\n",
      "Caso 9 - Dice: [0.9937687298273168, 0.3974214952346227, 0.6725353586038971], Sensitivity: [0.9881876119286299, 0.8010750412324428, 0.5922305949058329], Precision: [0.9994132482366569, 0.26426237747808967, 0.7780346121169306]\n",
      "Guardado mapa de probabilidad en trained_models/mapas/probability_maps_case_9.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas/labels_case_9.nii.gz\n",
      "Guardada segmentación en trained_models/mapas/segmentation_case_9.nii.gz\n",
      "Mapas de probabilidad para caso 10, shape: torch.Size([3, 128, 128, 128])\n",
      "Caso 10 - Dice: [0.9897584220555152, 0.2485354422873266, 0.37671466248909413], Sensitivity: [0.9798564224571417, 0.3806212851429156, 0.6296296296030697], Precision: [0.9998625952684336, 0.18450665940828992, 0.26875776508870197]\n",
      "Guardado mapa de probabilidad en trained_models/mapas/probability_maps_case_10.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas/labels_case_10.nii.gz\n",
      "Guardada segmentación en trained_models/mapas/segmentation_case_10.nii.gz\n",
      "Mapas de probabilidad para caso 11, shape: torch.Size([3, 128, 128, 128])\n",
      "Caso 11 - Dice: [0.9867795286288504, 0.06513265434324889, 0.43936957220029227], Sensitivity: [0.974003858261803, 0.11669294885578435, 0.9675134572700431], Precision: [0.9998948015068196, 0.04517310543199675, 0.28422016311701104]\n",
      "Guardado mapa de probabilidad en trained_models/mapas/probability_maps_case_11.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas/labels_case_11.nii.gz\n",
      "Guardada segmentación en trained_models/mapas/segmentation_case_11.nii.gz\n",
      "Mapas de probabilidad para caso 12, shape: torch.Size([3, 128, 128, 128])\n",
      "Caso 12 - Dice: [0.9902503950967574, 0.13444174605721274, 0.605229393218416], Sensitivity: [0.9807890061495461, 0.2840415734398051, 0.8403556108744861], Precision: [0.9998961047180333, 0.08806134518546065, 0.47291167824032176]\n",
      "Guardado mapa de probabilidad en trained_models/mapas/probability_maps_case_12.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas/labels_case_12.nii.gz\n",
      "Guardada segmentación en trained_models/mapas/segmentation_case_12.nii.gz\n",
      "Mapas de probabilidad para caso 13, shape: torch.Size([3, 128, 128, 128])\n",
      "Caso 13 - Dice: [0.9899066615824047, 0.14536138079500716, 0.846281108096567], Sensitivity: [0.9807906812705909, 0.567667193159967, 0.854790195612242], Precision: [0.9991936890496292, 0.08335266372629829, 0.8379397597861553]\n",
      "Guardado mapa de probabilidad en trained_models/mapas/probability_maps_case_13.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas/labels_case_13.nii.gz\n",
      "Guardada segmentación en trained_models/mapas/segmentation_case_13.nii.gz\n",
      "Mapas de probabilidad para caso 14, shape: torch.Size([3, 128, 128, 128])\n",
      "Caso 14 - Dice: [0.9949470316437937, 0.0, 0.7804458708145975], Sensitivity: [0.9899699282810493, 0.0, 0.7978299587364588], Precision: [0.9999744329518927, 0.0, 0.7638031992290444]\n",
      "Guardado mapa de probabilidad en trained_models/mapas/probability_maps_case_14.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas/labels_case_14.nii.gz\n",
      "Guardada segmentación en trained_models/mapas/segmentation_case_14.nii.gz\n",
      "Mapas de probabilidad para caso 15, shape: torch.Size([3, 128, 128, 128])\n",
      "Caso 15 - Dice: [0.9857961037020491, 0.015594257814967712, 0.6743013781973], Sensitivity: [0.9720711901185656, 0.19657072859977295, 0.9655879273828686], Precision: [0.999914138764164, 0.008119182516994153, 0.5180288742654368]\n",
      "Guardado mapa de probabilidad en trained_models/mapas/probability_maps_case_15.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas/labels_case_15.nii.gz\n",
      "Guardada segmentación en trained_models/mapas/segmentation_case_15.nii.gz\n",
      "Mapas de probabilidad para caso 16, shape: torch.Size([3, 128, 128, 128])\n",
      "Caso 16 - Dice: [0.9880707503136877, 0.005822416302626711, 0.828482885888586], Sensitivity: [0.9773430938180196, 0.08924652517246047, 0.8573137511947119], Precision: [0.9990365213877124, 0.0030093734582385453, 0.8015280547512327]\n",
      "Guardado mapa de probabilidad en trained_models/mapas/probability_maps_case_16.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas/labels_case_16.nii.gz\n",
      "Guardada segmentación en trained_models/mapas/segmentation_case_16.nii.gz\n",
      "Mapas de probabilidad para caso 17, shape: torch.Size([3, 128, 128, 128])\n",
      "Caso 17 - Dice: [0.9913603987468181, 0.0, 0.6454774513084486], Sensitivity: [0.9829699620983552, 0.0, 0.810016345539537], Precision: [0.9998953067802687, 0.0, 0.5364983583186352]\n",
      "Guardado mapa de probabilidad en trained_models/mapas/probability_maps_case_17.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas/labels_case_17.nii.gz\n",
      "Guardada segmentación en trained_models/mapas/segmentation_case_17.nii.gz\n",
      "Mapas de probabilidad para caso 18, shape: torch.Size([3, 128, 128, 128])\n",
      "Caso 18 - Dice: [0.9921604075131669, 0.0, 0.5285976213183529], Sensitivity: [0.9844773383177582, 0.0, 0.9419418261179702], Precision: [0.9999643405463126, 0.0, 0.36738238079373486]\n",
      "Guardado mapa de probabilidad en trained_models/mapas/probability_maps_case_18.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas/labels_case_18.nii.gz\n",
      "Guardada segmentación en trained_models/mapas/segmentation_case_18.nii.gz\n",
      "Mapas de probabilidad para caso 19, shape: torch.Size([3, 128, 128, 128])\n",
      "Caso 19 - Dice: [0.9944881369435715, 0.14366700317281456, 0.6497328335463407], Sensitivity: [0.9890590830694129, 0.6339958871597012, 0.8704524813894731], Precision: [0.9999771211265946, 0.08101243649667084, 0.5183064541617105]\n",
      "Guardado mapa de probabilidad en trained_models/mapas/probability_maps_case_19.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas/labels_case_19.nii.gz\n",
      "Guardada segmentación en trained_models/mapas/segmentation_case_19.nii.gz\n",
      "Mapas de probabilidad para caso 20, shape: torch.Size([3, 128, 128, 128])\n",
      "Caso 20 - Dice: [0.9928745879565217, 0.02605641780751918, 0.7505504579908436], Sensitivity: [0.9861895421432697, 0.03889734482683962, 0.9036244598988015], Precision: [0.9996508836567303, 0.019589472785998683, 0.6418252479142077]\n",
      "Guardado mapa de probabilidad en trained_models/mapas/probability_maps_case_20.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas/labels_case_20.nii.gz\n",
      "Guardada segmentación en trained_models/mapas/segmentation_case_20.nii.gz\n",
      "Mapas de probabilidad para caso 21, shape: torch.Size([3, 128, 128, 128])\n",
      "Caso 21 - Dice: [0.9897705937885978, 0.0, 0.518040002582458], Sensitivity: [0.9799344439134046, 0.0, 0.712434126167482], Precision: [0.9998062076659908, 0.0, 0.40698928011214197]\n",
      "Guardado mapa de probabilidad en trained_models/mapas/probability_maps_case_21.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas/labels_case_21.nii.gz\n",
      "Guardada segmentación en trained_models/mapas/segmentation_case_21.nii.gz\n",
      "Mapas de probabilidad para caso 22, shape: torch.Size([3, 128, 128, 128])\n",
      "Caso 22 - Dice: [0.990126416672129, 0.1092636579550073, 0.7948435767297505], Sensitivity: [0.980656150747387, 0.42141841724507684, 0.8000862535448512], Precision: [0.9997813762540502, 0.0627690859884068, 0.7896691593711218]\n",
      "Guardado mapa de probabilidad en trained_models/mapas/probability_maps_case_22.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas/labels_case_22.nii.gz\n",
      "Guardada segmentación en trained_models/mapas/segmentation_case_22.nii.gz\n",
      "Mapas de probabilidad para caso 23, shape: torch.Size([3, 128, 128, 128])\n",
      "Caso 23 - Dice: [0.9915475692910196, 0.0016657990628145965, 0.23502496667326428], Sensitivity: [0.9832410729699602, 0.0012503907469130367, 0.9725057621052838], Precision: [0.999995609224433, 0.002494543186001078, 0.13366370239328257]\n",
      "Guardado mapa de probabilidad en trained_models/mapas/probability_maps_case_23.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas/labels_case_23.nii.gz\n",
      "Guardada segmentación en trained_models/mapas/segmentation_case_23.nii.gz\n",
      "Mapas de probabilidad para caso 24, shape: torch.Size([3, 128, 128, 128])\n",
      "Caso 24 - Dice: [0.9907388158453214, 0.023454856684144123, 0.6847633337786829], Sensitivity: [0.9817288638921481, 0.16683937815189667, 0.8460008763070545], Precision: [0.9999156798076316, 0.012614094879436906, 0.5751472239276105]\n",
      "Guardado mapa de probabilidad en trained_models/mapas/probability_maps_case_24.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas/labels_case_24.nii.gz\n",
      "Guardada segmentación en trained_models/mapas/segmentation_case_24.nii.gz\n",
      "Mapas de probabilidad para caso 25, shape: torch.Size([3, 128, 128, 128])\n",
      "Caso 25 - Dice: [0.9906031655636433, 0.3657922168119384, 0.5013581900675248], Sensitivity: [0.9821276633542416, 0.36252762073169775, 0.7464336167766423], Precision: [0.9992262238312017, 0.3691161431594122, 0.37743539463019216]\n",
      "Guardado mapa de probabilidad en trained_models/mapas/probability_maps_case_25.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas/labels_case_25.nii.gz\n",
      "Guardada segmentación en trained_models/mapas/segmentation_case_25.nii.gz\n",
      "Mapas de probabilidad para caso 26, shape: torch.Size([3, 128, 128, 128])\n",
      "Caso 26 - Dice: [0.992602052027608, 0.3310998798160924, 0.6977050807704717], Sensitivity: [0.9854827777186147, 0.3002597712728715, 0.9593891305997913], Precision: [0.9998249362244681, 0.369000440321114, 0.5481822724269901]\n",
      "Guardado mapa de probabilidad en trained_models/mapas/probability_maps_case_26.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas/labels_case_26.nii.gz\n",
      "Guardada segmentación en trained_models/mapas/segmentation_case_26.nii.gz\n",
      "Mapas de probabilidad para caso 27, shape: torch.Size([3, 128, 128, 128])\n",
      "Caso 27 - Dice: [0.9938984072161082, 0.13298732408158523, 0.7202405982981384], Sensitivity: [0.9888824397466918, 0.4410752118060017, 0.6672386926182295], Precision: [0.9989655196937415, 0.07829725219005954, 0.782389471605713]\n",
      "Guardado mapa de probabilidad en trained_models/mapas/probability_maps_case_27.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas/labels_case_27.nii.gz\n",
      "Guardada segmentación en trained_models/mapas/segmentation_case_27.nii.gz\n",
      "Mapas de probabilidad para caso 28, shape: torch.Size([3, 128, 128, 128])\n",
      "Caso 28 - Dice: [0.9918879295353197, 0.07078898461980157, 0.417721996072333], Sensitivity: [0.9840316934818462, 0.11733408005675328, 0.8940787350037045], Precision: [0.9998706191770697, 0.050683440177732735, 0.272523851670293]\n",
      "Guardado mapa de probabilidad en trained_models/mapas/probability_maps_case_28.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas/labels_case_28.nii.gz\n",
      "Guardada segmentación en trained_models/mapas/segmentation_case_28.nii.gz\n",
      "Mapas de probabilidad para caso 29, shape: torch.Size([3, 128, 128, 128])\n",
      "Caso 29 - Dice: [0.9914956800356376, 0.008299087902008638, 0.7627420331199674], Sensitivity: [0.983594212279947, 0.6106194672253114, 0.7062908394141187], Precision: [0.9995251249496989, 0.004177935655669924, 0.8290009918312793]\n",
      "Guardado mapa de probabilidad en trained_models/mapas/probability_maps_case_29.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas/labels_case_29.nii.gz\n",
      "Guardada segmentación en trained_models/mapas/segmentation_case_29.nii.gz\n",
      "\n",
      "Clase 0 (Fondo):\n",
      "  Dice: 0.9916 ± 0.0023\n",
      "  Sensibilidad: 0.9836 ± 0.0045\n",
      "  Precisión: 0.9997 ± 0.0003\n",
      "\n",
      "Clase 1 (Vasogénico):\n",
      "  Dice: 0.1340 ± 0.1407\n",
      "  Sensibilidad: 0.3551 ± 0.2629\n",
      "  Precisión: 0.1092 ± 0.1430\n",
      "\n",
      "Clase 2 (Infiltrado):\n",
      "  Dice: 0.6409 ± 0.1637\n",
      "  Sensibilidad: 0.8250 ± 0.0993\n",
      "  Precisión: 0.5686 ± 0.2154\n"
     ]
    }
   ],
   "source": [
    "# Función para generar mapas de probabilidad\n",
    "def generate_probability_maps(embeddings, projection_head, classifier, device):\n",
    "    with torch.no_grad():\n",
    "        embeddings = embeddings.to(device).squeeze(0).permute(1, 2, 3, 0)\n",
    "        embeddings_flat = embeddings.reshape(-1, 48)\n",
    "        \n",
    "        z = projection_head(embeddings_flat)\n",
    "        z = F.normalize(z, dim=1)\n",
    "        \n",
    "        logits = classifier(z)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        \n",
    "        probs = probs.view(128, 128, 128, 3).permute(3, 0, 1, 2)\n",
    "        return probs\n",
    "\n",
    "# Funciones para calcular métricas\n",
    "def calculate_metrics(pred, true, num_classes=3):\n",
    "    dice_scores = []\n",
    "    sensitivity_scores = []\n",
    "    precision_scores = []\n",
    "    \n",
    "    for cls in range(num_classes):\n",
    "        pred_cls = (pred == cls).astype(np.uint8)\n",
    "        true_cls = (true == cls).astype(np.uint8)\n",
    "        \n",
    "        # True Positives (TP), False Positives (FP), False Negatives (FN)\n",
    "        tp = np.sum(pred_cls * true_cls)\n",
    "        fp = np.sum(pred_cls * (1 - true_cls))\n",
    "        fn = np.sum((1 - pred_cls) * true_cls)\n",
    "        \n",
    "        # Dice\n",
    "        dice = 2 * tp / (2 * tp + fp + fn + 1e-6)  # Evitar división por 0\n",
    "        dice_scores.append(dice)\n",
    "        \n",
    "        # Sensibilidad (Recall)\n",
    "        sensitivity = tp / (tp + fn + 1e-6)\n",
    "        sensitivity_scores.append(sensitivity)\n",
    "        \n",
    "        # Precisión\n",
    "        precision = tp / (tp + fp + 1e-6)\n",
    "        precision_scores.append(precision)\n",
    "    \n",
    "    return dice_scores, sensitivity_scores, precision_scores\n",
    "# Directorio de salida\n",
    "output_dir = \"trained_models/mapas\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Listas para almacenar métricas por caso\n",
    "all_dice = {0: [], 1: [], 2: []}  # Fondo, Vasogénico, Infiltrado\n",
    "all_sensitivity = {0: [], 1: [], 2: []}\n",
    "all_precision = {0: [], 1: [], 2: []}\n",
    "\n",
    "# Procesar y guardar como NIfTI\n",
    "for idx, (embeddings, labels) in enumerate(loader):\n",
    "    # Generar mapas de probabilidad\n",
    "    prob_maps = generate_probability_maps(embeddings, projection_head, classifier, device)\n",
    "    print(f\"Mapas de probabilidad para caso {idx}, shape: {prob_maps.shape}\")\n",
    "    \n",
    "    # Convertir mapas de probabilidad a numpy\n",
    "    prob_maps_np = prob_maps.cpu().numpy()  # [3, 128, 128, 128]\n",
    "    prob_maps_np_nifti = np.transpose(prob_maps_np, (1, 2, 3, 0))  # [128, 128, 128, 3]\n",
    "    \n",
    "    # Generar segmentación semántica\n",
    "    segmentation = np.argmax(prob_maps_np, axis=0)  # [128, 128, 128]\n",
    "    segmentation_np = segmentation.astype(np.uint8)\n",
    "    \n",
    "    # Convertir etiquetas a numpy\n",
    "    labels = labels.squeeze(0)  # [128, 128, 128]\n",
    "    labels_np = labels.cpu().numpy().astype(np.uint8)\n",
    "\n",
    "    # hacer cero segmentation_np en donde labels_np es cero\n",
    "    # segmentation_np[labels_np == 0] = 0\n",
    "    \n",
    "    # Calcular métricas\n",
    "    dice, sensitivity, precision = calculate_metrics(segmentation_np, labels_np)\n",
    "    for cls in range(3):\n",
    "        all_dice[cls].append(dice[cls])\n",
    "        all_sensitivity[cls].append(sensitivity[cls])\n",
    "        all_precision[cls].append(precision[cls])\n",
    "    \n",
    "    print(f\"Caso {idx} - Dice: {dice}, Sensitivity: {sensitivity}, Precision: {precision}\")\n",
    "    \n",
    "    # Crear imágenes NIfTI\n",
    "    affine = np.eye(4)\n",
    "    \n",
    "    # Guardar mapas de probabilidad\n",
    "    nifti_prob_img = nib.Nifti1Image(prob_maps_np_nifti, affine)\n",
    "    prob_output_path = os.path.join(output_dir, f\"probability_maps_case_{idx}.nii.gz\")\n",
    "    nib.save(nifti_prob_img, prob_output_path)\n",
    "    print(f\"Guardado mapa de probabilidad en {prob_output_path}\")\n",
    "    \n",
    "    # Guardar etiquetas\n",
    "    nifti_label_img = nib.Nifti1Image(labels_np, affine)\n",
    "    label_output_path = os.path.join(output_dir, f\"labels_case_{idx}.nii.gz\")\n",
    "    nib.save(nifti_label_img, label_output_path)\n",
    "    print(f\"Guardadas etiquetas en {label_output_path}\")\n",
    "    \n",
    "    # Guardar segmentación semántica\n",
    "    nifti_seg_img = nib.Nifti1Image(segmentation_np, affine)\n",
    "    seg_output_path = os.path.join(output_dir, f\"segmentation_case_{idx}.nii.gz\")\n",
    "    nib.save(nifti_seg_img, seg_output_path)\n",
    "    print(f\"Guardada segmentación en {seg_output_path}\")\n",
    "\n",
    "# Calcular promedios y desviaciones estándar\n",
    "class_names = [\"Fondo\", \"Vasogénico\", \"Infiltrado\"]\n",
    "for cls in range(3):\n",
    "    dice_mean = np.mean(all_dice[cls])\n",
    "    dice_std = np.std(all_dice[cls])\n",
    "    sens_mean = np.mean(all_sensitivity[cls])\n",
    "    sens_std = np.std(all_sensitivity[cls])\n",
    "    prec_mean = np.mean(all_precision[cls])\n",
    "    prec_std = np.std(all_precision[cls])\n",
    "    \n",
    "    print(f\"\\nClase {cls} ({class_names[cls]}):\")\n",
    "    print(f\"  Dice: {dice_mean:.4f} ± {dice_std:.4f}\")\n",
    "    print(f\"  Sensibilidad: {sens_mean:.4f} ± {sens_std:.4f}\")\n",
    "    print(f\"  Precisión: {prec_mean:.4f} ± {prec_std:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
