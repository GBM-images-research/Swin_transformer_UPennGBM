{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from monai.networks.nets import SwinUNETR\n",
    "import nibabel as nib\n",
    "from src.get_data import CustomDataset\n",
    "from monai.data import DataLoader\n",
    "from monai import transforms\n",
    "from src.custom_transforms import ConvertToMultiChannelBasedOnAnotatedInfiltration\n",
    "from monai.inferers import sliding_window_inference\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generar los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Modelos\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, input_dim=48, hidden_dim=128, output_dim=128):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim=128, num_classes=3):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Función para generar mapas de probabilidad y segmentación por lotes\n",
    "def generate_probability_maps(embeddings, projection_head, classifier, device, batch_size=100000):\n",
    "    with torch.no_grad():\n",
    "        embeddings = embeddings.permute(1, 2, 3, 0)  # [H, W, D, 48]\n",
    "        H, W, D = embeddings.shape[:3]\n",
    "        embeddings_flat = embeddings.reshape(-1, 48)  # [H*W*D, 48]\n",
    "        total_voxels = embeddings_flat.shape[0]\n",
    "        \n",
    "        prob_maps = torch.zeros(total_voxels, 3, device=device)\n",
    "        \n",
    "        for start_idx in range(0, total_voxels, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, total_voxels)\n",
    "            batch_embeddings = embeddings_flat[start_idx:end_idx].to(device)\n",
    "            \n",
    "            z = projection_head(batch_embeddings)\n",
    "            z = F.normalize(z, dim=1)\n",
    "            logits = classifier(z)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            \n",
    "            prob_maps[start_idx:end_idx] = probs\n",
    "            \n",
    "            del batch_embeddings, z, logits, probs\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        prob_maps = prob_maps.view(H, W, D, 3).permute(3, 0, 1, 2)  # [3, H, W, D]\n",
    "        segmentation = torch.argmax(prob_maps, dim=0).to(torch.uint8)  # [H, W, D]\n",
    "        \n",
    "        return prob_maps, segmentation\n",
    "\n",
    "\n",
    "def save_img(I_img,savename,header=None,affine=None):\n",
    "    if header is None or affine is None:\n",
    "        affine = np.diag([1, 1, 1, 1])\n",
    "        new_img = nib.nifti1.Nifti1Image(I_img, affine, header=None)\n",
    "    else:\n",
    "        new_img = nib.nifti1.Nifti1Image(I_img, affine, header=header)\n",
    "\n",
    "    nib.save(new_img, savename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 images and 1 labels.\n",
      "torch.Size([11, 240, 240, 155]) <class 'monai.data.meta_tensor.MetaTensor'>\n",
      "torch.Size([2, 240, 240, 155]) <class 'monai.data.meta_tensor.MetaTensor'>\n"
     ]
    }
   ],
   "source": [
    "# Configuración del DataLoader\n",
    "\n",
    "test_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\", \"label\"]), #Leer imagenes \n",
    "        ConvertToMultiChannelBasedOnAnotatedInfiltration(keys=\"label\"),\n",
    "        # transforms.CropForegroundd(\n",
    "        #     keys=[\"image\", \"label\"],\n",
    "        #     source_key=\"label\",\n",
    "        #     k_divisible=[128, 128, 128],\n",
    "        # ),\n",
    "        # transforms.RandSpatialCropd(\n",
    "        #     keys=[\"label\", \"image\"],\n",
    "        #     roi_size=[128, 128, 128],\n",
    "        #     random_size=False,\n",
    "        # ),   \n",
    "        transforms.NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True), #Normalizar intensidades\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset_path = './Dataset/Dataset_recurrence'\n",
    "test_set = CustomDataset(dataset_path, section=\"test\", transform=test_transform)  # Ajusta transform\n",
    "test_loader = DataLoader(test_set, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "# Obtener header + affine\n",
    "mri = nib.load('./Dataset/Dataset_recurrence/test/images/images_structural/UPENN-GBM-00036_11/UPENN-GBM-00036_11_T1GD.nii.gz')\n",
    "header = mri.header\n",
    "affine = mri.affine\n",
    "\n",
    "im_t = test_set[0]\n",
    "\n",
    "print(im_t['image'].shape, type(im_t['image']))\n",
    "print(im_t['label'].shape, type(im_t['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minigo/anaconda3/envs/monai_env/lib/python3.11/site-packages/monai/utils/deprecate_utils.py:221: FutureWarning: monai.networks.nets.swin_unetr SwinUNETR.__init__:img_size: Argument `img_size` has been deprecated since version 1.3. It will be removed in version 1.5. The img_size argument is not required anymore and checks on the input size are run during forward().\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (fc): Linear(in_features=128, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar SwinUNETR\n",
    "### Hyperparameter\n",
    "roi = (128, 128, 128)\n",
    "swin_model = SwinUNETR(\n",
    "    img_size=roi,\n",
    "    in_channels=11,\n",
    "    out_channels=2,  # mdificar con edema\n",
    "    feature_size=48,\n",
    "    drop_rate=0.0,\n",
    "    attn_drop_rate=0.0,\n",
    "    dropout_path_rate=0.0,\n",
    "    use_checkpoint=True,\n",
    ").to(device)\n",
    "\n",
    "# model_path = \"Dataset/model.pt\"\n",
    "model_path = \"artifacts/15cwmu45_best_model:v0/model.pt\"  \n",
    "\n",
    "# Load the model on CPU\n",
    "loaded_model = torch.load(model_path, map_location=torch.device(device))[\"state_dict\"]\n",
    "# model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "swin_model.load_state_dict(loaded_model)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "swin_model.eval()\n",
    "\n",
    "\n",
    "# Hook para capturar embeddings de decoder1.conv_block\n",
    "decoder_features = None\n",
    "def decoder_hook_fn(module, input, output):\n",
    "    global decoder_features\n",
    "    decoder_features = output\n",
    "\n",
    "hook_handle = swin_model.decoder1.conv_block.register_forward_hook(decoder_hook_fn)\n",
    "\n",
    "# Configurar sliding window inference\n",
    "roi_size = (128, 128, 128)\n",
    "model_inferer = partial(\n",
    "    sliding_window_inference,\n",
    "    roi_size=roi_size,\n",
    "    sw_batch_size=1,  # Número de ventanas por iteración\n",
    "    predictor=swin_model,\n",
    "    overlap=0.5,  # Solapamiento para suavizar bordes\n",
    ")\n",
    "\n",
    "# Cargar modelos contrastivo y clasificador\n",
    "projection_head = ProjectionHead(input_dim=48).to(device)\n",
    "projection_head.load_state_dict(torch.load(\"trained_models/checkpoints_contrastive/contrastive_projection_head_final.pth\", map_location=device))\n",
    "projection_head.eval()\n",
    "\n",
    "classifier = Classifier(input_dim=128, num_classes=3).to(device)\n",
    "classifier.load_state_dict(torch.load(\"trained_models/checkpoints/supervised_classifier_final.pth\", map_location=device))\n",
    "classifier.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando MRI 0, shape: torch.Size([1, 11, 240, 240, 155])\n",
      "Mapas de probabilidad generados, shape: torch.Size([3, 240, 240, 155])\n",
      "Segmentación generada, shape: torch.Size([240, 240, 155])\n",
      "Guardado mapa de probabilidad en trained_models/inference_results/probability_maps_mri_0.nii.gz\n",
      "Guardada segmentación en trained_models/inference_results/segmentation_mri_0.nii.gz\n",
      "Inferencia completada.\n"
     ]
    }
   ],
   "source": [
    "# Directorio de salida\n",
    "output_dir = \"trained_models/inference_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Pipeline de inferencia con sliding window\n",
    "for idx, batch_data in enumerate(test_loader):\n",
    "    mri = batch_data[\"image\"].to(device)  # [1, 11, 240, 240, 155]\n",
    "    print(f\"Procesando MRI {idx}, shape: {mri.shape}\")\n",
    "    \n",
    "    # Dimensiones del volumen completo\n",
    "    _, _, H, W, D = mri.shape\n",
    "    \n",
    "    # Buffer para embeddings completos y conteo de contribuciones (para solapamiento)\n",
    "    embeddings_full = torch.zeros(48, H, W, D, device=device)\n",
    "    count_map = torch.zeros(H, W, D, device=device)\n",
    "    \n",
    "    # Calcular posiciones de las ventanas\n",
    "    step_h = int(roi_size[0] * (1 - 0.25))\n",
    "    step_w = int(roi_size[1] * (1 - 0.25))\n",
    "    step_d = int(roi_size[2] * (1 - 0.25))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for h in range(0, H - roi_size[0] + 1, step_h):\n",
    "            for w in range(0, W - roi_size[1] + 1, step_w):\n",
    "                for d in range(0, D - roi_size[2] + 1, step_d):\n",
    "                    h_end = min(h + roi_size[0], H)\n",
    "                    w_end = min(w + roi_size[1], W)\n",
    "                    d_end = min(d + roi_size[2], D)\n",
    "                    \n",
    "                    # Extraer parche\n",
    "                    patch = mri[:, :, h:h_end, w:w_end, d:d_end]\n",
    "                    if patch.shape[2:] != torch.Size(roi_size):\n",
    "                        # Rellenar si el parche es más pequeño (bordes)\n",
    "                        pad_h = roi_size[0] - patch.shape[2]\n",
    "                        pad_w = roi_size[1] - patch.shape[3]\n",
    "                        pad_d = roi_size[2] - patch.shape[4]\n",
    "                        patch = F.pad(patch, (0, pad_d, 0, pad_w, 0, pad_h))\n",
    "                    \n",
    "                    # Obtener embeddings del parche\n",
    "                    _ = swin_model(patch)\n",
    "                    embeddings_patch = decoder_features.squeeze(0)  # [48, 128, 128, 128]\n",
    "                    \n",
    "                    # Acumular en el volumen completo\n",
    "                    embeddings_full[:, h:h_end, w:w_end, d:d_end] += embeddings_patch[:, :h_end-h, :w_end-w, :d_end-d]\n",
    "                    count_map[h:h_end, w:w_end, d:d_end] += 1\n",
    "                    \n",
    "                    del patch, embeddings_patch\n",
    "                    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Normalizar por el conteo de solapamientos\n",
    "    embeddings_full /= count_map.clamp(min=1).unsqueeze(0)  # [48, 240, 240, 155]\n",
    "    \n",
    "    # Generar mapas de probabilidad y segmentación\n",
    "    prob_maps, segmentation = generate_probability_maps(embeddings_full, projection_head, classifier, device, batch_size=100000)\n",
    "    print(f\"Mapas de probabilidad generados, shape: {prob_maps.shape}\")\n",
    "    print(f\"Segmentación generada, shape: {segmentation.shape}\")\n",
    "    \n",
    "    # Convertir a numpy para guardar\n",
    "    prob_maps_np = prob_maps.cpu().numpy()  # [3, 240, 240, 155]\n",
    "    prob_maps_np_nifti = np.transpose(prob_maps_np, (1, 2, 3, 0))  # [240, 240, 155, 3]\n",
    "    segmentation_np = segmentation.cpu().numpy()  # [240, 240, 155]\n",
    "    \n",
    "    # Crear imágenes NIfTI\n",
    "    # affine = np.eye(4)  # Ajusta si tienes una matriz afín real\n",
    "    \n",
    "    # Guardar mapas de probabilidad\n",
    "    nifti_prob_img = nib.Nifti1Image(prob_maps_np_nifti, affine)\n",
    "    prob_output_path = os.path.join(output_dir, f\"probability_maps_mri_{idx}.nii.gz\")\n",
    "    # nib.save(nifti_prob_img, prob_output_path)\n",
    "    save_img(\n",
    "            prob_maps_np_nifti, #output_tensor.numpy(),\n",
    "            prob_output_path,\n",
    "            header,\n",
    "            affine,\n",
    "        )\n",
    "    print(f\"Guardado mapa de probabilidad en {prob_output_path}\")\n",
    "    \n",
    "    # Guardar segmentación semántica\n",
    "    nifti_seg_img = nib.Nifti1Image(segmentation_np, affine)\n",
    "    seg_output_path = os.path.join(output_dir, f\"segmentation_mri_{idx}.nii.gz\")\n",
    "    save_img(\n",
    "            segmentation_np, #output_tensor.numpy(),\n",
    "            seg_output_path,\n",
    "            header,\n",
    "            affine,\n",
    "        )\n",
    "    print(f\"Guardada segmentación en {seg_output_path}\")\n",
    "    \n",
    "    # Liberar memoria\n",
    "    del mri, embeddings_full, count_map, prob_maps, segmentation\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Remover el hook\n",
    "hook_handle.remove()\n",
    "print(\"Inferencia completada.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
