{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading functions\n",
    "import os\n",
    "import time\n",
    "from monai.data import DataLoader, decollate_batch\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "\n",
    "from src.get_data import CustomDataset, CustomDatasetSeg\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from types import SimpleNamespace\n",
    "import wandb\n",
    "import logging\n",
    "\n",
    "#####\n",
    "import json\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai import transforms\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    Activations,\n",
    "    MapTransform,\n",
    "    Transform,\n",
    ")\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.utils.enums import MetricReduction\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai import data\n",
    "\n",
    "# from monai.data import decollate_batch\n",
    "from functools import partial\n",
    "from src.custom_transforms import (ConvertToMultiChannelBasedOnN_Froi, \n",
    "                                   ConvertToMultiChannelBasedOnAnotatedInfiltration, \n",
    "                                   masked, ConvertToMultiChannelBasedOnBratsClassesdI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trasnformaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/training2/miniconda3/envs/monai_env/lib/python3.11/site-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.croppad.dictionary CropForegroundd.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    }
   ],
   "source": [
    "roi = (128, 128, 128) # (220, 220, 155) (128, 128, 64)\n",
    "source_k=\"label\"\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        # ConvertToMultiChannelBasedOnN_Froi(keys=\"label\"),\n",
    "        # masked(keys=\"image\"),\n",
    "        # ConvertToMultiChannelBasedOnAnotatedInfiltration(keys=\"label\"),\n",
    "        ConvertToMultiChannelBasedOnBratsClassesdI(keys=\"label\"),\n",
    "        transforms.CropForegroundd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            source_key=source_k,\n",
    "            k_divisible=[roi[0], roi[1], roi[2]],\n",
    "        ),\n",
    "        transforms.RandSpatialCropd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            roi_size=[roi[0], roi[1], roi[2]],\n",
    "            random_size=False,\n",
    "        ),\n",
    "        transforms.NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "        \n",
    "    ]\n",
    ")\n",
    "val_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        # ConvertToMultiChannelBasedOnN_Froi(keys=\"label\"),\n",
    "        # masked(keys=\"image\"),\n",
    "        # ConvertToMultiChannelBasedOnAnotatedInfiltration(keys=\"label\"),\n",
    "        ConvertToMultiChannelBasedOnBratsClassesdI(keys=\"label\"),\n",
    "        transforms.RandSpatialCropd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            roi_size=[-1, -1, -1], #[224, 224, 128],\n",
    "            random_size=False,\n",
    "        ),\n",
    "        transforms.NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Crear el modelo\n",
    "######################\n",
    "\n",
    "### Hyperparameter\n",
    "roi = (128, 128, 128)  # (128, 128, 128)\n",
    "\n",
    "# Create Swin transformer\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def define_model(model_path):\n",
    "    model = SwinUNETR(\n",
    "        img_size=roi,\n",
    "        in_channels=11,\n",
    "        out_channels=2,  # mdificar con edema\n",
    "        feature_size=48, #48\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        dropout_path_rate=0.0,\n",
    "        use_checkpoint=True,\n",
    "    )\n",
    "\n",
    "    # # Load the best model\n",
    "    # model_path = \"artifacts/o9kppyr5_best_model:v0/model.pt\"\n",
    "\n",
    "    # Load the model on CPU\n",
    "    loaded_model = torch.load(model_path, map_location=torch.device('cuda:0'), weights_only=False)[\"state_dict\"]\n",
    "\n",
    "    # Load the state dictionary into the model\n",
    "    model.load_state_dict(loaded_model)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/training2/miniconda3/envs/monai_env/lib/python3.11/site-packages/monai/utils/deprecate_utils.py:221: FutureWarning: monai.networks.nets.swin_unetr SwinUNETR.__init__:img_size: Argument `img_size` has been deprecated since version 1.3. It will be removed in version 1.5. The img_size argument is not required anymore and checks on the input size are run during forward().\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SwinUNETR(\n",
       "  (swinViT): SwinTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv3d(11, 48, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (layers1): ModuleList(\n",
       "      (0): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=48, out_features=144, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=48, out_features=48, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (linear1): Linear(in_features=48, out_features=192, bias=True)\n",
       "              (linear2): Linear(in_features=192, out_features=48, bias=True)\n",
       "              (fn): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=384, out_features=96, bias=False)\n",
       "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layers2): ModuleList(\n",
       "      (0): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (linear1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (linear2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (fn): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=768, out_features=192, bias=False)\n",
       "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layers3): ModuleList(\n",
       "      (0): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (fn): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=1536, out_features=384, bias=False)\n",
       "          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layers4): ModuleList(\n",
       "      (0): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (linear1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (linear2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (fn): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder1): UnetrBasicBlock(\n",
       "    (layer): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(11, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(11, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder2): UnetrBasicBlock(\n",
       "    (layer): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder3): UnetrBasicBlock(\n",
       "    (layer): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder4): UnetrBasicBlock(\n",
       "    (layer): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder10): UnetrBasicBlock(\n",
       "    (layer): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(768, 768, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(768, 768, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder5): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(768, 384, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(768, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(768, 384, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder4): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(384, 192, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(384, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(384, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder3): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(192, 96, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(192, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(192, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder2): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(96, 48, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(96, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(96, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder1): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(48, 48, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(96, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(96, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (out): UnetOutBlock(\n",
       "    (conv): Convolution(\n",
       "      (conv): Conv3d(48, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo TC+Edema\n",
    "model1=define_model(\"artifacts/vtzpbajf_best_model:v0/model.pt\") # o9kppyr5\n",
    "model1.to(device)\n",
    "model1.eval()\n",
    "\n",
    "# Modelo Infitracion+Edema\n",
    "model2=define_model(\"artifacts/1dhzmigz_best_model:v0/model.pt\") # uixfayrn - rvu24jip\n",
    "model2.to(device)\n",
    "model2.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtener embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 images and 6 labels.\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 0\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 1\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 2\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 3\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 4\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 5\n"
     ]
    }
   ],
   "source": [
    "# Create dataset data loader\n",
    "# dataset_path='./Dataset/Dataset_recurrence'\n",
    "dataset_path='./Dataset/Dataset_30_6'\n",
    "train_set=CustomDataset(dataset_path, section=\"test_6\", transform=train_transform) # v_transform\n",
    "train_loader = DataLoader(train_set, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "# Directorios para embeddings de cada modelo\n",
    "embedding_dir_model1 = \"Dataset/contrastive_voxel_wise/embeddings_model1\"\n",
    "embedding_dir_model2 = \"Dataset/contrastive_voxel_wise/embeddings_model2\"\n",
    "label_output_dir = \"Dataset/contrastive_voxel_wise/labels\"\n",
    "\n",
    "# Crear carpetas si no existen\n",
    "os.makedirs(embedding_dir_model1, exist_ok=True)\n",
    "os.makedirs(embedding_dir_model2, exist_ok=True)\n",
    "os.makedirs(label_output_dir, exist_ok=True)\n",
    "\n",
    "# Variables para las características de los decoders de ambos modelos\n",
    "decoder_features_model1 = None\n",
    "decoder_features_model2 = None\n",
    "\n",
    "# Funciones hook para cada modelo\n",
    "def decoder_hook_fn_model1(module, input, output):\n",
    "    global decoder_features_model1\n",
    "    decoder_features_model1 = output\n",
    "\n",
    "def decoder_hook_fn_model2(module, input, output):\n",
    "    global decoder_features_model2\n",
    "    decoder_features_model2 = output\n",
    "\n",
    "# Registrar los hooks en los decoders de ambos modelos\n",
    "hook_handle_decoder1 = model1.decoder1.conv_block.register_forward_hook(decoder_hook_fn_model1)\n",
    "hook_handle_decoder2 = model2.decoder1.conv_block.register_forward_hook(decoder_hook_fn_model2)\n",
    "\n",
    "# Extraer y guardar\n",
    "with torch.no_grad():\n",
    "    for idx, batch_data in enumerate(train_loader):\n",
    "        image, label = batch_data[\"image\"], batch_data[\"label\"]\n",
    "        print(\"Image\", image.shape)  # [1, 11, 128, 128, 128]\n",
    "        print(\"label before squeeze\", label.shape)  # [1, 2, 128, 128, 128]\n",
    "        \n",
    "        image = image.to(device)\n",
    "        label = label.squeeze(0)  # [2, 128, 128, 128]\n",
    "        \n",
    "        # Convertir one-hot a etiquetas únicas\n",
    "        label_sum = label.sum(dim=0)  # [128, 128, 128], suma de canales\n",
    "        label_class = torch.zeros_like(label_sum, dtype=torch.long)  # [128, 128, 128]\n",
    "        \n",
    "        # Asignar clases:\n",
    "        # - Fondo (0, 0) -> 0\n",
    "        # - Vasogénico (1, 0) -> 1\n",
    "        # - Infiltrado (0, 1) -> 2\n",
    "        label_class[label[1] == 1] = 2  # Infiltrado\n",
    "        label_class[(label[0] == 1) & (label[1] == 0)] = 1  # Vasogénico\n",
    "        # Donde label_sum == 0, ya es fondo (0)\n",
    "        \n",
    "        label = label_class.cpu().numpy()  # [128, 128, 128]\n",
    "        print(\"label\", label.shape)\n",
    "        \n",
    "        # Obtener embeddings de ambos modelos\n",
    "        _ = model1(image)  # Forward para model1\n",
    "        print(\"decoder_features_model1:\", decoder_features_model1.shape)  # [1, 48, 128, 128, 128]\n",
    "        \n",
    "        _ = model2(image)  # Forward para model2\n",
    "        print(\"decoder_features_model2:\", decoder_features_model2.shape)  # [1, 48, 128, 128, 128]\n",
    "        \n",
    "        # Guardar embeddings y etiquetas\n",
    "        np.save(f\"{embedding_dir_model1}/case_{idx}.npy\", decoder_features_model1.cpu().numpy())\n",
    "        np.save(f\"{embedding_dir_model2}/case_{idx}.npy\", decoder_features_model2.cpu().numpy())\n",
    "        np.save(f\"{label_output_dir}/case_{idx}.npy\", label)\n",
    "        \n",
    "        print(f\"Guardado embeddings y etiquetas para caso {idx}\")\n",
    "\n",
    "# Remover los hooks\n",
    "hook_handle_decoder1.remove()\n",
    "hook_handle_decoder2.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nibabel as nib\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dispositivo\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset (ya lo tienes)\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, embedding_dir, label_dir):\n",
    "        self.embedding_dir = embedding_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.case_files = [f for f in os.listdir(embedding_dir) if f.endswith(\".npy\")]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.case_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        embedding_path = os.path.join(self.embedding_dir, f\"case_{idx}.npy\")\n",
    "        label_path = os.path.join(self.label_dir, f\"case_{idx}.npy\")\n",
    "        \n",
    "        embeddings = np.load(embedding_path)  # [1, 48, 128, 128, 128]\n",
    "        labels = np.load(label_path)  # [128, 128, 128]\n",
    "        \n",
    "        embeddings = torch.tensor(embeddings, dtype=torch.float32).squeeze(0)  # [48, 128, 128, 128]\n",
    "        labels = torch.tensor(labels, dtype=torch.long)  # [128, 128, 128]\n",
    "        \n",
    "        return embeddings, labels\n",
    "\n",
    "# # Modelo de proyección\n",
    "# class ProjectionHead(nn.Module):\n",
    "#     def __init__(self, input_dim=48, hidden_dim=128, output_dim=128):\n",
    "#         super(ProjectionHead, self).__init__()\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.Linear(input_dim, hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim, output_dim)\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         return self.net(x)\n",
    "\n",
    "# Modelo de proyección (MLP más profundo)\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, input_dim=48, hidden_dim1=256, hidden_dim2=128, output_dim=128, dropout_p=0.3):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_dim2, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# # Clasificador supervisado\n",
    "# class Classifier(nn.Module):\n",
    "#     def __init__(self, input_dim=128, num_classes=3):\n",
    "#         super(Classifier, self).__init__()\n",
    "#         self.fc = nn.Linear(input_dim, num_classes)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         return self.fc(x)\n",
    "\n",
    "# Clasificador supervisado (MLP)\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim=128, hidden_dim1=256, hidden_dim2=128, num_classes=3, dropout_p=0.3):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_dim2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Configuración para ambos modelos\n",
    "embedding_dir_model1 = \"Dataset/contrastive_voxel_wise/embeddings_model1\"\n",
    "embedding_dir_model2 = \"Dataset/contrastive_voxel_wise/embeddings_model2\"\n",
    "label_dir = \"Dataset/contrastive_voxel_wise/labels\"\n",
    "batch_size = 1\n",
    "\n",
    "# Cargar datasets y DataLoaders para ambos modelos\n",
    "dataset_model1 = EmbeddingDataset(embedding_dir_model1, label_dir)\n",
    "dataset_model2 = EmbeddingDataset(embedding_dir_model2, label_dir)\n",
    "loader_model1 = DataLoader(dataset_model1, batch_size=batch_size, shuffle=False)\n",
    "loader_model2 = DataLoader(dataset_model2, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Cargar modelos contrastivos preentrenados\n",
    "projection_head1 = ProjectionHead(input_dim=48).to(device)\n",
    "projection_head1.load_state_dict(torch.load(\"trained_models/checkpoints_contrastive/contrastive_projection_head_final_new_pipe1_v01_m1.pth\", map_location=device))\n",
    "projection_head1.eval()\n",
    "\n",
    "projection_head2 = ProjectionHead(input_dim=48).to(device)\n",
    "projection_head2.load_state_dict(torch.load(\"trained_models/checkpoints_contrastive/contrastive_projection_head_final_new_pipe2_m1_1dhzmigz.pth\", map_location=device))\n",
    "projection_head2.eval()\n",
    "\n",
    "# Cargar clasificadores preentrenados\n",
    "classifier1 = Classifier(input_dim=128, num_classes=3).to(device)\n",
    "classifier1.load_state_dict(torch.load(\"trained_models/checkpoints/supervised_classifier_final_pipe1_v01_m1.pth\", map_location=device))\n",
    "classifier1.eval()\n",
    "\n",
    "classifier2 = Classifier(input_dim=128, num_classes=3).to(device)\n",
    "classifier2.load_state_dict(torch.load(\"trained_models/checkpoints/supervised_classifier_final_pipe2_m1_1dhzmigz.pth\", map_location=device))\n",
    "classifier2.eval()\n",
    "\n",
    "# Función para generar mapas de probabilidad\n",
    "def generate_probability_maps(embeddings, projection_head, classifier, device):\n",
    "    with torch.no_grad():\n",
    "        embeddings = embeddings.to(device).squeeze(0).permute(1, 2, 3, 0)\n",
    "        embeddings_flat = embeddings.reshape(-1, 48)\n",
    "        \n",
    "        z = projection_head(embeddings_flat)\n",
    "        z = F.normalize(z, dim=1)\n",
    "        \n",
    "        logits = classifier(z)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        \n",
    "        probs = probs.view(128, 128, 128, 3).permute(3, 0, 1, 2)\n",
    "        return probs\n",
    "\n",
    "# Funciones para calcular métricas\n",
    "def calculate_metrics(pred, true,prob_maps=None, num_classes=3):\n",
    "    dice_scores = []\n",
    "    sensitivity_scores = []\n",
    "    precision_scores = []\n",
    "    auc_scores = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    # Calcular Accuracy global\n",
    "    accuracy = accuracy_score(true.flatten(), pred.flatten())\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        pred_cls = (pred == cls).astype(np.uint8)\n",
    "        true_cls = (true == cls).astype(np.uint8)\n",
    "        \n",
    "        tp = np.sum(pred_cls * true_cls)\n",
    "        fp = np.sum(pred_cls * (1 - true_cls))\n",
    "        fn = np.sum((1 - pred_cls) * true_cls)\n",
    "        \n",
    "        dice = 2 * tp / (2 * tp + fp + fn + 1e-6)\n",
    "        dice_scores.append(dice)\n",
    "        \n",
    "        sensitivity = tp / (tp + fn + 1e-6)\n",
    "        sensitivity_scores.append(sensitivity)\n",
    "        \n",
    "        precision = tp / (tp + fp + 1e-6)\n",
    "        precision_scores.append(precision)\n",
    "\n",
    "        # F1 Score\n",
    "        f1 = f1_score(true_cls.flatten(), pred_cls.flatten(), zero_division=0)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        # AUC-ROC (requiere mapas de probabilidad)\n",
    "        if prob_maps is not None:\n",
    "            try:\n",
    "                auc = roc_auc_score(true_cls.flatten(), prob_maps[cls].flatten())\n",
    "                auc_scores.append(auc)\n",
    "            except ValueError:\n",
    "                auc_scores.append(np.nan)  # Manejar casos donde AUC no se puede calcular\n",
    "        else:\n",
    "            auc_scores.append(np.nan)  # Si no se proporcionan mapas de probabilidad\n",
    "    \n",
    "    return dice_scores, sensitivity_scores, precision_scores, auc_scores, accuracy, f1_scores\n",
    "\n",
    "# Directorio de salida\n",
    "output_dir = \"trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test_cube8\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecutar calculo y guardar mapas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardada curva ROC en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/roc_curve_case_0.png\n",
      "Caso 0 - Dice: [0.9970035703989371, 0.520193908666654, 0.6986080252122276], Sensitivity: [0.9941729438903477, 0.4651015729859273, 0.8572294069472858], Precision: [0.9998503617499696, 0.5900915958885441, 0.5895229395928157], AUC-ROC: [0.9997574644423223, 0.9910906713472637, 0.9931217498754038], Accuracy: 0.9818, F1 Score: [0.9970035703991849, 0.5201939086743189, 0.698608025218861]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/probability_maps_case_0.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/labels_case_0.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/segmentation_case_0.nii.gz\n",
      "Guardada curva ROC en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/roc_curve_case_1.png\n",
      "Caso 1 - Dice: [0.9961940335339866, 0.7573532532253574, 0.7093981720864045], Sensitivity: [0.9927684989178468, 0.8805763915435673, 0.8734924409345383], Precision: [0.9996432895239844, 0.6643830515552622, 0.5972068985367515], AUC-ROC: [0.9991617904186789, 0.9987704127823962, 0.9967641799853623], Accuracy: 0.9909, F1 Score: [0.9961940335342289, 0.7573532532575018, 0.7093981720986376]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/probability_maps_case_1.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/labels_case_1.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/segmentation_case_1.nii.gz\n",
      "Guardada curva ROC en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/roc_curve_case_2.png\n",
      "Caso 2 - Dice: [0.9954207128587987, 0.7141549611409692, 0.8115478311414778], Sensitivity: [0.9918475049000461, 0.9249993951340334, 0.7700496806169659], Precision: [0.9990197594230913, 0.5815877941468294, 0.8577734344881823], AUC-ROC: [0.9994691909437659, 0.9964043270508957, 0.9960599556149978], Accuracy: 0.9798, F1 Score: [0.9954207128590542, 0.714154961147639, 0.8115478311456931]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/probability_maps_case_2.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/labels_case_2.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/segmentation_case_2.nii.gz\n",
      "Guardada curva ROC en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/roc_curve_case_3.png\n",
      "Caso 3 - Dice: [0.9946954256219234, 0.6495593571237637, 0.3871328671230227], Sensitivity: [0.9896221671657137, 0.5427375107503988, 0.9631785396731395], Precision: [0.9998209678212706, 0.8087349395850397, 0.2422506524014305], AUC-ROC: [0.9983193412576696, 0.9983511843142627, 0.9961697289113187], Accuracy: 0.9880, F1 Score: [0.9946954256221634, 0.649559357179886, 0.38713286713286715]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/probability_maps_case_3.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/labels_case_3.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/segmentation_case_3.nii.gz\n",
      "Guardada curva ROC en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/roc_curve_case_4.png\n",
      "Caso 4 - Dice: [0.9965279044150087, 0.7510474573280651, 0.6372424828180269], Sensitivity: [0.9930836863287986, 0.8135382059318024, 0.8549704811941695], Precision: [0.9999960961570087, 0.6974721529577604, 0.5078999574710252], AUC-ROC: [0.9996140716399262, 0.9982643383338923, 0.9963530494438688], Accuracy: 0.9905, F1 Score: [0.996527904415251, 0.7510474573486321, 0.6372424828321889]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/probability_maps_case_4.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/labels_case_4.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/segmentation_case_4.nii.gz\n",
      "Guardada curva ROC en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/roc_curve_case_5.png\n",
      "Caso 5 - Dice: [0.9969356400590003, 0.7873596864002423, 0.6544481470151394], Sensitivity: [0.9939036829084408, 0.7655546205819543, 0.8678748381525744], Precision: [0.9999861521116298, 0.8104433032694811, 0.5252735377407733], AUC-ROC: [0.9998924458565644, 0.9978815404296216, 0.996220551229559], Accuracy: 0.9882, F1 Score: [0.996935640059246, 0.7873596864102629, 0.6544481470261537]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/probability_maps_case_5.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/labels_case_5.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/segmentation_case_5.nii.gz\n",
      "Guardada curva ROC promedio en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/roc_curve_average.png\n",
      "\n",
      "Clase 0 (Fondo):\n",
      "  Dice: 0.9961 ± 0.0008\n",
      "  Sensibilidad: 0.9926 ± 0.0015\n",
      "  Precisión: 0.9997 ± 0.0003\n",
      "  AUC-ROC: 0.9994 ± 0.0005\n",
      "  F1 Score: 0.9961 ± 0.0008\n",
      "\n",
      "Clase 1 (Vasogénico):\n",
      "  Dice: 0.6966 ± 0.0899\n",
      "  Sensibilidad: 0.7321 ± 0.1704\n",
      "  Precisión: 0.6921 ± 0.0922\n",
      "  AUC-ROC: 0.9968 ± 0.0027\n",
      "  F1 Score: 0.6966 ± 0.0899\n",
      "\n",
      "Clase 2 (Infiltrado):\n",
      "  Dice: 0.6497 ± 0.1299\n",
      "  Sensibilidad: 0.8645 ± 0.0561\n",
      "  Precisión: 0.5533 ± 0.1805\n",
      "  AUC-ROC: 0.9958 ± 0.0012\n",
      "  F1 Score: 0.6497 ± 0.1299\n",
      "\n",
      "Accuracy Global: 0.9865 ± 0.0042\n"
     ]
    }
   ],
   "source": [
    "# Listas para métricas\n",
    "all_dice = {0: [], 1: [], 2: []}\n",
    "all_sensitivity = {0: [], 1: [], 2: []}\n",
    "all_precision = {0: [], 1: [], 2: []}\n",
    "all_auc = {0: [], 1: [], 2: []}  # Lista para AUC-ROC\n",
    "all_accuracy = []  # Lista para Accuracy global\n",
    "all_f1 = {0: [], 1: [], 2: []}  # Lista para F1 Score\n",
    "# Listas para almacenar FPR y TPR de todos los casos\n",
    "all_fpr = {0: [], 1: [], 2: []}\n",
    "all_tpr = {0: [], 1: [], 2: []}\n",
    "\n",
    "\n",
    "# Procesar y combinar\n",
    "for idx, ((embeddings1, labels1), (embeddings2, labels2)) in enumerate(zip(loader_model1, loader_model2)):\n",
    "    # Generar mapas de probabilidad para ambos modelos\n",
    "    prob_maps1 = generate_probability_maps(embeddings1, projection_head1, classifier1, device)  # [3, 128, 128, 128]\n",
    "    prob_maps2 = generate_probability_maps(embeddings2, projection_head2, classifier2, device)  # [3, 128, 128, 128]\n",
    "    \n",
    "    # Combinar mapas:\n",
    "    # - Clase 0: del modelo 1\n",
    "    # - Clase 1: máximo entre ambos modelos\n",
    "    # - Clase 2: del modelo 2\n",
    "    combined_prob_maps = torch.zeros_like(prob_maps1)  # [3, 128, 128, 128]\n",
    "    combined_prob_maps[0] = prob_maps1[0]  # Clase 0 del modelo 1\n",
    "    combined_prob_maps[1] = torch.max(prob_maps1[1], prob_maps2[1])  # Clase 1 máximo entre ambos\n",
    "    combined_prob_maps[2] = prob_maps2[2]  # Clase 2 del modelo 2\n",
    "    \n",
    "    # Normalizar probabilidades para que sumen 1 en cada vóxel\n",
    "    combined_prob_maps = combined_prob_maps / combined_prob_maps.sum(dim=0, keepdim=True)\n",
    "    \n",
    "    # Convertir a numpy\n",
    "    prob_maps_np = combined_prob_maps.cpu().numpy()  # [3, 128, 128, 128]\n",
    "    prob_maps_np_nifti = np.transpose(prob_maps_np, (1, 2, 3, 0))  # [128, 128, 128, 3]\n",
    "    \n",
    "    # Generar segmentación semántica\n",
    "    segmentation = np.argmax(prob_maps_np, axis=0)  # [128, 128, 128]\n",
    "    # segmentation_np = segmentation.astype(np.uint8)\n",
    "\n",
    "    # Aplicar umbral: asignar clase 1 si la probabilidad es > 0.4\n",
    "    # class_1_mask = prob_maps_np[1] > 0.3  # Máscara booleana para clase 1\n",
    "    # segmentation[class_1_mask] = 1  # Asignar clase 1 a los vóxeles que cumplen el criterio\n",
    "    segmentation_np = segmentation.astype(np.uint8)\n",
    "    \n",
    "    # Etiquetas (usamos las del modelo 2, asumiendo que son iguales)\n",
    "    labels = labels2.squeeze(0)  # [128, 128, 128]\n",
    "    labels_np = labels.cpu().numpy().astype(np.uint8)\n",
    "    \n",
    "    # Calcular métricas\n",
    "    dice, sensitivity, precision, auc, accuracy, f1 = calculate_metrics(segmentation_np, labels_np, prob_maps=prob_maps_np)\n",
    "    for cls in range(3):\n",
    "        all_dice[cls].append(dice[cls])\n",
    "        all_sensitivity[cls].append(sensitivity[cls])\n",
    "        all_precision[cls].append(precision[cls])\n",
    "        all_auc[cls].append(auc[cls])\n",
    "        all_f1[cls].append(f1[cls])\n",
    "    all_accuracy.append(accuracy)\n",
    "\n",
    "     # Graficar curvas ROC\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    class_names = [\"Fondo\", \"Vasogénico\", \"Infiltrado\"]\n",
    "    colors = ['blue', 'green', 'red']\n",
    "    \n",
    "    for cls in range(3):\n",
    "        # Etiquetas binarias para la clase actual\n",
    "        true_cls = (labels_np == cls).astype(np.uint8).flatten()\n",
    "        prob_cls = prob_maps_np[cls].flatten()\n",
    "        \n",
    "        # Calcular puntos de la curva ROC\n",
    "        fpr, tpr, _ = roc_curve(true_cls, prob_cls)\n",
    "        auc_value = auc[cls]  # Usar el AUC calculado previamente\n",
    "        all_fpr[cls].append(fpr)\n",
    "        all_tpr[cls].append(tpr)\n",
    "        \n",
    "        # Graficar\n",
    "        plt.plot(fpr, tpr, color=colors[cls], label=f'{class_names[cls]} (AUC = {auc_value:.4f})')\n",
    "    \n",
    "    # Configurar el gráfico\n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # Línea diagonal (clasificador aleatorio)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Tasa de Falsos Positivos (FPR)')\n",
    "    plt.ylabel('Tasa de Verdaderos Positivos (TPR)')\n",
    "    plt.title(f'Curva ROC - Caso {idx}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    # Guardar el gráfico\n",
    "    roc_output_path = os.path.join(output_dir, f\"roc_curve_case_{idx}.png\")\n",
    "    plt.savefig(roc_output_path)\n",
    "    plt.close()\n",
    "    print(f\"Guardada curva ROC en {roc_output_path}\")\n",
    "\n",
    "    print(f\"Caso {idx} - Dice: {dice}, Sensitivity: {sensitivity}, Precision: {precision}, \"\n",
    "          f\"AUC-ROC: {auc}, Accuracy: {accuracy:.4f}, F1 Score: {f1}\")\n",
    "    \n",
    "    # Crear imágenes NIfTI\n",
    "    affine = np.eye(4)\n",
    "    \n",
    "    # Guardar mapas de probabilidad combinados\n",
    "    nifti_prob_img = nib.Nifti1Image(prob_maps_np_nifti, affine)\n",
    "    prob_output_path = os.path.join(output_dir, f\"probability_maps_case_{idx}.nii.gz\")\n",
    "    nib.save(nifti_prob_img, prob_output_path)\n",
    "    print(f\"Guardado mapa de probabilidad en {prob_output_path}\")\n",
    "    \n",
    "    # Guardar etiquetas\n",
    "    nifti_label_img = nib.Nifti1Image(labels_np, affine)\n",
    "    label_output_path = os.path.join(output_dir, f\"labels_case_{idx}.nii.gz\")\n",
    "    nib.save(nifti_label_img, label_output_path)\n",
    "    print(f\"Guardadas etiquetas en {label_output_path}\")\n",
    "    \n",
    "    # Guardar segmentación semántica\n",
    "    nifti_seg_img = nib.Nifti1Image(segmentation_np, affine)\n",
    "    seg_output_path = os.path.join(output_dir, f\"segmentation_case_{idx}.nii.gz\")\n",
    "    nib.save(nifti_seg_img, seg_output_path)\n",
    "    print(f\"Guardada segmentación en {seg_output_path}\")\n",
    "\n",
    "# Curva ROC Promedio\n",
    "plt.figure(figsize=(8, 6))\n",
    "for cls in range(3):\n",
    "    # Interpolar FPR y TPR a una base común\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    tprs = []\n",
    "    for fpr, tpr in zip(all_fpr[cls], all_tpr[cls]):\n",
    "        tpr_interp = np.interp(mean_fpr, fpr, tpr)\n",
    "        tpr_interp[0] = 0.0  # Asegurar que comienza en 0\n",
    "        tprs.append(tpr_interp)\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0  # Asegurar que termina en 1\n",
    "    mean_auc = np.nanmean(all_auc[cls])\n",
    "    \n",
    "    plt.plot(mean_fpr, mean_tpr, color=colors[cls], label=f'{class_names[cls]} (AUC = {mean_auc:.4f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Tasa de Falsos Positivos (FPR)')\n",
    "plt.ylabel('Tasa de Verdaderos Positivos (TPR)')\n",
    "plt.title('Curva ROC Promedio')\n",
    "plt.legend(loc=\"lower right\")\n",
    "roc_avg_path = os.path.join(output_dir, \"roc_curve_average.png\")\n",
    "plt.savefig(roc_avg_path)\n",
    "plt.close()\n",
    "print(f\"Guardada curva ROC promedio en {roc_avg_path}\")\n",
    "\n",
    "# # Calcular promedios y desviaciones estándar\n",
    "# class_names = [\"Fondo\", \"Vasogénico\", \"Infiltrado\"]\n",
    "# for cls in range(3):\n",
    "#     dice_mean = np.mean(all_dice[cls])\n",
    "#     dice_std = np.std(all_dice[cls])\n",
    "#     sens_mean = np.mean(all_sensitivity[cls])\n",
    "#     sens_std = np.std(all_sensitivity[cls])\n",
    "#     prec_mean = np.mean(all_precision[cls])\n",
    "#     prec_std = np.std(all_precision[cls])\n",
    "    \n",
    "#     print(f\"\\nClase {cls} ({class_names[cls]}):\")\n",
    "#     print(f\"  Dice: {dice_mean:.4f} ± {dice_std:.4f}\")\n",
    "#     print(f\"  Sensibilidad: {sens_mean:.4f} ± {sens_std:.4f}\")\n",
    "#     print(f\"  Precisión: {prec_mean:.4f} ± {prec_std:.4f}\")\n",
    "\n",
    "# Calcular promedios y desviaciones estándar\n",
    "class_names = [\"Fondo\", \"Vasogénico\", \"Infiltrado\"]\n",
    "for cls in range(3):\n",
    "    dice_mean = np.nanmean(all_dice[cls])\n",
    "    dice_std = np.nanstd(all_dice[cls])\n",
    "    sens_mean = np.nanmean(all_sensitivity[cls])\n",
    "    sens_std = np.nanstd(all_sensitivity[cls])\n",
    "    prec_mean = np.nanmean(all_precision[cls])\n",
    "    prec_std = np.nanstd(all_precision[cls])\n",
    "    auc_mean = np.nanmean(all_auc[cls])\n",
    "    auc_std = np.nanstd(all_auc[cls])\n",
    "    f1_mean = np.nanmean(all_f1[cls])\n",
    "    f1_std = np.nanstd(all_f1[cls])\n",
    "    \n",
    "    print(f\"\\nClase {cls} ({class_names[cls]}):\")\n",
    "    print(f\"  Dice: {dice_mean:.4f} ± {dice_std:.4f}\")\n",
    "    print(f\"  Sensibilidad: {sens_mean:.4f} ± {sens_std:.4f}\")\n",
    "    print(f\"  Precisión: {prec_mean:.4f} ± {prec_std:.4f}\")\n",
    "    print(f\"  AUC-ROC: {auc_mean:.4f} ± {auc_std:.4f}\")\n",
    "    print(f\"  F1 Score: {f1_mean:.4f} ± {f1_std:.4f}\")\n",
    "\n",
    "# Accuracy global\n",
    "accuracy_mean = np.nanmean(all_accuracy)\n",
    "accuracy_std = np.nanstd(all_accuracy)\n",
    "print(f\"\\nAccuracy Global: {accuracy_mean:.4f} ± {accuracy_std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metricas basdas en regions (supervoxeles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caso 0 - Voxel-wise:\n",
      "  Dice: [0.9970035703989371, 0.520193908666654, 0.6986080252122276], Sensitivity: [0.9941729438903477, 0.4651015729859273, 0.8572294069472858], Precision: [0.9998503617499696, 0.5900915958885441, 0.5895229395928157], AUC-ROC: [0.9997442857524647, 0.9908721550503753, 0.9926824584823732], Accuracy: 0.9818, F1 Score: [0.9970035703991849, 0.5201939086743189, 0.698608025218861]\n",
      "Caso 0 - Cube-wise (tamaño 8):\n",
      "  Dice: [0.9978418178243186, 0.539682535399345, 0.6560846526133087], Sensitivity: [0.9956929310879927, 0.4415584358239164, 0.8611110991512347], Precision: [0.9999999997455471, 0.6938775368596422, 0.529914525385346], AUC-ROC: [0.9998240104199434, 0.9316412624449448, 0.9923341893085929], Accuracy: 0.9829, F1 Score: [0.9978418179509966, 0.5396825396825397, 0.6560846560846562]\n",
      "  Centro de masa Infiltrado (Pred): [6.81632653 8.55102041 7.75510204]\n",
      "  Centro de masa Infiltrado (True): [7.50649351 7.45454545 6.74025974]\n",
      "Caso 1 - Voxel-wise:\n",
      "  Dice: [0.9961940335339866, 0.7573532532253574, 0.7093981720864045], Sensitivity: [0.9927684989178468, 0.8805763915435673, 0.8734924409345383], Precision: [0.9996432895239844, 0.6643830515552622, 0.5972068985367515], AUC-ROC: [0.9982288768831301, 0.9987986678355506, 0.9947559103374306], Accuracy: 0.9909, F1 Score: [0.9961940335342289, 0.7573532532575018, 0.7093981720986376]\n",
      "Caso 1 - Cube-wise (tamaño 8):\n",
      "  Dice: [0.9973955102322466, 0.8648648414901394, 0.7391304267485823], Sensitivity: [0.9952970294566097, 0.8888888395061756, 0.8947368185595574], Precision: [0.9995028583148141, 0.8421052188365674, 0.6296296179698219], AUC-ROC: [0.9992441654879773, 0.9997752166094491, 0.9984274078493425], Accuracy: 0.9939, F1 Score: [0.9973955103559469, 0.8648648648648649, 0.7391304347826088]\n",
      "  Centro de masa Infiltrado (Pred): [7.78947368 7.78947368 7.26315789]\n",
      "  Centro de masa Infiltrado (True): [8.05555556 8.         7.27777778]\n",
      "Caso 2 - Voxel-wise:\n",
      "  Dice: [0.9954207128587987, 0.7141549611409692, 0.8115478311414778], Sensitivity: [0.9918475049000461, 0.9249993951340334, 0.7700496806169659], Precision: [0.9990197594230913, 0.5815877941468294, 0.8577734344881823], AUC-ROC: [0.9992538722066036, 0.9960538811731321, 0.9950507396812097], Accuracy: 0.9798, F1 Score: [0.9954207128590542, 0.714154961147639, 0.8115478311456931]\n",
      "Caso 2 - Cube-wise (tamaño 8):\n",
      "  Dice: [0.9972502290169896, 0.7373737336496277, 0.8291316503385668], Sensitivity: [0.9955555552952796, 0.9605263031509698, 0.7589743550821828], Precision: [0.998950681794609, 0.5983606508331094, 0.913580241274196], AUC-ROC: [0.9995678074427804, 0.9972096753076722, 0.9817811343573969], Accuracy: 0.9836, F1 Score: [0.997250229147571, 0.7373737373737373, 0.8291316526610644]\n",
      "  Centro de masa Infiltrado (Pred): [9.64754098 5.71311475 7.29508197]\n",
      "  Centro de masa Infiltrado (True): [9.71052632 6.19736842 6.77631579]\n",
      "Caso 3 - Voxel-wise:\n",
      "  Dice: [0.9946954256219234, 0.6495593571237637, 0.3871328671230227], Sensitivity: [0.9896221671657137, 0.5427375107503988, 0.9631785396731395], Precision: [0.9998209678212706, 0.8087349395850397, 0.2422506524014305], AUC-ROC: [0.9982360359904578, 0.9980275841103045, 0.9913611417658387], Accuracy: 0.9880, F1 Score: [0.9946954256221634, 0.649559357179886, 0.38713286713286715]\n",
      "Caso 3 - Cube-wise (tamaño 8):\n",
      "  Dice: [0.9967988179023887, 0.6999999650000017, 0.3999999920000002], Sensitivity: [0.9936180655391218, 0.5833332847222263, 0.99999990000001], Precision: [0.9999999997529645, 0.8749998906250137, 0.24999999375000018], AUC-ROC: [0.9991911009952247, 0.9556909076069212, 0.9987273617229565], Accuracy: 0.9924, F1 Score: [0.9967988180251169, 0.7000000000000001, 0.4]\n",
      "  Centro de masa Infiltrado (Pred): [7.    9.    9.125]\n",
      "  Centro de masa Infiltrado (True): [6.83333333 8.83333333 8.58333333]\n",
      "Caso 4 - Voxel-wise:\n",
      "  Dice: [0.9965279044150087, 0.7510474573280651, 0.6372424828180269], Sensitivity: [0.9930836863287986, 0.8135382059318024, 0.8549704811941695], Precision: [0.9999960961570087, 0.6974721529577604, 0.5078999574710252], AUC-ROC: [0.9996016155347753, 0.9980592234123471, 0.9952250280934484], Accuracy: 0.9905, F1 Score: [0.996527904415251, 0.7510474573486321, 0.6372424828321889]\n",
      "Caso 4 - Cube-wise (tamaño 8):\n",
      "  Dice: [0.9980153806750166, 0.7428571322448981, 0.7333333211111114], Sensitivity: [0.9962852895006723, 0.8124999746093758, 0.8461538136094686], Precision: [0.9997514908052307, 0.6842105083102499, 0.6470588044982705], AUC-ROC: [0.9997779713412239, 0.9509258120078741, 0.9987006237006237], Accuracy: 0.9939, F1 Score: [0.9980153807988091, 0.742857142857143, 0.7333333333333334]\n",
      "  Centro de masa Infiltrado (Pred): [7.23684211 7.57894737 7.        ]\n",
      "  Centro de masa Infiltrado (True): [7.40625 8.09375 7.     ]\n",
      "Caso 5 - Voxel-wise:\n",
      "  Dice: [0.9969356400590003, 0.7873596864002423, 0.6544481470151394], Sensitivity: [0.9939036829084408, 0.7655546205819543, 0.8678748381525744], Precision: [0.9999861521116298, 0.8104433032694811, 0.5252735377407733], AUC-ROC: [0.9998858128010235, 0.997705949643649, 0.9948220587097338], Accuracy: 0.9882, F1 Score: [0.996935640059246, 0.7873596864102629, 0.6544481470261537]\n",
      "Caso 5 - Cube-wise (tamaño 8):\n",
      "  Dice: [0.9973581581334308, 0.82993196714332, 0.7083333259548612], Sensitivity: [0.995479658213089, 0.7922077819193795, 0.9189188940832732], Precision: [0.999243760776596, 0.871428558979592, 0.5762711766733699], AUC-ROC: [0.9997874205856178, 0.991478787447934, 0.9960381667698741], Accuracy: 0.9910, F1 Score: [0.9973581582589005, 0.8299319727891157, 0.7083333333333333]\n",
      "  Centro de masa Infiltrado (Pred): [8.01428571 7.51428571 7.51428571]\n",
      "  Centro de masa Infiltrado (True): [7.80519481 7.49350649 7.15584416]\n",
      "\n",
      "Resultados Voxel-wise:\n",
      "\n",
      "Clase 0 (Fondo):\n",
      "  Dice: 0.9961 ± 0.0008\n",
      "  Sensibilidad: 0.9926 ± 0.0015\n",
      "  Precisión: 0.9997 ± 0.0003\n",
      "  AUC-ROC: 0.9992 ± 0.0007\n",
      "  F1 Score: 0.9961 ± 0.0008\n",
      "\n",
      "Clase 1 (Vasogénico):\n",
      "  Dice: 0.6966 ± 0.0899\n",
      "  Sensibilidad: 0.7321 ± 0.1704\n",
      "  Precisión: 0.6921 ± 0.0922\n",
      "  AUC-ROC: 0.9966 ± 0.0027\n",
      "  F1 Score: 0.6966 ± 0.0899\n",
      "\n",
      "Clase 2 (Infiltrado):\n",
      "  Dice: 0.6497 ± 0.1299\n",
      "  Sensibilidad: 0.8645 ± 0.0561\n",
      "  Precisión: 0.5533 ± 0.1805\n",
      "  AUC-ROC: 0.9940 ± 0.0014\n",
      "  F1 Score: 0.6497 ± 0.1299\n",
      "\n",
      "Accuracy Global: 0.9865 ± 0.0042\n",
      "\n",
      "Resultados Cube-wise:\n",
      "\n",
      "Clase 0 (Fondo):\n",
      "  Dice: 0.9974 ± 0.0004\n",
      "  Sensibilidad: 0.9953 ± 0.0008\n",
      "  Precisión: 0.9996 ± 0.0004\n",
      "  AUC-ROC: 0.9996 ± 0.0003\n",
      "  F1 Score: 0.9974 ± 0.0004\n",
      "\n",
      "Clase 1 (Vasogénico):\n",
      "  Dice: 0.7358 ± 0.1043\n",
      "  Sensibilidad: 0.7465 ± 0.1790\n",
      "  Precisión: 0.7608 ± 0.1069\n",
      "  AUC-ROC: 0.9711 ± 0.0262\n",
      "  F1 Score: 0.7358 ± 0.1043\n",
      "\n",
      "Clase 2 (Infiltrado):\n",
      "  Dice: 0.6777 ± 0.1344\n",
      "  Sensibilidad: 0.8800 ± 0.0734\n",
      "  Precisión: 0.5911 ± 0.1953\n",
      "  AUC-ROC: 0.9943 ± 0.0060\n",
      "  F1 Score: 0.6777 ± 0.1344\n",
      "\n",
      "Accuracy Global Cube-wise: 0.9896 ± 0.0046\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, f1_score\n",
    "from scipy import stats\n",
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import torch\n",
    "\n",
    "# Función para calcular métricas (ya definida previamente)\n",
    "def calculate_metrics(pred, true, prob_maps=None, num_classes=3):\n",
    "    dice_scores = []\n",
    "    sensitivity_scores = []\n",
    "    precision_scores = []\n",
    "    auc_scores = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    accuracy = accuracy_score(true.flatten(), pred.flatten())\n",
    "    \n",
    "    for cls in range(num_classes):\n",
    "        pred_cls = (pred == cls).astype(np.uint8)\n",
    "        true_cls = (true == cls).astype(np.uint8)\n",
    "        \n",
    "        tp = np.sum(pred_cls * true_cls)\n",
    "        fp = np.sum(pred_cls * (1 - true_cls))\n",
    "        fn = np.sum((1 - pred_cls) * true_cls)\n",
    "        \n",
    "        dice = 2 * tp / (2 * tp + fp + fn + 1e-6)\n",
    "        dice_scores.append(dice)\n",
    "        \n",
    "        sensitivity = tp / (tp + fn + 1e-6)\n",
    "        sensitivity_scores.append(sensitivity)\n",
    "        \n",
    "        precision = tp / (tp + fp + 1e-6)\n",
    "        precision_scores.append(precision)\n",
    "        \n",
    "        f1 = f1_score(true_cls.flatten(), pred_cls.flatten(), zero_division=0)\n",
    "        f1_scores.append(f1)\n",
    "        \n",
    "        if prob_maps is not None:\n",
    "            try:\n",
    "                auc = roc_auc_score(true_cls.flatten(), prob_maps[cls].flatten())\n",
    "                auc_scores.append(auc)\n",
    "            except ValueError:\n",
    "                auc_scores.append(np.nan)\n",
    "        else:\n",
    "            auc_scores.append(np.nan)\n",
    "    \n",
    "    return dice_scores, sensitivity_scores, precision_scores, auc_scores, accuracy, f1_scores\n",
    "\n",
    "# Función para dividir en cubos y obtener clases predominantes\n",
    "def get_cube_labels(volume, cube_size, num_classes=3):\n",
    "    dims = volume.shape\n",
    "    assert dims[0] % cube_size == 0, \"El tamaño del cubo debe dividir exactamente el tamaño del volumen\"\n",
    "    num_cubes = dims[0] // cube_size\n",
    "    \n",
    "    cube_labels = np.zeros((num_cubes, num_cubes, num_cubes), dtype=np.uint8)\n",
    "    cube_probs = np.zeros((num_classes, num_cubes, num_cubes, num_cubes))\n",
    "    \n",
    "    for i in range(num_cubes):\n",
    "        for j in range(num_cubes):\n",
    "            for k in range(num_cubes):\n",
    "                cube = volume[i*cube_size:(i+1)*cube_size, \n",
    "                             j*cube_size:(j+1)*cube_size, \n",
    "                             k*cube_size:(k+1)*cube_size]\n",
    "                # Clase predominante (modo)\n",
    "                mode_value = stats.mode(cube.flatten(), keepdims=True)[0][0]\n",
    "                cube_labels[i, j, k] = mode_value\n",
    "                # Proporción de cada clase como \"probabilidad\" suavizada\n",
    "                for cls in range(num_classes):\n",
    "                    cube_probs[cls, i, j, k] = np.mean(cube == cls)\n",
    "    \n",
    "    return cube_labels, cube_probs\n",
    "\n",
    "# Listas para métricas voxel-wise\n",
    "all_dice = {0: [], 1: [], 2: []}\n",
    "all_sensitivity = {0: [], 1: [], 2: []}\n",
    "all_precision = {0: [], 1: [], 2: []}\n",
    "all_auc = {0: [], 1: [], 2: []}\n",
    "all_accuracy = []\n",
    "all_f1 = {0: [], 1: [], 2: []}\n",
    "\n",
    "# Listas para métricas cube-wise\n",
    "all_dice_cube = {0: [], 1: [], 2: []}\n",
    "all_sensitivity_cube = {0: [], 1: [], 2: []}\n",
    "all_precision_cube = {0: [], 1: [], 2: []}\n",
    "all_auc_cube = {0: [], 1: [], 2: []}\n",
    "all_accuracy_cube = []\n",
    "all_f1_cube = {0: [], 1: [], 2: []}\n",
    "\n",
    "# Tamaño del cubo (ajusta según necesites)\n",
    "cube_size = 8  # 128 / 16 = 8 cubos por dimensión\n",
    "\n",
    "# Procesar y combinar\n",
    "for idx, ((embeddings1, labels1), (embeddings2, labels2)) in enumerate(zip(loader_model1, loader_model2)):\n",
    "    # Generar mapas de probabilidad para ambos modelos\n",
    "    prob_maps1 = generate_probability_maps(embeddings1, projection_head1, classifier1, device)  # [3, 128, 128, 128]\n",
    "    prob_maps2 = generate_probability_maps(embeddings2, projection_head2, classifier2, device)  # [3, 128, 128, 128]\n",
    "    \n",
    "    # Combinar mapas\n",
    "    combined_prob_maps = torch.zeros_like(prob_maps1)\n",
    "    combined_prob_maps[0] = prob_maps1[0]\n",
    "    combined_prob_maps[1] = torch.max(prob_maps1[1], prob_maps2[1])\n",
    "    combined_prob_maps[2] = prob_maps2[2]\n",
    "    \n",
    "    # Convertir a numpy\n",
    "    prob_maps_np = combined_prob_maps.cpu().numpy()  # [3, 128, 128, 128]\n",
    "    prob_maps_np_nifti = np.transpose(prob_maps_np, (1, 2, 3, 0))  # [128, 128, 128, 3]\n",
    "    \n",
    "    # Generar segmentación semántica\n",
    "    segmentation = np.argmax(prob_maps_np, axis=0)\n",
    "    # Colocar nuevo umbral para infiltracion\n",
    "    # class_1_mask = prob_maps_np[1] > 0.4\n",
    "    # segmentation[class_1_mask] = 1\n",
    "    segmentation_np = segmentation.astype(np.uint8)\n",
    "    \n",
    "    # Etiquetas\n",
    "    labels = labels2.squeeze(0)\n",
    "    labels_np = labels.cpu().numpy().astype(np.uint8)\n",
    "    \n",
    "    # Calcular métricas voxel-wise\n",
    "    dice, sensitivity, precision, auc, accuracy, f1 = calculate_metrics(segmentation_np, labels_np, prob_maps=prob_maps_np)\n",
    "    \n",
    "    # Almacenar métricas voxel-wise\n",
    "    for cls in range(3):\n",
    "        all_dice[cls].append(dice[cls])\n",
    "        all_sensitivity[cls].append(sensitivity[cls])\n",
    "        all_precision[cls].append(precision[cls])\n",
    "        all_auc[cls].append(auc[cls])\n",
    "        all_f1[cls].append(f1[cls])\n",
    "    all_accuracy.append(accuracy)\n",
    "    \n",
    "    # Evaluación basada en cubos\n",
    "    pred_cube_labels, pred_cube_probs = get_cube_labels(segmentation_np, cube_size)\n",
    "    true_cube_labels, true_cube_probs = get_cube_labels(labels_np, cube_size)\n",
    "    \n",
    "    # Calcular métricas cube-wise\n",
    "    dice_cube, sensitivity_cube, precision_cube, auc_cube, accuracy_cube, f1_cube = calculate_metrics(\n",
    "        pred_cube_labels, true_cube_labels, prob_maps=pred_cube_probs\n",
    "    )\n",
    "    \n",
    "    # Almacenar métricas cube-wise\n",
    "    for cls in range(3):\n",
    "        all_dice_cube[cls].append(dice_cube[cls])\n",
    "        all_sensitivity_cube[cls].append(sensitivity_cube[cls])\n",
    "        all_precision_cube[cls].append(precision_cube[cls])\n",
    "        all_auc_cube[cls].append(auc_cube[cls])\n",
    "        all_f1_cube[cls].append(f1_cube[cls])\n",
    "    all_accuracy_cube.append(accuracy_cube)\n",
    "    \n",
    "    # Mapa de coincidencias/discrepancias\n",
    "    match_map = (pred_cube_labels == true_cube_labels).astype(np.uint8)\n",
    "    mismatch_map = (pred_cube_labels != true_cube_labels).astype(np.uint8)\n",
    "    \n",
    "    # Guardar mapas de cubos y coincidencias\n",
    "    affine = np.eye(4) * cube_size  # Ajustar el affine para reflejar el tamaño del cubo\n",
    "    nib.save(nib.Nifti1Image(pred_cube_labels, affine), os.path.join(output_dir, f\"pred_cube_labels_case_{idx}.nii.gz\"))\n",
    "    nib.save(nib.Nifti1Image(true_cube_labels, affine), os.path.join(output_dir, f\"true_cube_labels_case_{idx}.nii.gz\"))\n",
    "    nib.save(nib.Nifti1Image(match_map, affine), os.path.join(output_dir, f\"match_map_case_{idx}.nii.gz\"))\n",
    "    nib.save(nib.Nifti1Image(mismatch_map, affine), os.path.join(output_dir, f\"mismatch_map_case_{idx}.nii.gz\"))\n",
    "    \n",
    "    # Análisis espacial (centro de masa de la clase Infiltrado)\n",
    "    infiltrado_pred = (pred_cube_labels == 1).astype(np.uint8)\n",
    "    infiltrado_true = (true_cube_labels == 1).astype(np.uint8)\n",
    "    if np.sum(infiltrado_pred) > 0:\n",
    "        pred_center = np.mean(np.where(infiltrado_pred), axis=1)\n",
    "    else:\n",
    "        pred_center = np.array([np.nan, np.nan, np.nan])\n",
    "    if np.sum(infiltrado_true) > 0:\n",
    "        true_center = np.mean(np.where(infiltrado_true), axis=1)\n",
    "    else:\n",
    "        true_center = np.array([np.nan, np.nan, np.nan])\n",
    "    \n",
    "    print(f\"Caso {idx} - Voxel-wise:\")\n",
    "    print(f\"  Dice: {dice}, Sensitivity: {sensitivity}, Precision: {precision}, AUC-ROC: {auc}, \"\n",
    "          f\"Accuracy: {accuracy:.4f}, F1 Score: {f1}\")\n",
    "    print(f\"Caso {idx} - Cube-wise (tamaño {cube_size}):\")\n",
    "    print(f\"  Dice: {dice_cube}, Sensitivity: {sensitivity_cube}, Precision: {precision_cube}, \"\n",
    "          f\"AUC-ROC: {auc_cube}, Accuracy: {accuracy_cube:.4f}, F1 Score: {f1_cube}\")\n",
    "    print(f\"  Centro de masa Infiltrado (Pred): {pred_center}\")\n",
    "    print(f\"  Centro de masa Infiltrado (True): {true_center}\")\n",
    "    \n",
    "    # Guardar mapas de probabilidad y segmentaciones voxel-wise\n",
    "    nib.save(nib.Nifti1Image(prob_maps_np_nifti, np.eye(4)), os.path.join(output_dir, f\"probability_maps_case_{idx}.nii.gz\"))\n",
    "    nib.save(nib.Nifti1Image(labels_np, np.eye(4)), os.path.join(output_dir, f\"labels_case_{idx}.nii.gz\"))\n",
    "    nib.save(nib.Nifti1Image(segmentation_np, np.eye(4)), os.path.join(output_dir, f\"segmentation_case_{idx}.nii.gz\"))\n",
    "\n",
    "# Calcular promedios y desviaciones estándar (voxel-wise)\n",
    "class_names = [\"Fondo\", \"Vasogénico\", \"Infiltrado\"]\n",
    "print(\"\\nResultados Voxel-wise:\")\n",
    "for cls in range(3):\n",
    "    dice_mean = np.nanmean(all_dice[cls])\n",
    "    dice_std = np.nanstd(all_dice[cls])\n",
    "    sens_mean = np.nanmean(all_sensitivity[cls])\n",
    "    sens_std = np.nanstd(all_sensitivity[cls])\n",
    "    prec_mean = np.nanmean(all_precision[cls])\n",
    "    prec_std = np.nanstd(all_precision[cls])\n",
    "    auc_mean = np.nanmean(all_auc[cls])\n",
    "    auc_std = np.nanstd(all_auc[cls])\n",
    "    f1_mean = np.nanmean(all_f1[cls])\n",
    "    f1_std = np.nanstd(all_f1[cls])\n",
    "    \n",
    "    print(f\"\\nClase {cls} ({class_names[cls]}):\")\n",
    "    print(f\"  Dice: {dice_mean:.4f} ± {dice_std:.4f}\")\n",
    "    print(f\"  Sensibilidad: {sens_mean:.4f} ± {sens_std:.4f}\")\n",
    "    print(f\"  Precisión: {prec_mean:.4f} ± {prec_std:.4f}\")\n",
    "    print(f\"  AUC-ROC: {auc_mean:.4f} ± {auc_std:.4f}\")\n",
    "    print(f\"  F1 Score: {f1_mean:.4f} ± {f1_std:.4f}\")\n",
    "\n",
    "accuracy_mean = np.nanmean(all_accuracy)\n",
    "accuracy_std = np.nanstd(all_accuracy)\n",
    "print(f\"\\nAccuracy Global: {accuracy_mean:.4f} ± {accuracy_std:.4f}\")\n",
    "\n",
    "# Calcular promedios y desviaciones estándar (cube-wise)\n",
    "print(\"\\nResultados Cube-wise:\")\n",
    "for cls in range(3):\n",
    "    dice_mean = np.nanmean(all_dice_cube[cls])\n",
    "    dice_std = np.nanstd(all_dice_cube[cls])\n",
    "    sens_mean = np.nanmean(all_sensitivity_cube[cls])\n",
    "    sens_std = np.nanstd(all_sensitivity_cube[cls])\n",
    "    prec_mean = np.nanmean(all_precision_cube[cls])\n",
    "    prec_std = np.nanstd(all_precision_cube[cls])\n",
    "    auc_mean = np.nanmean(all_auc_cube[cls])\n",
    "    auc_std = np.nanstd(all_auc_cube[cls])\n",
    "    f1_mean = np.nanmean(all_f1_cube[cls])\n",
    "    f1_std = np.nanstd(all_f1_cube[cls])\n",
    "    \n",
    "    print(f\"\\nClase {cls} ({class_names[cls]}):\")\n",
    "    print(f\"  Dice: {dice_mean:.4f} ± {dice_std:.4f}\")\n",
    "    print(f\"  Sensibilidad: {sens_mean:.4f} ± {sens_std:.4f}\")\n",
    "    print(f\"  Precisión: {prec_mean:.4f} ± {prec_std:.4f}\")\n",
    "    print(f\"  AUC-ROC: {auc_mean:.4f} ± {auc_std:.4f}\")\n",
    "    print(f\"  F1 Score: {f1_mean:.4f} ± {f1_std:.4f}\")\n",
    "\n",
    "accuracy_mean_cube = np.nanmean(all_accuracy_cube)\n",
    "accuracy_std_cube = np.nanstd(all_accuracy_cube)\n",
    "print(f\"\\nAccuracy Global Cube-wise: {accuracy_mean_cube:.4f} ± {accuracy_std_cube:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
