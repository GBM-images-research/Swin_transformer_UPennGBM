{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading functions\n",
    "import os\n",
    "import time\n",
    "from monai.data import DataLoader, decollate_batch\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "\n",
    "from src.get_data import CustomDataset, CustomDatasetSeg\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from types import SimpleNamespace\n",
    "import wandb\n",
    "import logging\n",
    "\n",
    "#####\n",
    "import json\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai import transforms\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    Activations,\n",
    "    MapTransform,\n",
    "    Transform,\n",
    ")\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.utils.enums import MetricReduction\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai import data\n",
    "\n",
    "# from monai.data import decollate_batch\n",
    "from functools import partial\n",
    "from src.custom_transforms import (ConvertToMultiChannelBasedOnN_Froi, \n",
    "                                   ConvertToMultiChannelBasedOnAnotatedInfiltration, \n",
    "                                   masked, ConvertToMultiChannelBasedOnBratsClassesdI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trasnformaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = (128, 128, 128) # (220, 220, 155) (128, 128, 64)\n",
    "source_k=\"label\"\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        # ConvertToMultiChannelBasedOnN_Froi(keys=\"label\"),\n",
    "        # masked(keys=\"image\"),\n",
    "        # ConvertToMultiChannelBasedOnAnotatedInfiltration(keys=\"label\"),\n",
    "        ConvertToMultiChannelBasedOnBratsClassesdI(keys=\"label\"),\n",
    "        transforms.CropForegroundd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            source_key=source_k,\n",
    "            k_divisible=[roi[0], roi[1], roi[2]],\n",
    "        ),\n",
    "        transforms.RandSpatialCropd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            roi_size=[roi[0], roi[1], roi[2]],\n",
    "            random_size=False,\n",
    "        ),\n",
    "        transforms.NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "        \n",
    "    ]\n",
    ")\n",
    "val_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        # ConvertToMultiChannelBasedOnN_Froi(keys=\"label\"),\n",
    "        # masked(keys=\"image\"),\n",
    "        # ConvertToMultiChannelBasedOnAnotatedInfiltration(keys=\"label\"),\n",
    "        ConvertToMultiChannelBasedOnBratsClassesdI(keys=\"label\"),\n",
    "        transforms.RandSpatialCropd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            roi_size=[-1, -1, -1], #[224, 224, 128],\n",
    "            random_size=False,\n",
    "        ),\n",
    "        transforms.NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Crear el modelo\n",
    "######################\n",
    "\n",
    "### Hyperparameter\n",
    "roi = (128, 128, 128)  # (128, 128, 128)\n",
    "\n",
    "# Create Swin transformer\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def define_model(model_path):\n",
    "    model = SwinUNETR(\n",
    "        img_size=roi,\n",
    "        in_channels=11,\n",
    "        out_channels=2,  # mdificar con edema\n",
    "        feature_size=48, #48\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        dropout_path_rate=0.0,\n",
    "        use_checkpoint=True,\n",
    "    )\n",
    "\n",
    "    # # Load the best model\n",
    "    # model_path = \"artifacts/o9kppyr5_best_model:v0/model.pt\"\n",
    "\n",
    "    # Load the model on CPU\n",
    "    loaded_model = torch.load(model_path, map_location=torch.device('cuda:0'))[\"state_dict\"]\n",
    "\n",
    "    # Load the state dictionary into the model\n",
    "    model.load_state_dict(loaded_model)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/training2/miniconda3/envs/monai_env/lib/python3.11/site-packages/monai/utils/deprecate_utils.py:221: FutureWarning: monai.networks.nets.swin_unetr SwinUNETR.__init__:img_size: Argument `img_size` has been deprecated since version 1.3. It will be removed in version 1.5. The img_size argument is not required anymore and checks on the input size are run during forward().\n",
      "  warn_deprecated(argname, msg, warning_category)\n",
      "/tmp/ipykernel_905890/2946448484.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model = torch.load(model_path, map_location=torch.device('cuda:0'))[\"state_dict\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SwinUNETR(\n",
       "  (swinViT): SwinTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv3d(11, 48, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (layers1): ModuleList(\n",
       "      (0): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=48, out_features=144, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=48, out_features=48, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (linear1): Linear(in_features=48, out_features=192, bias=True)\n",
       "              (linear2): Linear(in_features=192, out_features=48, bias=True)\n",
       "              (fn): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=384, out_features=96, bias=False)\n",
       "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layers2): ModuleList(\n",
       "      (0): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (linear1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (linear2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (fn): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=768, out_features=192, bias=False)\n",
       "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layers3): ModuleList(\n",
       "      (0): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (fn): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=1536, out_features=384, bias=False)\n",
       "          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layers4): ModuleList(\n",
       "      (0): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (linear1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (linear2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (fn): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder1): UnetrBasicBlock(\n",
       "    (layer): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(11, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(11, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder2): UnetrBasicBlock(\n",
       "    (layer): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder3): UnetrBasicBlock(\n",
       "    (layer): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder4): UnetrBasicBlock(\n",
       "    (layer): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder10): UnetrBasicBlock(\n",
       "    (layer): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(768, 768, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(768, 768, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder5): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(768, 384, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(768, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(768, 384, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder4): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(384, 192, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(384, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(384, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder3): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(192, 96, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(192, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(192, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder2): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(96, 48, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(96, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(96, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder1): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(48, 48, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(96, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(96, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (out): UnetOutBlock(\n",
       "    (conv): Convolution(\n",
       "      (conv): Conv3d(48, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo TC+Edema\n",
    "model1=define_model(\"artifacts/o9kppyr5_best_model:v0/model.pt\") \n",
    "model1.to(device)\n",
    "model1.eval()\n",
    "\n",
    "# Modelo Infitracion+Edema\n",
    "model2=define_model(\"artifacts/uixfayrn_best_model:v0/model.pt\") \n",
    "model2.to(device)\n",
    "model2.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtener embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36 images and 36 labels.\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 0\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 1\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 2\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 3\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 4\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 5\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 6\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 7\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 8\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 9\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 10\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 11\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 12\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 13\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 14\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 15\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 16\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 17\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 18\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 19\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 20\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 21\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 22\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 23\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 24\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 25\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 26\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 27\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 28\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 29\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 30\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 31\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 32\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 33\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 34\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 35\n"
     ]
    }
   ],
   "source": [
    "# Create dataset data loader\n",
    "dataset_path='./Dataset/Dataset_recurrence'\n",
    "train_set=CustomDataset(dataset_path, section=\"train\", transform=train_transform) # v_transform\n",
    "train_loader = DataLoader(train_set, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "# Directorios para embeddings de cada modelo\n",
    "embedding_dir_model1 = \"Dataset/contrastive_voxel_wise/embeddings_model1\"\n",
    "embedding_dir_model2 = \"Dataset/contrastive_voxel_wise/embeddings_model2\"\n",
    "label_output_dir = \"Dataset/contrastive_voxel_wise/labels\"\n",
    "\n",
    "# Crear carpetas si no existen\n",
    "os.makedirs(embedding_dir_model1, exist_ok=True)\n",
    "os.makedirs(embedding_dir_model2, exist_ok=True)\n",
    "os.makedirs(label_output_dir, exist_ok=True)\n",
    "\n",
    "# Variables para las características de los decoders de ambos modelos\n",
    "decoder_features_model1 = None\n",
    "decoder_features_model2 = None\n",
    "\n",
    "# Funciones hook para cada modelo\n",
    "def decoder_hook_fn_model1(module, input, output):\n",
    "    global decoder_features_model1\n",
    "    decoder_features_model1 = output\n",
    "\n",
    "def decoder_hook_fn_model2(module, input, output):\n",
    "    global decoder_features_model2\n",
    "    decoder_features_model2 = output\n",
    "\n",
    "# Registrar los hooks en los decoders de ambos modelos\n",
    "hook_handle_decoder1 = model1.decoder1.conv_block.register_forward_hook(decoder_hook_fn_model1)\n",
    "hook_handle_decoder2 = model2.decoder1.conv_block.register_forward_hook(decoder_hook_fn_model2)\n",
    "\n",
    "# Extraer y guardar\n",
    "with torch.no_grad():\n",
    "    for idx, batch_data in enumerate(train_loader):\n",
    "        image, label = batch_data[\"image\"], batch_data[\"label\"]\n",
    "        print(\"Image\", image.shape)  # [1, 11, 128, 128, 128]\n",
    "        print(\"label before squeeze\", label.shape)  # [1, 2, 128, 128, 128]\n",
    "        \n",
    "        image = image.to(device)\n",
    "        label = label.squeeze(0)  # [2, 128, 128, 128]\n",
    "        \n",
    "        # Convertir one-hot a etiquetas únicas\n",
    "        label_sum = label.sum(dim=0)  # [128, 128, 128], suma de canales\n",
    "        label_class = torch.zeros_like(label_sum, dtype=torch.long)  # [128, 128, 128]\n",
    "        \n",
    "        # Asignar clases:\n",
    "        # - Fondo (0, 0) -> 0\n",
    "        # - Vasogénico (1, 0) -> 1\n",
    "        # - Infiltrado (0, 1) -> 2\n",
    "        label_class[label[1] == 1] = 2  # Infiltrado\n",
    "        label_class[(label[0] == 1) & (label[1] == 0)] = 1  # Vasogénico\n",
    "        # Donde label_sum == 0, ya es fondo (0)\n",
    "        \n",
    "        label = label_class.cpu().numpy()  # [128, 128, 128]\n",
    "        print(\"label\", label.shape)\n",
    "        \n",
    "        # Obtener embeddings de ambos modelos\n",
    "        _ = model1(image)  # Forward para model1\n",
    "        print(\"decoder_features_model1:\", decoder_features_model1.shape)  # [1, 48, 128, 128, 128]\n",
    "        \n",
    "        _ = model2(image)  # Forward para model2\n",
    "        print(\"decoder_features_model2:\", decoder_features_model2.shape)  # [1, 48, 128, 128, 128]\n",
    "        \n",
    "        # Guardar embeddings y etiquetas\n",
    "        np.save(f\"{embedding_dir_model1}/case_{idx}.npy\", decoder_features_model1.cpu().numpy())\n",
    "        np.save(f\"{embedding_dir_model2}/case_{idx}.npy\", decoder_features_model2.cpu().numpy())\n",
    "        np.save(f\"{label_output_dir}/case_{idx}.npy\", label)\n",
    "        \n",
    "        print(f\"Guardado embeddings y etiquetas para caso {idx}\")\n",
    "\n",
    "# Remover los hooks\n",
    "hook_handle_decoder1.remove()\n",
    "hook_handle_decoder2.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1215566/749241035.py:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  projection_head1.load_state_dict(torch.load(\"trained_models/checkpoints_contrastive/contrastive_projection_head_final_o9kppyr5.pth\", map_location=device))\n",
      "/tmp/ipykernel_1215566/749241035.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  projection_head2.load_state_dict(torch.load(\"trained_models/checkpoints_contrastive/contrastive_projection_head_final_uixfayrn.pth\", map_location=device))\n",
      "/tmp/ipykernel_1215566/749241035.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  classifier1.load_state_dict(torch.load(\"trained_models/checkpoints/supervised_classifier_final_o9kppyr5.pth\", map_location=device))\n",
      "/tmp/ipykernel_1215566/749241035.py:83: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  classifier2.load_state_dict(torch.load(\"trained_models/checkpoints/supervised_classifier_final_uixfayrn.pth\", map_location=device))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nibabel as nib\n",
    "\n",
    "# Dispositivo\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset (ya lo tienes)\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, embedding_dir, label_dir):\n",
    "        self.embedding_dir = embedding_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.case_files = [f for f in os.listdir(embedding_dir) if f.endswith(\".npy\")]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.case_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        embedding_path = os.path.join(self.embedding_dir, f\"case_{idx}.npy\")\n",
    "        label_path = os.path.join(self.label_dir, f\"case_{idx}.npy\")\n",
    "        \n",
    "        embeddings = np.load(embedding_path)  # [1, 48, 128, 128, 128]\n",
    "        labels = np.load(label_path)  # [128, 128, 128]\n",
    "        \n",
    "        embeddings = torch.tensor(embeddings, dtype=torch.float32).squeeze(0)  # [48, 128, 128, 128]\n",
    "        labels = torch.tensor(labels, dtype=torch.long)  # [128, 128, 128]\n",
    "        \n",
    "        return embeddings, labels\n",
    "\n",
    "# Modelo de proyección\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, input_dim=48, hidden_dim=128, output_dim=128):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Clasificador supervisado\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim=128, num_classes=3):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Configuración para ambos modelos\n",
    "embedding_dir_model1 = \"Dataset/contrastive_voxel_wise/embeddings_model1\"\n",
    "embedding_dir_model2 = \"Dataset/contrastive_voxel_wise/embeddings_model2\"\n",
    "label_dir = \"Dataset/contrastive_voxel_wise/labels\"\n",
    "batch_size = 1\n",
    "\n",
    "# Cargar datasets y DataLoaders para ambos modelos\n",
    "dataset_model1 = EmbeddingDataset(embedding_dir_model1, label_dir)\n",
    "dataset_model2 = EmbeddingDataset(embedding_dir_model2, label_dir)\n",
    "loader_model1 = DataLoader(dataset_model1, batch_size=batch_size, shuffle=False)\n",
    "loader_model2 = DataLoader(dataset_model2, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Cargar modelos contrastivos preentrenados\n",
    "projection_head1 = ProjectionHead(input_dim=48).to(device)\n",
    "projection_head1.load_state_dict(torch.load(\"trained_models/checkpoints_contrastive/contrastive_projection_head_final_o9kppyr5.pth\", map_location=device))\n",
    "projection_head1.eval()\n",
    "\n",
    "projection_head2 = ProjectionHead(input_dim=48).to(device)\n",
    "projection_head2.load_state_dict(torch.load(\"trained_models/checkpoints_contrastive/contrastive_projection_head_final_uixfayrn.pth\", map_location=device))\n",
    "projection_head2.eval()\n",
    "\n",
    "# Cargar clasificadores preentrenados\n",
    "classifier1 = Classifier(input_dim=128, num_classes=3).to(device)\n",
    "classifier1.load_state_dict(torch.load(\"trained_models/checkpoints/supervised_classifier_final_o9kppyr5.pth\", map_location=device))\n",
    "classifier1.eval()\n",
    "\n",
    "classifier2 = Classifier(input_dim=128, num_classes=3).to(device)\n",
    "classifier2.load_state_dict(torch.load(\"trained_models/checkpoints/supervised_classifier_final_uixfayrn.pth\", map_location=device))\n",
    "classifier2.eval()\n",
    "\n",
    "# Función para generar mapas de probabilidad\n",
    "def generate_probability_maps(embeddings, projection_head, classifier, device):\n",
    "    with torch.no_grad():\n",
    "        embeddings = embeddings.to(device).squeeze(0).permute(1, 2, 3, 0)\n",
    "        embeddings_flat = embeddings.reshape(-1, 48)\n",
    "        \n",
    "        z = projection_head(embeddings_flat)\n",
    "        z = F.normalize(z, dim=1)\n",
    "        \n",
    "        logits = classifier(z)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        \n",
    "        probs = probs.view(128, 128, 128, 3).permute(3, 0, 1, 2)\n",
    "        return probs\n",
    "\n",
    "# Funciones para calcular métricas\n",
    "def calculate_metrics(pred, true, num_classes=3):\n",
    "    dice_scores = []\n",
    "    sensitivity_scores = []\n",
    "    precision_scores = []\n",
    "    \n",
    "    for cls in range(num_classes):\n",
    "        pred_cls = (pred == cls).astype(np.uint8)\n",
    "        true_cls = (true == cls).astype(np.uint8)\n",
    "        \n",
    "        tp = np.sum(pred_cls * true_cls)\n",
    "        fp = np.sum(pred_cls * (1 - true_cls))\n",
    "        fn = np.sum((1 - pred_cls) * true_cls)\n",
    "        \n",
    "        dice = 2 * tp / (2 * tp + fp + fn + 1e-6)\n",
    "        dice_scores.append(dice)\n",
    "        \n",
    "        sensitivity = tp / (tp + fn + 1e-6)\n",
    "        sensitivity_scores.append(sensitivity)\n",
    "        \n",
    "        precision = tp / (tp + fp + 1e-6)\n",
    "        precision_scores.append(precision)\n",
    "    \n",
    "    return dice_scores, sensitivity_scores, precision_scores\n",
    "\n",
    "# Directorio de salida\n",
    "output_dir = \"trained_models/mapas_combinados\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecutar calculo y guardar mapas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caso 0 - Dice: [0.9984164779656527, 0.421750040611237, 0.686223303396002], Sensitivity: [0.9970624177938241, 0.30777540641562523, 0.8939531632088092], Precision: [0.9997742208999835, 0.6697821100533382, 0.5568312118008502]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados/probability_maps_case_0.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados/labels_case_0.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados/segmentation_case_0.nii.gz\n",
      "Caso 1 - Dice: [0.9952324265824696, 0.6344339622552444, 0.47209805837979996], Sensitivity: [0.990528524581974, 0.5808441725211855, 0.861302242554708], Precision: [0.9999812182992083, 0.6989174141447907, 0.32516355588981233]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados/probability_maps_case_1.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados/labels_case_1.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados/segmentation_case_1.nii.gz\n",
      "Caso 2 - Dice: [0.9960911779713028, 0.779693135269274, 0.8077301119185157], Sensitivity: [0.9922854949031711, 0.9586381400701814, 0.7883823847864887], Precision: [0.9999261650741587, 0.6570452281902358, 0.8280513570481383]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados/probability_maps_case_2.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados/labels_case_2.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados/segmentation_case_2.nii.gz\n",
      "Caso 3 - Dice: [0.9967413888225356, 0.8307952182844233, 0.6698408452408763], Sensitivity: [0.9935175381200746, 0.9908259723852215, 0.6846566053868844], Precision: [0.9999862296882992, 0.7152701644319215, 0.655652720288231]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados/probability_maps_case_3.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados/labels_case_3.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados/segmentation_case_3.nii.gz\n",
      "Caso 4 - Dice: [0.9963004152742312, 0.7753830710522958, 0.5564433865874293], Sensitivity: [0.9926406734533882, 0.9321384205525417, 0.621455696163199], Precision: [0.9999872429761547, 0.6637602041938039, 0.5037451261797792]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados/probability_maps_case_4.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados/labels_case_4.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados/segmentation_case_4.nii.gz\n",
      "Caso 5 - Dice: [0.9962017378192517, 0.7511341542399065, 0.7365857277042642], Sensitivity: [0.9925574619031656, 0.9962572303326184, 0.6477559363241743], Precision: [0.9998728730125428, 0.6028151310520137, 0.8536508482225235]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados/probability_maps_case_5.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados/labels_case_5.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados/segmentation_case_5.nii.gz\n",
      "Caso 6 - Dice: [0.9948701214097255, 0.5388782560567399, 0.6127541913021464], Sensitivity: [0.9899019710463772, 0.819778918197811, 0.7434601664369178], Precision: [0.9998883919278974, 0.4013529837931214, 0.5211347264237324]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados/probability_maps_case_6.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados/labels_case_6.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados/segmentation_case_6.nii.gz\n",
      "Caso 7 - Dice: [0.9920052056415338, 0.5984924065736341, 0.35895932575377504], Sensitivity: [0.9841372308602953, 0.5692745676329318, 0.946381991986631], Precision: [0.9999999999995108, 0.6308716989213751, 0.22148364194936618]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados/probability_maps_case_7.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados/labels_case_7.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados/segmentation_case_7.nii.gz\n",
      "Caso 8 - Dice: [0.9985102934459723, 0.6639725372471218, 0.8891016645982165], Sensitivity: [0.9978031745130969, 0.6697641798601839, 0.9010662978395081], Precision: [0.9992184153252158, 0.6582801998682457, 0.8774506076868713]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados/probability_maps_case_8.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados/labels_case_8.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados/segmentation_case_8.nii.gz\n",
      "Caso 9 - Dice: [0.9960463481033938, 0.7775372509239253, 0.9028142441200238], Sensitivity: [0.9922587019853917, 0.9534216848425396, 0.9178201558370443], Precision: [0.9998630213998396, 0.656439262542183, 0.8882911177587627]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados/probability_maps_case_9.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados/labels_case_9.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados/segmentation_case_9.nii.gz\n",
      "Caso 10 - Dice: [0.9962795931258989, 0.80713415813056, 0.8694765598279454], Sensitivity: [0.9944222463385947, 0.8578375631854006, 0.8784500433633148], Precision: [0.9981438910694155, 0.7620900141692367, 0.8606845531198868]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados/probability_maps_case_10.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados/labels_case_10.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados/segmentation_case_10.nii.gz\n",
      "Caso 11 - Dice: [0.9962736439462965, 0.8145819553020888, 0.7796413187905975], Sensitivity: [0.99257849705724, 0.9900543694306118, 0.7198503078250393], Precision: [0.9999964060445188, 0.6919449947208385, 0.8502645837819598]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados/probability_maps_case_11.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados/labels_case_11.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados/segmentation_case_11.nii.gz\n",
      "Caso 12 - Dice: [0.9968476373502958, 0.4900059472985674, 0.48235444167793917], Sensitivity: [0.9938452168317047, 0.9169650633669563, 0.34838882030741947], Precision: [0.9998682535478545, 0.3343330980916167, 0.7837160145726186]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados/probability_maps_case_12.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados/labels_case_12.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados/segmentation_case_12.nii.gz\n",
      "Caso 13 - Dice: [0.99603340506626, 0.6925856697646707, 0.6247325096355144], Sensitivity: [0.9921150657811719, 0.6848201083940453, 0.8435839027738301], Precision: [0.9999828178857949, 0.7005293672447427, 0.4960436561950628]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados/probability_maps_case_13.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados/labels_case_13.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados/segmentation_case_13.nii.gz\n",
      "Caso 14 - Dice: [0.9957847214638633, 0.8676436168037652, 0.45014336505447544], Sensitivity: [0.9916425860384838, 0.8397851719967756, 0.9266219661038226], Precision: [0.9999616058063134, 0.8974137930928506, 0.2972792825456802]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados/probability_maps_case_14.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados/labels_case_14.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados/segmentation_case_14.nii.gz\n",
      "Caso 15 - Dice: [0.9931376396062161, 0.8376784774684077, 0.6449918420862352], Sensitivity: [0.9864700055052704, 0.9168933812396113, 0.841012868381036], Precision: [0.99989602128148, 0.7710626074920081, 0.5230749370570994]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados/probability_maps_case_15.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados/labels_case_15.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados/segmentation_case_15.nii.gz\n",
      "Caso 16 - Dice: [0.9962672276181077, 0.8201071514000251, 0.8743461984670186], Sensitivity: [0.9934849641400256, 0.9721098728289914, 0.8279184532512183], Precision: [0.9990651183672018, 0.7092121044791143, 0.9262904149620809]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados/probability_maps_case_16.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados/labels_case_16.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados/segmentation_case_16.nii.gz\n",
      "Caso 17 - Dice: [0.9931086127110459, 0.7399514431656359, 0.332421985473451], Sensitivity: [0.9865275463984279, 0.7920877850430136, 0.8923193722773226], Precision: [0.9997780724909661, 0.6942546189959181, 0.2042577842084212]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados/probability_maps_case_17.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados/labels_case_17.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados/segmentation_case_17.nii.gz\n",
      "Caso 18 - Dice: [0.9973686632160016, 0.875213169912058, 0.8394409647466645], Sensitivity: [0.9947511379557742, 0.9919082592193813, 0.84172290961657], Precision: [0.9999999999995013, 0.7830854678090231, 0.8371713592943042]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados/probability_maps_case_18.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados/labels_case_18.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados/segmentation_case_18.nii.gz\n",
      "Caso 19 - Dice: [0.9925649574532067, 0.8239652895617886, 0.7536052971072779], Sensitivity: [0.9852782097886135, 0.9883064223990242, 0.8786200278751474], Precision: [0.9999602882564835, 0.7064867172952409, 0.6597347306153681]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados/probability_maps_case_19.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados/labels_case_19.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados/segmentation_case_19.nii.gz\n",
      "Caso 20 - Dice: [0.994980341196168, 0.8698145025247793, 0.8306059462158476], Sensitivity: [0.990574423264147, 0.9813425965584291, 0.8087283628437986], Precision: [0.9994256278820343, 0.7810494231125034, 0.8537000973579951]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados/probability_maps_case_20.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados/labels_case_20.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados/segmentation_case_20.nii.gz\n",
      "Caso 21 - Dice: [0.9932147581245211, 0.8213978802537311, 0.614547208371253], Sensitivity: [0.9866846614346597, 0.9887931034198623, 0.7170074224733511], Precision: [0.9998318659125653, 0.7024742773007008, 0.5377087394140685]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados/probability_maps_case_21.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados/labels_case_21.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados/segmentation_case_21.nii.gz\n",
      "Caso 22 - Dice: [0.9948271092934571, 0.15322497151328035, 0.5208277894802607], Sensitivity: [0.989708889198452, 0.5831363273576545, 0.7356733938162451], Precision: [0.9999985417081064, 0.0882002383685101, 0.4031051964384314]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados/probability_maps_case_22.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados/labels_case_22.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados/segmentation_case_22.nii.gz\n",
      "Caso 23 - Dice: [0.998444064897818, 0.8265699737482238, 0.6886840077400249], Sensitivity: [0.9968977985373065, 0.982995901829898, 0.6923446764948799], Precision: [0.9999951354693809, 0.7130940231302705, 0.6850618458248725]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados/probability_maps_case_23.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados/labels_case_23.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados/segmentation_case_23.nii.gz\n",
      "Caso 24 - Dice: [0.996875951730988, 0.7144433441585955, 0.6637911768501366], Sensitivity: [0.9937742515600747, 0.749050783059501, 0.8856222791528641], Precision: [0.9999970741493335, 0.6828925306569911, 0.5308288951449108]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados/probability_maps_case_24.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados/labels_case_24.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados/segmentation_case_24.nii.gz\n",
      "Caso 25 - Dice: [0.9958802182954494, 0.17645901380597145, 0.7592033251818197], Sensitivity: [0.9921726351076087, 0.2958564264587152, 0.7943623705063232], Precision: [0.999615614652453, 0.12572197610465305, 0.7270246934773438]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados/probability_maps_case_25.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados/labels_case_25.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados/segmentation_case_25.nii.gz\n",
      "Caso 26 - Dice: [0.9910263670703368, 0.545784513482368, 0.5177042380686578], Sensitivity: [0.9822895841549404, 0.9789834379235883, 0.7244338413073481], Precision: [0.9999199599201773, 0.37836048203865447, 0.4027676043710972]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados/probability_maps_case_26.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados/labels_case_26.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados/segmentation_case_26.nii.gz\n",
      "Caso 27 - Dice: [0.9961853421998902, 0.6428253493331181, 0.7470987748233244], Sensitivity: [0.9926135359965186, 0.9087120664925069, 0.6641608036745352], Precision: [0.9997829467066338, 0.4973129205561881, 0.8537065217298511]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados/probability_maps_case_27.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados/labels_case_27.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados/segmentation_case_27.nii.gz\n",
      "Caso 28 - Dice: [0.9942749491029144, 0.28281063912006266, 0.17011838420332923], Sensitivity: [0.988618397067661, 0.2783682400315148, 0.5051037206939243], Precision: [0.9999966035240858, 0.2873971275960308, 0.10228371394891536]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados/probability_maps_case_28.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados/labels_case_28.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados/segmentation_case_28.nii.gz\n",
      "Caso 29 - Dice: [0.994600179109904, 0.8251388989045014, 0.6400110014673219], Sensitivity: [0.9892839601271208, 0.9863037328195283, 0.6274562674634313], Precision: [0.9999738434394065, 0.7092460740689518, 0.6530784072740544]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados/probability_maps_case_29.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados/labels_case_29.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados/segmentation_case_29.nii.gz\n",
      "Caso 30 - Dice: [0.9943146439752478, 0.6703532332630784, 0.589430518863352], Sensitivity: [0.988985150246535, 0.5892954579178012, 0.8839090329662915], Precision: [0.9997018886109325, 0.7772664820016735, 0.4421320992553707]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados/probability_maps_case_30.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados/labels_case_30.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados/segmentation_case_30.nii.gz\n",
      "Caso 31 - Dice: [0.9958797560425802, 0.7037949823757265, 0.7128865979324027], Sensitivity: [0.9918456248259258, 0.6188396227907488, 0.9439467622015197], Precision: [0.9999468372998614, 0.8157877117232623, 0.5727004503264571]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados/probability_maps_case_31.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados/labels_case_31.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados/segmentation_case_31.nii.gz\n",
      "Caso 32 - Dice: [0.9954043793619359, 0.6197264218815526, 0.7119449897269233], Sensitivity: [0.9909005536716756, 0.9317928032421369, 0.618803873990386], Precision: [0.9999493334238094, 0.46424597497031767, 0.8380930754322761]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados/probability_maps_case_32.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados/labels_case_32.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados/segmentation_case_32.nii.gz\n",
      "Caso 33 - Dice: [0.9958465925725238, 0.5064610253920758, 0.47970879038615505], Sensitivity: [0.9918926827197619, 0.5297965115509017, 0.7867053887716436], Precision: [0.9998321509511123, 0.4850944902202429, 0.3450569669398819]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados/probability_maps_case_33.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados/labels_case_33.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados/segmentation_case_33.nii.gz\n",
      "Caso 34 - Dice: [0.9970256612515106, 0.7821498903778578, 0.5866877692677976], Sensitivity: [0.9940885091643867, 0.7814433499781853, 0.7445877783892966], Precision: [0.9999802210979565, 0.7828577095484378, 0.4840403923021287]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados/probability_maps_case_34.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados/labels_case_34.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados/segmentation_case_34.nii.gz\n",
      "Caso 35 - Dice: [0.9948955266102516, 0.523220105876629, 0.55812089673189], Sensitivity: [0.9901875742333076, 0.9913378706155981, 0.4253323628052732], Precision: [0.9996484617850842, 0.3553981998860607, 0.8114574853419615]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados/probability_maps_case_35.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados/labels_case_35.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados/segmentation_case_35.nii.gz\n",
      "\n",
      "Clase 0 (Fondo):\n",
      "  Dice: 0.9955 ± 0.0017\n",
      "  Sensibilidad: 0.9912 ± 0.0035\n",
      "  Precisión: 0.9998 ± 0.0004\n",
      "\n",
      "Clase 1 (Vasogénico):\n",
      "  Dice: 0.6724 ± 0.1856\n",
      "  Sensibilidad: 0.8029 ± 0.2138\n",
      "  Precisión: 0.6125 ± 0.1898\n",
      "\n",
      "Clase 2 (Infiltrado):\n",
      "  Dice: 0.6426 ± 0.1655\n",
      "  Sensibilidad: 0.7656 ± 0.1410\n",
      "  Precisión: 0.6087 ± 0.2238\n"
     ]
    }
   ],
   "source": [
    "# Listas para métricas\n",
    "all_dice = {0: [], 1: [], 2: []}\n",
    "all_sensitivity = {0: [], 1: [], 2: []}\n",
    "all_precision = {0: [], 1: [], 2: []}\n",
    "\n",
    "# Procesar y combinar\n",
    "for idx, ((embeddings1, labels1), (embeddings2, labels2)) in enumerate(zip(loader_model1, loader_model2)):\n",
    "    # Generar mapas de probabilidad para ambos modelos\n",
    "    prob_maps1 = generate_probability_maps(embeddings1, projection_head1, classifier1, device)  # [3, 128, 128, 128]\n",
    "    prob_maps2 = generate_probability_maps(embeddings2, projection_head2, classifier2, device)  # [3, 128, 128, 128]\n",
    "    \n",
    "    # Combinar mapas:\n",
    "    # - Clase 0: del modelo 1\n",
    "    # - Clase 1: máximo entre ambos modelos\n",
    "    # - Clase 2: del modelo 2\n",
    "    combined_prob_maps = torch.zeros_like(prob_maps1)  # [3, 128, 128, 128]\n",
    "    combined_prob_maps[0] = prob_maps1[0]  # Clase 0 del modelo 1\n",
    "    combined_prob_maps[1] = torch.max(prob_maps1[1], prob_maps2[1])  # Clase 1 máximo entre ambos\n",
    "    combined_prob_maps[2] = prob_maps2[2]  # Clase 2 del modelo 2\n",
    "    \n",
    "    # Normalizar probabilidades para que sumen 1 en cada vóxel\n",
    "    combined_prob_maps = combined_prob_maps / combined_prob_maps.sum(dim=0, keepdim=True)\n",
    "    \n",
    "    # Convertir a numpy\n",
    "    prob_maps_np = combined_prob_maps.cpu().numpy()  # [3, 128, 128, 128]\n",
    "    prob_maps_np_nifti = np.transpose(prob_maps_np, (1, 2, 3, 0))  # [128, 128, 128, 3]\n",
    "    \n",
    "    # Generar segmentación semántica\n",
    "    segmentation = np.argmax(prob_maps_np, axis=0)  # [128, 128, 128]\n",
    "    segmentation_np = segmentation.astype(np.uint8)\n",
    "    \n",
    "    # Etiquetas (usamos las del modelo 2, asumiendo que son iguales)\n",
    "    labels = labels2.squeeze(0)  # [128, 128, 128]\n",
    "    labels_np = labels.cpu().numpy().astype(np.uint8)\n",
    "    \n",
    "    # Calcular métricas\n",
    "    dice, sensitivity, precision = calculate_metrics(segmentation_np, labels_np)\n",
    "    for cls in range(3):\n",
    "        all_dice[cls].append(dice[cls])\n",
    "        all_sensitivity[cls].append(sensitivity[cls])\n",
    "        all_precision[cls].append(precision[cls])\n",
    "    \n",
    "    print(f\"Caso {idx} - Dice: {dice}, Sensitivity: {sensitivity}, Precision: {precision}\")\n",
    "    \n",
    "    # Crear imágenes NIfTI\n",
    "    affine = np.eye(4)\n",
    "    \n",
    "    # Guardar mapas de probabilidad combinados\n",
    "    nifti_prob_img = nib.Nifti1Image(prob_maps_np_nifti, affine)\n",
    "    prob_output_path = os.path.join(output_dir, f\"probability_maps_case_{idx}.nii.gz\")\n",
    "    nib.save(nifti_prob_img, prob_output_path)\n",
    "    print(f\"Guardado mapa de probabilidad en {prob_output_path}\")\n",
    "    \n",
    "    # Guardar etiquetas\n",
    "    nifti_label_img = nib.Nifti1Image(labels_np, affine)\n",
    "    label_output_path = os.path.join(output_dir, f\"labels_case_{idx}.nii.gz\")\n",
    "    nib.save(nifti_label_img, label_output_path)\n",
    "    print(f\"Guardadas etiquetas en {label_output_path}\")\n",
    "    \n",
    "    # Guardar segmentación semántica\n",
    "    nifti_seg_img = nib.Nifti1Image(segmentation_np, affine)\n",
    "    seg_output_path = os.path.join(output_dir, f\"segmentation_case_{idx}.nii.gz\")\n",
    "    nib.save(nifti_seg_img, seg_output_path)\n",
    "    print(f\"Guardada segmentación en {seg_output_path}\")\n",
    "\n",
    "# Calcular promedios y desviaciones estándar\n",
    "class_names = [\"Fondo\", \"Vasogénico\", \"Infiltrado\"]\n",
    "for cls in range(3):\n",
    "    dice_mean = np.mean(all_dice[cls])\n",
    "    dice_std = np.std(all_dice[cls])\n",
    "    sens_mean = np.mean(all_sensitivity[cls])\n",
    "    sens_std = np.std(all_sensitivity[cls])\n",
    "    prec_mean = np.mean(all_precision[cls])\n",
    "    prec_std = np.std(all_precision[cls])\n",
    "    \n",
    "    print(f\"\\nClase {cls} ({class_names[cls]}):\")\n",
    "    print(f\"  Dice: {dice_mean:.4f} ± {dice_std:.4f}\")\n",
    "    print(f\"  Sensibilidad: {sens_mean:.4f} ± {sens_std:.4f}\")\n",
    "    print(f\"  Precisión: {prec_mean:.4f} ± {prec_std:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
