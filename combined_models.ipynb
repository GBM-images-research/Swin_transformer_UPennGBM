{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading functions\n",
    "import os\n",
    "import time\n",
    "from monai.data import DataLoader, decollate_batch\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "\n",
    "from src.get_data import CustomDataset, CustomDatasetSeg\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from types import SimpleNamespace\n",
    "import wandb\n",
    "import logging\n",
    "\n",
    "#####\n",
    "import json\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai import transforms\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    Activations,\n",
    "    MapTransform,\n",
    "    Transform,\n",
    ")\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.utils.enums import MetricReduction\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai import data\n",
    "\n",
    "# from monai.data import decollate_batch\n",
    "from functools import partial\n",
    "from src.custom_transforms import (ConvertToMultiChannelBasedOnN_Froi, \n",
    "                                   ConvertToMultiChannelBasedOnAnotatedInfiltration, \n",
    "                                   masked, ConvertToMultiChannelBasedOnBratsClassesdI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trasnformaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/training2/miniconda3/envs/monai_env/lib/python3.11/site-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.croppad.dictionary CropForegroundd.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    }
   ],
   "source": [
    "roi = (128, 128, 128) # (220, 220, 155) (128, 128, 64)\n",
    "source_k=\"label\"\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        # ConvertToMultiChannelBasedOnN_Froi(keys=\"label\"),\n",
    "        # masked(keys=\"image\"),\n",
    "        # ConvertToMultiChannelBasedOnAnotatedInfiltration(keys=\"label\"),\n",
    "        ConvertToMultiChannelBasedOnBratsClassesdI(keys=\"label\"),\n",
    "        transforms.CropForegroundd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            source_key=source_k,\n",
    "            k_divisible=[roi[0], roi[1], roi[2]],\n",
    "        ),\n",
    "        transforms.RandSpatialCropd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            roi_size=[roi[0], roi[1], roi[2]],\n",
    "            random_size=False,\n",
    "        ),\n",
    "        transforms.NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "        \n",
    "    ]\n",
    ")\n",
    "val_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        # ConvertToMultiChannelBasedOnN_Froi(keys=\"label\"),\n",
    "        # masked(keys=\"image\"),\n",
    "        # ConvertToMultiChannelBasedOnAnotatedInfiltration(keys=\"label\"),\n",
    "        ConvertToMultiChannelBasedOnBratsClassesdI(keys=\"label\"),\n",
    "        transforms.RandSpatialCropd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            roi_size=[-1, -1, -1], #[224, 224, 128],\n",
    "            random_size=False,\n",
    "        ),\n",
    "        transforms.NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Crear el modelo\n",
    "######################\n",
    "\n",
    "### Hyperparameter\n",
    "roi = (128, 128, 128)  # (128, 128, 128)\n",
    "\n",
    "# Create Swin transformer\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def define_model(model_path):\n",
    "    model = SwinUNETR(\n",
    "        img_size=roi,\n",
    "        in_channels=11,\n",
    "        out_channels=2,  # mdificar con edema\n",
    "        feature_size=48, #48\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        dropout_path_rate=0.0,\n",
    "        use_checkpoint=True,\n",
    "    )\n",
    "\n",
    "    # # Load the best model\n",
    "    # model_path = \"artifacts/o9kppyr5_best_model:v0/model.pt\"\n",
    "\n",
    "    # Load the model on CPU\n",
    "    loaded_model = torch.load(model_path, map_location=torch.device('cuda:0'), weights_only=False)[\"state_dict\"]\n",
    "\n",
    "    # Load the state dictionary into the model\n",
    "    model.load_state_dict(loaded_model)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/training2/miniconda3/envs/monai_env/lib/python3.11/site-packages/monai/utils/deprecate_utils.py:221: FutureWarning: monai.networks.nets.swin_unetr SwinUNETR.__init__:img_size: Argument `img_size` has been deprecated since version 1.3. It will be removed in version 1.5. The img_size argument is not required anymore and checks on the input size are run during forward().\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SwinUNETR(\n",
       "  (swinViT): SwinTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv3d(11, 48, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (layers1): ModuleList(\n",
       "      (0): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=48, out_features=144, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=48, out_features=48, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (linear1): Linear(in_features=48, out_features=192, bias=True)\n",
       "              (linear2): Linear(in_features=192, out_features=48, bias=True)\n",
       "              (fn): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=384, out_features=96, bias=False)\n",
       "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layers2): ModuleList(\n",
       "      (0): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (linear1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (linear2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (fn): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=768, out_features=192, bias=False)\n",
       "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layers3): ModuleList(\n",
       "      (0): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (fn): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=1536, out_features=384, bias=False)\n",
       "          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layers4): ModuleList(\n",
       "      (0): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (linear1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (linear2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (fn): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder1): UnetrBasicBlock(\n",
       "    (layer): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(11, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(11, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder2): UnetrBasicBlock(\n",
       "    (layer): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder3): UnetrBasicBlock(\n",
       "    (layer): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder4): UnetrBasicBlock(\n",
       "    (layer): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder10): UnetrBasicBlock(\n",
       "    (layer): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(768, 768, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(768, 768, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder5): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(768, 384, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(768, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(768, 384, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder4): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(384, 192, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(384, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(384, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder3): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(192, 96, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(192, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(192, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder2): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(96, 48, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(96, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(96, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder1): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(48, 48, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(96, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(96, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (out): UnetOutBlock(\n",
       "    (conv): Convolution(\n",
       "      (conv): Conv3d(48, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo TC+Edema\n",
    "model1=define_model(\"artifacts/vtzpbajf_best_model:v0/model.pt\") # o9kppyr5\n",
    "model1.to(device)\n",
    "model1.eval()\n",
    "\n",
    "# Modelo Infitracion+Edema\n",
    "model2=define_model(\"artifacts/1dhzmigz_best_model:v0/model.pt\") # uixfayrn - rvu24jip\n",
    "model2.to(device)\n",
    "model2.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtener embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 images and 6 labels.\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 0\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 1\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 2\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 3\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 4\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 5\n"
     ]
    }
   ],
   "source": [
    "# Create dataset data loader\n",
    "# dataset_path='./Dataset/Dataset_recurrence'\n",
    "dataset_path='./Dataset/Dataset_30_6'\n",
    "train_set=CustomDataset(dataset_path, section=\"test_6\", transform=train_transform) # v_transform\n",
    "train_loader = DataLoader(train_set, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "# Directorios para embeddings de cada modelo\n",
    "embedding_dir_model1 = \"Dataset/contrastive_voxel_wise/embeddings_model1\"\n",
    "embedding_dir_model2 = \"Dataset/contrastive_voxel_wise/embeddings_model2\"\n",
    "label_output_dir = \"Dataset/contrastive_voxel_wise/labels\"\n",
    "\n",
    "# Crear carpetas si no existen\n",
    "os.makedirs(embedding_dir_model1, exist_ok=True)\n",
    "os.makedirs(embedding_dir_model2, exist_ok=True)\n",
    "os.makedirs(label_output_dir, exist_ok=True)\n",
    "\n",
    "# Variables para las características de los decoders de ambos modelos\n",
    "decoder_features_model1 = None\n",
    "decoder_features_model2 = None\n",
    "\n",
    "# Funciones hook para cada modelo\n",
    "def decoder_hook_fn_model1(module, input, output):\n",
    "    global decoder_features_model1\n",
    "    decoder_features_model1 = output\n",
    "\n",
    "def decoder_hook_fn_model2(module, input, output):\n",
    "    global decoder_features_model2\n",
    "    decoder_features_model2 = output\n",
    "\n",
    "# Registrar los hooks en los decoders de ambos modelos\n",
    "hook_handle_decoder1 = model1.decoder1.conv_block.register_forward_hook(decoder_hook_fn_model1)\n",
    "hook_handle_decoder2 = model2.decoder1.conv_block.register_forward_hook(decoder_hook_fn_model2)\n",
    "\n",
    "# Extraer y guardar\n",
    "with torch.no_grad():\n",
    "    for idx, batch_data in enumerate(train_loader):\n",
    "        image, label = batch_data[\"image\"], batch_data[\"label\"]\n",
    "        print(\"Image\", image.shape)  # [1, 11, 128, 128, 128]\n",
    "        print(\"label before squeeze\", label.shape)  # [1, 2, 128, 128, 128]\n",
    "        \n",
    "        image = image.to(device)\n",
    "        label = label.squeeze(0)  # [2, 128, 128, 128]\n",
    "        \n",
    "        # Convertir one-hot a etiquetas únicas\n",
    "        label_sum = label.sum(dim=0)  # [128, 128, 128], suma de canales\n",
    "        label_class = torch.zeros_like(label_sum, dtype=torch.long)  # [128, 128, 128]\n",
    "        \n",
    "        # Asignar clases:\n",
    "        # - Fondo (0, 0) -> 0\n",
    "        # - Vasogénico (1, 0) -> 1\n",
    "        # - Infiltrado (0, 1) -> 2\n",
    "        label_class[label[1] == 1] = 2  # Infiltrado\n",
    "        label_class[(label[0] == 1) & (label[1] == 0)] = 1  # Vasogénico\n",
    "        # Donde label_sum == 0, ya es fondo (0)\n",
    "        \n",
    "        label = label_class.cpu().numpy()  # [128, 128, 128]\n",
    "        print(\"label\", label.shape)\n",
    "        \n",
    "        # Obtener embeddings de ambos modelos\n",
    "        _ = model1(image)  # Forward para model1\n",
    "        print(\"decoder_features_model1:\", decoder_features_model1.shape)  # [1, 48, 128, 128, 128]\n",
    "        \n",
    "        _ = model2(image)  # Forward para model2\n",
    "        print(\"decoder_features_model2:\", decoder_features_model2.shape)  # [1, 48, 128, 128, 128]\n",
    "        \n",
    "        # Guardar embeddings y etiquetas\n",
    "        np.save(f\"{embedding_dir_model1}/case_{idx}.npy\", decoder_features_model1.cpu().numpy())\n",
    "        np.save(f\"{embedding_dir_model2}/case_{idx}.npy\", decoder_features_model2.cpu().numpy())\n",
    "        np.save(f\"{label_output_dir}/case_{idx}.npy\", label)\n",
    "        \n",
    "        print(f\"Guardado embeddings y etiquetas para caso {idx}\")\n",
    "\n",
    "# Remover los hooks\n",
    "hook_handle_decoder1.remove()\n",
    "hook_handle_decoder2.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nibabel as nib\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dispositivo\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset (ya lo tienes)\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, embedding_dir, label_dir):\n",
    "        self.embedding_dir = embedding_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.case_files = [f for f in os.listdir(embedding_dir) if f.endswith(\".npy\")]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.case_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        embedding_path = os.path.join(self.embedding_dir, f\"case_{idx}.npy\")\n",
    "        label_path = os.path.join(self.label_dir, f\"case_{idx}.npy\")\n",
    "        \n",
    "        embeddings = np.load(embedding_path)  # [1, 48, 128, 128, 128]\n",
    "        labels = np.load(label_path)  # [128, 128, 128]\n",
    "        \n",
    "        embeddings = torch.tensor(embeddings, dtype=torch.float32).squeeze(0)  # [48, 128, 128, 128]\n",
    "        labels = torch.tensor(labels, dtype=torch.long)  # [128, 128, 128]\n",
    "        \n",
    "        return embeddings, labels\n",
    "\n",
    "# # Modelo de proyección\n",
    "# class ProjectionHead(nn.Module):\n",
    "#     def __init__(self, input_dim=48, hidden_dim=128, output_dim=128):\n",
    "#         super(ProjectionHead, self).__init__()\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.Linear(input_dim, hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim, output_dim)\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         return self.net(x)\n",
    "\n",
    "# Modelo de proyección (MLP más profundo)\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, input_dim=48, hidden_dim1=256, hidden_dim2=128, output_dim=128, dropout_p=0.3):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_dim2, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# # Clasificador supervisado\n",
    "# class Classifier(nn.Module):\n",
    "#     def __init__(self, input_dim=128, num_classes=3):\n",
    "#         super(Classifier, self).__init__()\n",
    "#         self.fc = nn.Linear(input_dim, num_classes)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         return self.fc(x)\n",
    "\n",
    "# Clasificador supervisado (MLP)\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim=128, hidden_dim1=256, hidden_dim2=128, num_classes=3, dropout_p=0.3):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_dim2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Configuración para ambos modelos\n",
    "embedding_dir_model1 = \"Dataset/contrastive_voxel_wise/embeddings_model1\"\n",
    "embedding_dir_model2 = \"Dataset/contrastive_voxel_wise/embeddings_model2\"\n",
    "label_dir = \"Dataset/contrastive_voxel_wise/labels\"\n",
    "batch_size = 1\n",
    "\n",
    "# Cargar datasets y DataLoaders para ambos modelos\n",
    "dataset_model1 = EmbeddingDataset(embedding_dir_model1, label_dir)\n",
    "dataset_model2 = EmbeddingDataset(embedding_dir_model2, label_dir)\n",
    "loader_model1 = DataLoader(dataset_model1, batch_size=batch_size, shuffle=False)\n",
    "loader_model2 = DataLoader(dataset_model2, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Cargar modelos contrastivos preentrenados\n",
    "projection_head1 = ProjectionHead(input_dim=48).to(device)\n",
    "projection_head1.load_state_dict(torch.load(\"trained_models/checkpoints_contrastive/contrastive_projection_head_final_new_pipe1_v01_m1.pth\", map_location=device))\n",
    "projection_head1.eval()\n",
    "\n",
    "projection_head2 = ProjectionHead(input_dim=48).to(device)\n",
    "projection_head2.load_state_dict(torch.load(\"trained_models/checkpoints_contrastive/contrastive_projection_head_final_new_pipe2_m1_1dhzmigz.pth\", map_location=device))\n",
    "projection_head2.eval()\n",
    "\n",
    "# Cargar clasificadores preentrenados\n",
    "classifier1 = Classifier(input_dim=128, num_classes=3).to(device)\n",
    "classifier1.load_state_dict(torch.load(\"trained_models/checkpoints/supervised_classifier_final_pipe1_v01_m1.pth\", map_location=device))\n",
    "classifier1.eval()\n",
    "\n",
    "classifier2 = Classifier(input_dim=128, num_classes=3).to(device)\n",
    "classifier2.load_state_dict(torch.load(\"trained_models/checkpoints/supervised_classifier_final_pipe2_m1_1dhzmigz.pth\", map_location=device))\n",
    "classifier2.eval()\n",
    "\n",
    "# Función para generar mapas de probabilidad\n",
    "def generate_probability_maps(embeddings, projection_head, classifier, device):\n",
    "    with torch.no_grad():\n",
    "        embeddings = embeddings.to(device).squeeze(0).permute(1, 2, 3, 0)\n",
    "        embeddings_flat = embeddings.reshape(-1, 48)\n",
    "        \n",
    "        z = projection_head(embeddings_flat)\n",
    "        z = F.normalize(z, dim=1)\n",
    "        \n",
    "        logits = classifier(z)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        \n",
    "        probs = probs.view(128, 128, 128, 3).permute(3, 0, 1, 2)\n",
    "        return probs\n",
    "\n",
    "# Funciones para calcular métricas\n",
    "def calculate_metrics(pred, true,prob_maps=None, num_classes=3):\n",
    "    dice_scores = []\n",
    "    sensitivity_scores = []\n",
    "    precision_scores = []\n",
    "    auc_scores = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    # Calcular Accuracy global\n",
    "    accuracy = accuracy_score(true.flatten(), pred.flatten())\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        pred_cls = (pred == cls).astype(np.uint8)\n",
    "        true_cls = (true == cls).astype(np.uint8)\n",
    "        \n",
    "        tp = np.sum(pred_cls * true_cls)\n",
    "        fp = np.sum(pred_cls * (1 - true_cls))\n",
    "        fn = np.sum((1 - pred_cls) * true_cls)\n",
    "        \n",
    "        dice = 2 * tp / (2 * tp + fp + fn + 1e-6)\n",
    "        dice_scores.append(dice)\n",
    "        \n",
    "        sensitivity = tp / (tp + fn + 1e-6)\n",
    "        sensitivity_scores.append(sensitivity)\n",
    "        \n",
    "        precision = tp / (tp + fp + 1e-6)\n",
    "        precision_scores.append(precision)\n",
    "\n",
    "        # F1 Score\n",
    "        f1 = f1_score(true_cls.flatten(), pred_cls.flatten(), zero_division=0)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        # AUC-ROC (requiere mapas de probabilidad)\n",
    "        if prob_maps is not None:\n",
    "            try:\n",
    "                auc = roc_auc_score(true_cls.flatten(), prob_maps[cls].flatten())\n",
    "                auc_scores.append(auc)\n",
    "            except ValueError:\n",
    "                auc_scores.append(np.nan)  # Manejar casos donde AUC no se puede calcular\n",
    "        else:\n",
    "            auc_scores.append(np.nan)  # Si no se proporcionan mapas de probabilidad\n",
    "    \n",
    "    return dice_scores, sensitivity_scores, precision_scores, auc_scores, accuracy, f1_scores\n",
    "\n",
    "# Directorio de salida\n",
    "output_dir = \"trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test_cube5_center\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecutar calculo y guardar mapas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardada curva ROC en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/roc_curve_case_0.png\n",
      "Caso 0 - Dice: [0.9970035703989371, 0.520193908666654, 0.6986080252122276], Sensitivity: [0.9941729438903477, 0.4651015729859273, 0.8572294069472858], Precision: [0.9998503617499696, 0.5900915958885441, 0.5895229395928157], AUC-ROC: [0.9997574644423223, 0.9910906713472637, 0.9931217498754038], Accuracy: 0.9818, F1 Score: [0.9970035703991849, 0.5201939086743189, 0.698608025218861]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/probability_maps_case_0.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/labels_case_0.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/segmentation_case_0.nii.gz\n",
      "Guardada curva ROC en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/roc_curve_case_1.png\n",
      "Caso 1 - Dice: [0.9961940335339866, 0.7573532532253574, 0.7093981720864045], Sensitivity: [0.9927684989178468, 0.8805763915435673, 0.8734924409345383], Precision: [0.9996432895239844, 0.6643830515552622, 0.5972068985367515], AUC-ROC: [0.9991617904186789, 0.9987704127823962, 0.9967641799853623], Accuracy: 0.9909, F1 Score: [0.9961940335342289, 0.7573532532575018, 0.7093981720986376]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/probability_maps_case_1.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/labels_case_1.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/segmentation_case_1.nii.gz\n",
      "Guardada curva ROC en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/roc_curve_case_2.png\n",
      "Caso 2 - Dice: [0.9954207128587987, 0.7141549611409692, 0.8115478311414778], Sensitivity: [0.9918475049000461, 0.9249993951340334, 0.7700496806169659], Precision: [0.9990197594230913, 0.5815877941468294, 0.8577734344881823], AUC-ROC: [0.9994691909437659, 0.9964043270508957, 0.9960599556149978], Accuracy: 0.9798, F1 Score: [0.9954207128590542, 0.714154961147639, 0.8115478311456931]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/probability_maps_case_2.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/labels_case_2.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/segmentation_case_2.nii.gz\n",
      "Guardada curva ROC en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/roc_curve_case_3.png\n",
      "Caso 3 - Dice: [0.9946954256219234, 0.6495593571237637, 0.3871328671230227], Sensitivity: [0.9896221671657137, 0.5427375107503988, 0.9631785396731395], Precision: [0.9998209678212706, 0.8087349395850397, 0.2422506524014305], AUC-ROC: [0.9983193412576696, 0.9983511843142627, 0.9961697289113187], Accuracy: 0.9880, F1 Score: [0.9946954256221634, 0.649559357179886, 0.38713286713286715]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/probability_maps_case_3.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/labels_case_3.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/segmentation_case_3.nii.gz\n",
      "Guardada curva ROC en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/roc_curve_case_4.png\n",
      "Caso 4 - Dice: [0.9965279044150087, 0.7510474573280651, 0.6372424828180269], Sensitivity: [0.9930836863287986, 0.8135382059318024, 0.8549704811941695], Precision: [0.9999960961570087, 0.6974721529577604, 0.5078999574710252], AUC-ROC: [0.9996140716399262, 0.9982643383338923, 0.9963530494438688], Accuracy: 0.9905, F1 Score: [0.996527904415251, 0.7510474573486321, 0.6372424828321889]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/probability_maps_case_4.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/labels_case_4.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/segmentation_case_4.nii.gz\n",
      "Guardada curva ROC en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/roc_curve_case_5.png\n",
      "Caso 5 - Dice: [0.9969356400590003, 0.7873596864002423, 0.6544481470151394], Sensitivity: [0.9939036829084408, 0.7655546205819543, 0.8678748381525744], Precision: [0.9999861521116298, 0.8104433032694811, 0.5252735377407733], AUC-ROC: [0.9998924458565644, 0.9978815404296216, 0.996220551229559], Accuracy: 0.9882, F1 Score: [0.996935640059246, 0.7873596864102629, 0.6544481470261537]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/probability_maps_case_5.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/labels_case_5.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/segmentation_case_5.nii.gz\n",
      "Guardada curva ROC promedio en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test/roc_curve_average.png\n",
      "\n",
      "Clase 0 (Fondo):\n",
      "  Dice: 0.9961 ± 0.0008\n",
      "  Sensibilidad: 0.9926 ± 0.0015\n",
      "  Precisión: 0.9997 ± 0.0003\n",
      "  AUC-ROC: 0.9994 ± 0.0005\n",
      "  F1 Score: 0.9961 ± 0.0008\n",
      "\n",
      "Clase 1 (Vasogénico):\n",
      "  Dice: 0.6966 ± 0.0899\n",
      "  Sensibilidad: 0.7321 ± 0.1704\n",
      "  Precisión: 0.6921 ± 0.0922\n",
      "  AUC-ROC: 0.9968 ± 0.0027\n",
      "  F1 Score: 0.6966 ± 0.0899\n",
      "\n",
      "Clase 2 (Infiltrado):\n",
      "  Dice: 0.6497 ± 0.1299\n",
      "  Sensibilidad: 0.8645 ± 0.0561\n",
      "  Precisión: 0.5533 ± 0.1805\n",
      "  AUC-ROC: 0.9958 ± 0.0012\n",
      "  F1 Score: 0.6497 ± 0.1299\n",
      "\n",
      "Accuracy Global: 0.9865 ± 0.0042\n"
     ]
    }
   ],
   "source": [
    "# Listas para métricas\n",
    "all_dice = {0: [], 1: [], 2: []}\n",
    "all_sensitivity = {0: [], 1: [], 2: []}\n",
    "all_precision = {0: [], 1: [], 2: []}\n",
    "all_auc = {0: [], 1: [], 2: []}  # Lista para AUC-ROC\n",
    "all_accuracy = []  # Lista para Accuracy global\n",
    "all_f1 = {0: [], 1: [], 2: []}  # Lista para F1 Score\n",
    "# Listas para almacenar FPR y TPR de todos los casos\n",
    "all_fpr = {0: [], 1: [], 2: []}\n",
    "all_tpr = {0: [], 1: [], 2: []}\n",
    "\n",
    "\n",
    "# Procesar y combinar\n",
    "for idx, ((embeddings1, labels1), (embeddings2, labels2)) in enumerate(zip(loader_model1, loader_model2)):\n",
    "    # Generar mapas de probabilidad para ambos modelos\n",
    "    prob_maps1 = generate_probability_maps(embeddings1, projection_head1, classifier1, device)  # [3, 128, 128, 128]\n",
    "    prob_maps2 = generate_probability_maps(embeddings2, projection_head2, classifier2, device)  # [3, 128, 128, 128]\n",
    "    \n",
    "    # Combinar mapas:\n",
    "    # - Clase 0: del modelo 1\n",
    "    # - Clase 1: máximo entre ambos modelos\n",
    "    # - Clase 2: del modelo 2\n",
    "    combined_prob_maps = torch.zeros_like(prob_maps1)  # [3, 128, 128, 128]\n",
    "    combined_prob_maps[0] = prob_maps1[0]  # Clase 0 del modelo 1\n",
    "    combined_prob_maps[1] = torch.max(prob_maps1[1], prob_maps2[1])  # Clase 1 máximo entre ambos\n",
    "    combined_prob_maps[2] = prob_maps2[2]  # Clase 2 del modelo 2\n",
    "    \n",
    "    # Normalizar probabilidades para que sumen 1 en cada vóxel\n",
    "    combined_prob_maps = combined_prob_maps / combined_prob_maps.sum(dim=0, keepdim=True)\n",
    "    \n",
    "    # Convertir a numpy\n",
    "    prob_maps_np = combined_prob_maps.cpu().numpy()  # [3, 128, 128, 128]\n",
    "    prob_maps_np_nifti = np.transpose(prob_maps_np, (1, 2, 3, 0))  # [128, 128, 128, 3]\n",
    "    \n",
    "    # Generar segmentación semántica\n",
    "    segmentation = np.argmax(prob_maps_np, axis=0)  # [128, 128, 128]\n",
    "    # segmentation_np = segmentation.astype(np.uint8)\n",
    "\n",
    "    # Aplicar umbral: asignar clase 1 si la probabilidad es > 0.4\n",
    "    # class_1_mask = prob_maps_np[1] > 0.3  # Máscara booleana para clase 1\n",
    "    # segmentation[class_1_mask] = 1  # Asignar clase 1 a los vóxeles que cumplen el criterio\n",
    "    segmentation_np = segmentation.astype(np.uint8)\n",
    "    \n",
    "    # Etiquetas (usamos las del modelo 2, asumiendo que son iguales)\n",
    "    labels = labels2.squeeze(0)  # [128, 128, 128]\n",
    "    labels_np = labels.cpu().numpy().astype(np.uint8)\n",
    "    \n",
    "    # Calcular métricas\n",
    "    dice, sensitivity, precision, auc, accuracy, f1 = calculate_metrics(segmentation_np, labels_np, prob_maps=prob_maps_np)\n",
    "    for cls in range(3):\n",
    "        all_dice[cls].append(dice[cls])\n",
    "        all_sensitivity[cls].append(sensitivity[cls])\n",
    "        all_precision[cls].append(precision[cls])\n",
    "        all_auc[cls].append(auc[cls])\n",
    "        all_f1[cls].append(f1[cls])\n",
    "    all_accuracy.append(accuracy)\n",
    "\n",
    "     # Graficar curvas ROC\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    class_names = [\"Fondo\", \"Vasogénico\", \"Infiltrado\"]\n",
    "    colors = ['blue', 'green', 'red']\n",
    "    \n",
    "    for cls in range(3):\n",
    "        # Etiquetas binarias para la clase actual\n",
    "        true_cls = (labels_np == cls).astype(np.uint8).flatten()\n",
    "        prob_cls = prob_maps_np[cls].flatten()\n",
    "        \n",
    "        # Calcular puntos de la curva ROC\n",
    "        fpr, tpr, _ = roc_curve(true_cls, prob_cls)\n",
    "        auc_value = auc[cls]  # Usar el AUC calculado previamente\n",
    "        all_fpr[cls].append(fpr)\n",
    "        all_tpr[cls].append(tpr)\n",
    "        \n",
    "        # Graficar\n",
    "        plt.plot(fpr, tpr, color=colors[cls], label=f'{class_names[cls]} (AUC = {auc_value:.4f})')\n",
    "    \n",
    "    # Configurar el gráfico\n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # Línea diagonal (clasificador aleatorio)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Tasa de Falsos Positivos (FPR)')\n",
    "    plt.ylabel('Tasa de Verdaderos Positivos (TPR)')\n",
    "    plt.title(f'Curva ROC - Caso {idx}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    # Guardar el gráfico\n",
    "    roc_output_path = os.path.join(output_dir, f\"roc_curve_case_{idx}.png\")\n",
    "    plt.savefig(roc_output_path)\n",
    "    plt.close()\n",
    "    print(f\"Guardada curva ROC en {roc_output_path}\")\n",
    "\n",
    "    print(f\"Caso {idx} - Dice: {dice}, Sensitivity: {sensitivity}, Precision: {precision}, \"\n",
    "          f\"AUC-ROC: {auc}, Accuracy: {accuracy:.4f}, F1 Score: {f1}\")\n",
    "    \n",
    "    # Crear imágenes NIfTI\n",
    "    affine = np.eye(4)\n",
    "    \n",
    "    # Guardar mapas de probabilidad combinados\n",
    "    nifti_prob_img = nib.Nifti1Image(prob_maps_np_nifti, affine)\n",
    "    prob_output_path = os.path.join(output_dir, f\"probability_maps_case_{idx}.nii.gz\")\n",
    "    nib.save(nifti_prob_img, prob_output_path)\n",
    "    print(f\"Guardado mapa de probabilidad en {prob_output_path}\")\n",
    "    \n",
    "    # Guardar etiquetas\n",
    "    nifti_label_img = nib.Nifti1Image(labels_np, affine)\n",
    "    label_output_path = os.path.join(output_dir, f\"labels_case_{idx}.nii.gz\")\n",
    "    nib.save(nifti_label_img, label_output_path)\n",
    "    print(f\"Guardadas etiquetas en {label_output_path}\")\n",
    "    \n",
    "    # Guardar segmentación semántica\n",
    "    nifti_seg_img = nib.Nifti1Image(segmentation_np, affine)\n",
    "    seg_output_path = os.path.join(output_dir, f\"segmentation_case_{idx}.nii.gz\")\n",
    "    nib.save(nifti_seg_img, seg_output_path)\n",
    "    print(f\"Guardada segmentación en {seg_output_path}\")\n",
    "\n",
    "# Curva ROC Promedio\n",
    "plt.figure(figsize=(8, 6))\n",
    "for cls in range(3):\n",
    "    # Interpolar FPR y TPR a una base común\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    tprs = []\n",
    "    for fpr, tpr in zip(all_fpr[cls], all_tpr[cls]):\n",
    "        tpr_interp = np.interp(mean_fpr, fpr, tpr)\n",
    "        tpr_interp[0] = 0.0  # Asegurar que comienza en 0\n",
    "        tprs.append(tpr_interp)\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0  # Asegurar que termina en 1\n",
    "    mean_auc = np.nanmean(all_auc[cls])\n",
    "    \n",
    "    plt.plot(mean_fpr, mean_tpr, color=colors[cls], label=f'{class_names[cls]} (AUC = {mean_auc:.4f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Tasa de Falsos Positivos (FPR)')\n",
    "plt.ylabel('Tasa de Verdaderos Positivos (TPR)')\n",
    "plt.title('Curva ROC Promedio')\n",
    "plt.legend(loc=\"lower right\")\n",
    "roc_avg_path = os.path.join(output_dir, \"roc_curve_average.png\")\n",
    "plt.savefig(roc_avg_path)\n",
    "plt.close()\n",
    "print(f\"Guardada curva ROC promedio en {roc_avg_path}\")\n",
    "\n",
    "# # Calcular promedios y desviaciones estándar\n",
    "# class_names = [\"Fondo\", \"Vasogénico\", \"Infiltrado\"]\n",
    "# for cls in range(3):\n",
    "#     dice_mean = np.mean(all_dice[cls])\n",
    "#     dice_std = np.std(all_dice[cls])\n",
    "#     sens_mean = np.mean(all_sensitivity[cls])\n",
    "#     sens_std = np.std(all_sensitivity[cls])\n",
    "#     prec_mean = np.mean(all_precision[cls])\n",
    "#     prec_std = np.std(all_precision[cls])\n",
    "    \n",
    "#     print(f\"\\nClase {cls} ({class_names[cls]}):\")\n",
    "#     print(f\"  Dice: {dice_mean:.4f} ± {dice_std:.4f}\")\n",
    "#     print(f\"  Sensibilidad: {sens_mean:.4f} ± {sens_std:.4f}\")\n",
    "#     print(f\"  Precisión: {prec_mean:.4f} ± {prec_std:.4f}\")\n",
    "\n",
    "# Calcular promedios y desviaciones estándar\n",
    "class_names = [\"Fondo\", \"Vasogénico\", \"Infiltrado\"]\n",
    "for cls in range(3):\n",
    "    dice_mean = np.nanmean(all_dice[cls])\n",
    "    dice_std = np.nanstd(all_dice[cls])\n",
    "    sens_mean = np.nanmean(all_sensitivity[cls])\n",
    "    sens_std = np.nanstd(all_sensitivity[cls])\n",
    "    prec_mean = np.nanmean(all_precision[cls])\n",
    "    prec_std = np.nanstd(all_precision[cls])\n",
    "    auc_mean = np.nanmean(all_auc[cls])\n",
    "    auc_std = np.nanstd(all_auc[cls])\n",
    "    f1_mean = np.nanmean(all_f1[cls])\n",
    "    f1_std = np.nanstd(all_f1[cls])\n",
    "    \n",
    "    print(f\"\\nClase {cls} ({class_names[cls]}):\")\n",
    "    print(f\"  Dice: {dice_mean:.4f} ± {dice_std:.4f}\")\n",
    "    print(f\"  Sensibilidad: {sens_mean:.4f} ± {sens_std:.4f}\")\n",
    "    print(f\"  Precisión: {prec_mean:.4f} ± {prec_std:.4f}\")\n",
    "    print(f\"  AUC-ROC: {auc_mean:.4f} ± {auc_std:.4f}\")\n",
    "    print(f\"  F1 Score: {f1_mean:.4f} ± {f1_std:.4f}\")\n",
    "\n",
    "# Accuracy global\n",
    "accuracy_mean = np.nanmean(all_accuracy)\n",
    "accuracy_std = np.nanstd(all_accuracy)\n",
    "print(f\"\\nAccuracy Global: {accuracy_mean:.4f} ± {accuracy_std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metricas basdas en regions (supervoxeles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, f1_score\n",
    "from scipy import stats\n",
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import torch\n",
    "\n",
    "# Función para calcular métricas (ya definida previamente)\n",
    "def calculate_metrics(pred, true, prob_maps=None, num_classes=3):\n",
    "    dice_scores = []\n",
    "    sensitivity_scores = []\n",
    "    precision_scores = []\n",
    "    auc_scores = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    accuracy = accuracy_score(true.flatten(), pred.flatten())\n",
    "    \n",
    "    for cls in range(num_classes):\n",
    "        pred_cls = (pred == cls).astype(np.uint8)\n",
    "        true_cls = (true == cls).astype(np.uint8)\n",
    "        \n",
    "        tp = np.sum(pred_cls * true_cls)\n",
    "        fp = np.sum(pred_cls * (1 - true_cls))\n",
    "        fn = np.sum((1 - pred_cls) * true_cls)\n",
    "        \n",
    "        dice = 2 * tp / (2 * tp + fp + fn + 1e-6)\n",
    "        dice_scores.append(dice)\n",
    "        \n",
    "        sensitivity = tp / (tp + fn + 1e-6)\n",
    "        sensitivity_scores.append(sensitivity)\n",
    "        \n",
    "        precision = tp / (tp + fp + 1e-6)\n",
    "        precision_scores.append(precision)\n",
    "        \n",
    "        f1 = f1_score(true_cls.flatten(), pred_cls.flatten(), zero_division=0)\n",
    "        f1_scores.append(f1)\n",
    "        \n",
    "        if prob_maps is not None:\n",
    "            try:\n",
    "                auc = roc_auc_score(true_cls.flatten(), prob_maps[cls].flatten())\n",
    "                auc_scores.append(auc)\n",
    "            except ValueError:\n",
    "                auc_scores.append(np.nan)\n",
    "        else:\n",
    "            auc_scores.append(np.nan)\n",
    "    \n",
    "    return dice_scores, sensitivity_scores, precision_scores, auc_scores, accuracy, f1_scores\n",
    "\n",
    "# Función para dividir en cubos y obtener clases predominantes\n",
    "def get_cube_labels(volume, cube_size, num_classes=3):\n",
    "    dims = volume.shape\n",
    "    assert dims[0] % cube_size == 0, \"El tamaño del cubo debe dividir exactamente el tamaño del volumen\"\n",
    "    num_cubes = dims[0] // cube_size\n",
    "    \n",
    "    cube_labels = np.zeros((num_cubes, num_cubes, num_cubes), dtype=np.uint8)\n",
    "    cube_probs = np.zeros((num_classes, num_cubes, num_cubes, num_cubes))\n",
    "    \n",
    "    for i in range(num_cubes):\n",
    "        for j in range(num_cubes):\n",
    "            for k in range(num_cubes):\n",
    "                cube = volume[i*cube_size:(i+1)*cube_size, \n",
    "                             j*cube_size:(j+1)*cube_size, \n",
    "                             k*cube_size:(k+1)*cube_size]\n",
    "                # Clase predominante (modo)\n",
    "                mode_value = stats.mode(cube.flatten(), keepdims=True)[0][0]\n",
    "                cube_labels[i, j, k] = mode_value\n",
    "                # Proporción de cada clase como \"probabilidad\" suavizada\n",
    "                for cls in range(num_classes):\n",
    "                    cube_probs[cls, i, j, k] = np.mean(cube == cls)\n",
    "    \n",
    "    return cube_labels, cube_probs\n",
    "\n",
    "# Listas para métricas voxel-wise\n",
    "all_dice = {0: [], 1: [], 2: []}\n",
    "all_sensitivity = {0: [], 1: [], 2: []}\n",
    "all_precision = {0: [], 1: [], 2: []}\n",
    "all_auc = {0: [], 1: [], 2: []}\n",
    "all_accuracy = []\n",
    "all_f1 = {0: [], 1: [], 2: []}\n",
    "\n",
    "# Listas para métricas cube-wise\n",
    "all_dice_cube = {0: [], 1: [], 2: []}\n",
    "all_sensitivity_cube = {0: [], 1: [], 2: []}\n",
    "all_precision_cube = {0: [], 1: [], 2: []}\n",
    "all_auc_cube = {0: [], 1: [], 2: []}\n",
    "all_accuracy_cube = []\n",
    "all_f1_cube = {0: [], 1: [], 2: []}\n",
    "\n",
    "# Tamaño del cubo (ajusta según necesites)\n",
    "cube_size = 8  # 128 / 16 = 8 cubos por dimensión\n",
    "\n",
    "# Procesar y combinar\n",
    "for idx, ((embeddings1, labels1), (embeddings2, labels2)) in enumerate(zip(loader_model1, loader_model2)):\n",
    "    # Generar mapas de probabilidad para ambos modelos\n",
    "    prob_maps1 = generate_probability_maps(embeddings1, projection_head1, classifier1, device)  # [3, 128, 128, 128]\n",
    "    prob_maps2 = generate_probability_maps(embeddings2, projection_head2, classifier2, device)  # [3, 128, 128, 128]\n",
    "    \n",
    "    # Combinar mapas\n",
    "    combined_prob_maps = torch.zeros_like(prob_maps1)\n",
    "    combined_prob_maps[0] = prob_maps1[0]\n",
    "    combined_prob_maps[1] = torch.max(prob_maps1[1], prob_maps2[1])\n",
    "    combined_prob_maps[2] = prob_maps2[2]\n",
    "    \n",
    "    # Convertir a numpy\n",
    "    prob_maps_np = combined_prob_maps.cpu().numpy()  # [3, 128, 128, 128]\n",
    "    prob_maps_np_nifti = np.transpose(prob_maps_np, (1, 2, 3, 0))  # [128, 128, 128, 3]\n",
    "    \n",
    "    # Generar segmentación semántica\n",
    "    segmentation = np.argmax(prob_maps_np, axis=0)\n",
    "    # Colocar nuevo umbral para infiltracion\n",
    "    # class_1_mask = prob_maps_np[1] > 0.4\n",
    "    # segmentation[class_1_mask] = 1\n",
    "    segmentation_np = segmentation.astype(np.uint8)\n",
    "    \n",
    "    # Etiquetas\n",
    "    labels = labels2.squeeze(0)\n",
    "    labels_np = labels.cpu().numpy().astype(np.uint8)\n",
    "    \n",
    "    # Calcular métricas voxel-wise\n",
    "    dice, sensitivity, precision, auc, accuracy, f1 = calculate_metrics(segmentation_np, labels_np, prob_maps=prob_maps_np)\n",
    "    \n",
    "    # Almacenar métricas voxel-wise\n",
    "    for cls in range(3):\n",
    "        all_dice[cls].append(dice[cls])\n",
    "        all_sensitivity[cls].append(sensitivity[cls])\n",
    "        all_precision[cls].append(precision[cls])\n",
    "        all_auc[cls].append(auc[cls])\n",
    "        all_f1[cls].append(f1[cls])\n",
    "    all_accuracy.append(accuracy)\n",
    "    \n",
    "    # Evaluación basada en cubos\n",
    "    pred_cube_labels, pred_cube_probs = get_cube_labels(segmentation_np, cube_size)\n",
    "    true_cube_labels, true_cube_probs = get_cube_labels(labels_np, cube_size)\n",
    "    \n",
    "    # Calcular métricas cube-wise\n",
    "    dice_cube, sensitivity_cube, precision_cube, auc_cube, accuracy_cube, f1_cube = calculate_metrics(\n",
    "        pred_cube_labels, true_cube_labels, prob_maps=pred_cube_probs\n",
    "    )\n",
    "    \n",
    "    # Almacenar métricas cube-wise\n",
    "    for cls in range(3):\n",
    "        all_dice_cube[cls].append(dice_cube[cls])\n",
    "        all_sensitivity_cube[cls].append(sensitivity_cube[cls])\n",
    "        all_precision_cube[cls].append(precision_cube[cls])\n",
    "        all_auc_cube[cls].append(auc_cube[cls])\n",
    "        all_f1_cube[cls].append(f1_cube[cls])\n",
    "    all_accuracy_cube.append(accuracy_cube)\n",
    "    \n",
    "    # Mapa de coincidencias/discrepancias\n",
    "    match_map = (pred_cube_labels == true_cube_labels).astype(np.uint8)\n",
    "    mismatch_map = (pred_cube_labels != true_cube_labels).astype(np.uint8)\n",
    "    \n",
    "    # Guardar mapas de cubos y coincidencias\n",
    "    affine = np.eye(4) * cube_size  # Ajustar el affine para reflejar el tamaño del cubo\n",
    "    nib.save(nib.Nifti1Image(pred_cube_labels, affine), os.path.join(output_dir, f\"pred_cube_labels_case_{idx}.nii.gz\"))\n",
    "    nib.save(nib.Nifti1Image(true_cube_labels, affine), os.path.join(output_dir, f\"true_cube_labels_case_{idx}.nii.gz\"))\n",
    "    nib.save(nib.Nifti1Image(match_map, affine), os.path.join(output_dir, f\"match_map_case_{idx}.nii.gz\"))\n",
    "    nib.save(nib.Nifti1Image(mismatch_map, affine), os.path.join(output_dir, f\"mismatch_map_case_{idx}.nii.gz\"))\n",
    "    \n",
    "    # Análisis espacial (centro de masa de la clase Infiltrado)\n",
    "    infiltrado_pred = (pred_cube_labels == 1).astype(np.uint8)\n",
    "    infiltrado_true = (true_cube_labels == 1).astype(np.uint8)\n",
    "    if np.sum(infiltrado_pred) > 0:\n",
    "        pred_center = np.mean(np.where(infiltrado_pred), axis=1)\n",
    "    else:\n",
    "        pred_center = np.array([np.nan, np.nan, np.nan])\n",
    "    if np.sum(infiltrado_true) > 0:\n",
    "        true_center = np.mean(np.where(infiltrado_true), axis=1)\n",
    "    else:\n",
    "        true_center = np.array([np.nan, np.nan, np.nan])\n",
    "    \n",
    "    print(f\"Caso {idx} - Voxel-wise:\")\n",
    "    print(f\"  Dice: {dice}, Sensitivity: {sensitivity}, Precision: {precision}, AUC-ROC: {auc}, \"\n",
    "          f\"Accuracy: {accuracy:.4f}, F1 Score: {f1}\")\n",
    "    print(f\"Caso {idx} - Cube-wise (tamaño {cube_size}):\")\n",
    "    print(f\"  Dice: {dice_cube}, Sensitivity: {sensitivity_cube}, Precision: {precision_cube}, \"\n",
    "          f\"AUC-ROC: {auc_cube}, Accuracy: {accuracy_cube:.4f}, F1 Score: {f1_cube}\")\n",
    "    print(f\"  Centro de masa Infiltrado (Pred): {pred_center}\")\n",
    "    print(f\"  Centro de masa Infiltrado (True): {true_center}\")\n",
    "    \n",
    "    # Guardar mapas de probabilidad y segmentaciones voxel-wise\n",
    "    nib.save(nib.Nifti1Image(prob_maps_np_nifti, np.eye(4)), os.path.join(output_dir, f\"probability_maps_case_{idx}.nii.gz\"))\n",
    "    nib.save(nib.Nifti1Image(labels_np, np.eye(4)), os.path.join(output_dir, f\"labels_case_{idx}.nii.gz\"))\n",
    "    nib.save(nib.Nifti1Image(segmentation_np, np.eye(4)), os.path.join(output_dir, f\"segmentation_case_{idx}.nii.gz\"))\n",
    "\n",
    "# Calcular promedios y desviaciones estándar (voxel-wise)\n",
    "class_names = [\"Fondo\", \"Vasogénico\", \"Infiltrado\"]\n",
    "print(\"\\nResultados Voxel-wise:\")\n",
    "for cls in range(3):\n",
    "    dice_mean = np.nanmean(all_dice[cls])\n",
    "    dice_std = np.nanstd(all_dice[cls])\n",
    "    sens_mean = np.nanmean(all_sensitivity[cls])\n",
    "    sens_std = np.nanstd(all_sensitivity[cls])\n",
    "    prec_mean = np.nanmean(all_precision[cls])\n",
    "    prec_std = np.nanstd(all_precision[cls])\n",
    "    auc_mean = np.nanmean(all_auc[cls])\n",
    "    auc_std = np.nanstd(all_auc[cls])\n",
    "    f1_mean = np.nanmean(all_f1[cls])\n",
    "    f1_std = np.nanstd(all_f1[cls])\n",
    "    \n",
    "    print(f\"\\nClase {cls} ({class_names[cls]}):\")\n",
    "    print(f\"  Dice: {dice_mean:.4f} ± {dice_std:.4f}\")\n",
    "    print(f\"  Sensibilidad: {sens_mean:.4f} ± {sens_std:.4f}\")\n",
    "    print(f\"  Precisión: {prec_mean:.4f} ± {prec_std:.4f}\")\n",
    "    print(f\"  AUC-ROC: {auc_mean:.4f} ± {auc_std:.4f}\")\n",
    "    print(f\"  F1 Score: {f1_mean:.4f} ± {f1_std:.4f}\")\n",
    "\n",
    "accuracy_mean = np.nanmean(all_accuracy)\n",
    "accuracy_std = np.nanstd(all_accuracy)\n",
    "print(f\"\\nAccuracy Global: {accuracy_mean:.4f} ± {accuracy_std:.4f}\")\n",
    "\n",
    "# Calcular promedios y desviaciones estándar (cube-wise)\n",
    "print(\"\\nResultados Cube-wise:\")\n",
    "for cls in range(3):\n",
    "    dice_mean = np.nanmean(all_dice_cube[cls])\n",
    "    dice_std = np.nanstd(all_dice_cube[cls])\n",
    "    sens_mean = np.nanmean(all_sensitivity_cube[cls])\n",
    "    sens_std = np.nanstd(all_sensitivity_cube[cls])\n",
    "    prec_mean = np.nanmean(all_precision_cube[cls])\n",
    "    prec_std = np.nanstd(all_precision_cube[cls])\n",
    "    auc_mean = np.nanmean(all_auc_cube[cls])\n",
    "    auc_std = np.nanstd(all_auc_cube[cls])\n",
    "    f1_mean = np.nanmean(all_f1_cube[cls])\n",
    "    f1_std = np.nanstd(all_f1_cube[cls])\n",
    "    \n",
    "    print(f\"\\nClase {cls} ({class_names[cls]}):\")\n",
    "    print(f\"  Dice: {dice_mean:.4f} ± {dice_std:.4f}\")\n",
    "    print(f\"  Sensibilidad: {sens_mean:.4f} ± {sens_std:.4f}\")\n",
    "    print(f\"  Precisión: {prec_mean:.4f} ± {prec_std:.4f}\")\n",
    "    print(f\"  AUC-ROC: {auc_mean:.4f} ± {auc_std:.4f}\")\n",
    "    print(f\"  F1 Score: {f1_mean:.4f} ± {f1_std:.4f}\")\n",
    "\n",
    "accuracy_mean_cube = np.nanmean(all_accuracy_cube)\n",
    "accuracy_std_cube = np.nanstd(all_accuracy_cube)\n",
    "print(f\"\\nAccuracy Global Cube-wise: {accuracy_mean_cube:.4f} ± {accuracy_std_cube:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metricas basadas en regiones centrada en tumor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caso 0 - Voxel-wise:\n",
      "  Dice: [0.9968297778503837, 0.5601522842568509, 0.6596068471084519], Sensitivity: [0.9937781582351525, 0.5815087081236923, 0.7322381451536237], Precision: [0.9999001965623705, 0.540308957804972, 0.6000840239735691], AUC-ROC: [0.9997442857524647, 0.9908721550503753, 0.9926824584823732], Accuracy: 0.9810, F1 Score: [0.9968297778506316, 0.5601522842639594, 0.6596068471153744]\n",
      "Caso 0 - Cube-wise (Grilla 5x5x5):\n",
      "  Dice: [0.9350649289930849, 0.5217391190926278, 0.5999999880000003], Sensitivity: [0.8780487697798931, 0.5454545206611581, 0.7142856802721105], Precision: [0.9999999861111113, 0.4999999791666675, 0.5172413614744358], AUC-ROC: [0.9937606352807713, 0.8239187996469549, 0.8413461538461539], Accuracy: 0.7920, F1 Score: [0.9350649350649352, 0.5217391304347826, 0.6000000000000001]\n",
      "  Centro de masa Infiltrado (Pred): [1.58333333 2.75       2.125     ]\n",
      "  Centro de masa Infiltrado (True): [2.40909091 2.         1.54545455]\n",
      "  Distancia entre centros: 1.26\n",
      "Caso 1 - Voxel-wise:\n",
      "  Dice: [0.9959712568975575, 0.704246355713222, 0.6889781347489696], Sensitivity: [0.9922334783311854, 0.9321950255692661, 0.8196025139791234], Precision: [0.9997373026346886, 0.5658738241827408, 0.5942667118085332], AUC-ROC: [0.9982288768831301, 0.9987986678355506, 0.9947559103374306], Accuracy: 0.9900, F1 Score: [0.9959712568977996, 0.7042463557394772, 0.6889781347612673]\n",
      "Caso 1 - Cube-wise (Grilla 5x5x5):\n",
      "  Dice: [0.9763033129085151, 0.7499999531250029, 0.6956521436672981], Sensitivity: [0.9537036948731139, 0.9999998333333611, 0.7272726611570308], Precision: [0.9999999902912623, 0.599999940000006, 0.6666666111111158], AUC-ROC: [0.9929193899782134, 1.0, 0.9696969696969696], Accuracy: 0.9360, F1 Score: [0.976303317535545, 0.7499999999999999, 0.6956521739130435]\n",
      "  Centro de masa Infiltrado (Pred): [2.6 2.7 1.8]\n",
      "  Centro de masa Infiltrado (True): [3.         2.83333333 2.        ]\n",
      "  Distancia entre centros: 0.47\n",
      "Caso 2 - Voxel-wise:\n",
      "  Dice: [0.995155590256476, 0.6609936305676349, 0.762320076063902], Sensitivity: [0.9912902923748913, 0.9415237219427207, 0.6875443576937196], Precision: [0.999051149732698, 0.5092584110798741, 0.8553454491838307], AUC-ROC: [0.9992538722066036, 0.9960538811731321, 0.9950507396812097], Accuracy: 0.9756, F1 Score: [0.9951555902567316, 0.6609936305732484, 0.7623200760680678]\n",
      "Caso 2 - Cube-wise (Grilla 5x5x5):\n",
      "  Dice: [0.9951690773180238, 0.6999999650000017, 0.7826086616257103], Sensitivity: [0.9903846058616865, 0.9999998571428775, 0.6428570969387788], Precision: [0.9999999902912623, 0.5384614970414233, 0.9999998888889013], AUC-ROC: [0.9995421245421247, 0.983050847457627, 0.9182754182754184], Accuracy: 0.9520, F1 Score: [0.9951690821256038, 0.7000000000000001, 0.782608695652174]\n",
      "  Centro de masa Infiltrado (Pred): [2.46153846 1.69230769 2.23076923]\n",
      "  Centro de masa Infiltrado (True): [2.57142857 1.85714286 1.57142857]\n",
      "  Distancia entre centros: 0.69\n",
      "Caso 3 - Voxel-wise:\n",
      "  Dice: [0.9946048730892474, 0.6172483414611156, 0.39024520539584795], Sensitivity: [0.9894348763371535, 0.6179613051374586, 0.9243325318329327], Precision: [0.9998291821849637, 0.6165370209425904, 0.24733367190630323], AUC-ROC: [0.9982360359904578, 0.9980275841103045, 0.9913611417658387], Accuracy: 0.9880, F1 Score: [0.9946048730894874, 0.6172483415056245, 0.39024520540627167]\n",
      "Caso 3 - Cube-wise (Grilla 5x5x5):\n",
      "  Dice: [0.9636363592561984, 0.599999940000006, 0.4999999750000012], Sensitivity: [0.938053089043778, 0.4285713673469475, 0.99999980000004], Precision: [0.9906541963490262, 0.9999996666667778, 0.3333333111111126], AUC-ROC: [0.9918879056047198, 0.9648910411622277, 0.9900000000000001], Accuracy: 0.9120, F1 Score: [0.9636363636363636, 0.6, 0.5]\n",
      "  Centro de masa Infiltrado (Pred): [2.33333333 3.         3.33333333]\n",
      "  Centro de masa Infiltrado (True): [1.85714286 2.71428571 3.        ]\n",
      "  Distancia entre centros: 0.65\n",
      "Caso 4 - Voxel-wise:\n",
      "  Dice: [0.9963739207298858, 0.7092713269967104, 0.5749213439831179], Sensitivity: [0.9927769299839582, 0.8523374465559838, 0.7137575287307676], Precision: [0.9999970712100977, 0.6073300642286384, 0.4813012706899911], AUC-ROC: [0.9996016155347753, 0.9980592234123471, 0.9952250280934484], Accuracy: 0.9894, F1 Score: [0.9963739207301281, 0.709271327014218, 0.5749213439969258]\n",
      "Caso 4 - Cube-wise (Grilla 5x5x5):\n",
      "  Dice: [0.9644670001803706, 0.7333333088888897, 0.6956521436672981], Sensitivity: [0.9313725398885045, 0.9166665902777842, 0.7272726611570308], Precision: [0.9999999894736844, 0.6111110771604957, 0.6666666111111158], AUC-ROC: [0.9970161977834612, 0.9823008849557522, 0.950956937799043], Accuracy: 0.9120, F1 Score: [0.9644670050761421, 0.7333333333333334, 0.6956521739130435]\n",
      "  Centro de masa Infiltrado (Pred): [1.83333333 2.05555556 1.61111111]\n",
      "  Centro de masa Infiltrado (True): [2.         2.08333333 1.5       ]\n",
      "  Distancia entre centros: 0.20\n",
      "Caso 5 - Voxel-wise:\n",
      "  Dice: [0.9967239082691858, 0.7870451708263554, 0.6235433461845501], Sensitivity: [0.9934794673263095, 0.842597633993897, 0.7284738650748349], Precision: [0.9999896096128889, 0.7383647798582007, 0.5450355675601966], AUC-ROC: [0.9998858128010235, 0.997705949643649, 0.9948220587097338], Accuracy: 0.9877, F1 Score: [0.9967239082694315, 0.7870451708354524, 0.6235433461964621]\n",
      "Caso 5 - Cube-wise (Grilla 5x5x5):\n",
      "  Dice: [0.9529411708650519, 0.8852458871271166, 0.7368420664819965], Sensitivity: [0.9204545349948349, 0.9999999629629643, 0.699999930000007], Precision: [0.9878048660023797, 0.7941176237024229, 0.7777776913580343], AUC-ROC: [0.9989250614250614, 0.9935752078609221, 0.9634782608695651], Accuracy: 0.9200, F1 Score: [0.9529411764705882, 0.8852459016393442, 0.7368421052631577]\n",
      "  Centro de masa Infiltrado (Pred): [2.47058824 2.14705882 2.05882353]\n",
      "  Centro de masa Infiltrado (True): [2.37037037 2.07407407 1.96296296]\n",
      "  Distancia entre centros: 0.16\n",
      "\n",
      "Resultados Voxel-wise:\n",
      "\n",
      "Clase 0 (Fondo):\n",
      "  Dice: 0.9959 ± 0.0008\n",
      "  Sensibilidad: 0.9922 ± 0.0015\n",
      "  Precisión: 0.9998 ± 0.0003\n",
      "  AUC-ROC: 0.9992 ± 0.0007\n",
      "  F1 Score: 0.9959 ± 0.0008\n",
      "\n",
      "Clase 1 (Infiltrado):\n",
      "  Dice: 0.6732 ± 0.0722\n",
      "  Sensibilidad: 0.7947 ± 0.1430\n",
      "  Precisión: 0.5963 ± 0.0734\n",
      "  AUC-ROC: 0.9966 ± 0.0027\n",
      "  F1 Score: 0.6732 ± 0.0722\n",
      "\n",
      "Clase 2 (Vasogénico):\n",
      "  Dice: 0.6166 ± 0.1164\n",
      "  Sensibilidad: 0.7677 ± 0.0810\n",
      "  Precisión: 0.5539 ± 0.1798\n",
      "  AUC-ROC: 0.9940 ± 0.0014\n",
      "  F1 Score: 0.6166 ± 0.1164\n",
      "\n",
      "Accuracy Global: 0.9853 ± 0.0052\n",
      "\n",
      "Resultados Cube-wise (Grilla 5x5x5):\n",
      "\n",
      "Clase 0 (Fondo):\n",
      "  Dice: 0.9646 ± 0.0186\n",
      "  Sensibilidad: 0.9353 ± 0.0339\n",
      "  Precisión: 0.9964 ± 0.0051\n",
      "  AUC-ROC: 0.9957 ± 0.0030\n",
      "  F1 Score: 0.9646 ± 0.0186\n",
      "\n",
      "Clase 1 (Infiltrado):\n",
      "  Dice: 0.6984 ± 0.1152\n",
      "  Sensibilidad: 0.8151 ± 0.2363\n",
      "  Precisión: 0.6739 ± 0.1726\n",
      "  AUC-ROC: 0.9580 ± 0.0609\n",
      "  F1 Score: 0.6984 ± 0.1152\n",
      "\n",
      "Clase 2 (Vasogénico):\n",
      "  Dice: 0.6685 ± 0.0933\n",
      "  Sensibilidad: 0.7519 ± 0.1146\n",
      "  Precisión: 0.6603 ± 0.2068\n",
      "  AUC-ROC: 0.9390 ± 0.0487\n",
      "  F1 Score: 0.6685 ± 0.0933\n",
      "\n",
      "Accuracy Global Cube-wise: 0.9040 ± 0.0521\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, f1_score\n",
    "from scipy import stats\n",
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import torch\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Función para calcular métricas (para todas las clases o un subconjunto)\n",
    "def calculate_metrics(pred, true, prob_maps=None, num_classes=3, class_offset=0):\n",
    "    dice_scores = []\n",
    "    sensitivity_scores = []\n",
    "    precision_scores = []\n",
    "    auc_scores = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    accuracy = accuracy_score(true.flatten(), pred.flatten())\n",
    "    \n",
    "    for cls in range(class_offset, class_offset + num_classes):\n",
    "        pred_cls = (pred == cls).astype(np.uint8)\n",
    "        true_cls = (true == cls).astype(np.uint8)\n",
    "        \n",
    "        tp = np.sum(pred_cls * true_cls)\n",
    "        fp = np.sum(pred_cls * (1 - true_cls))\n",
    "        fn = np.sum((1 - pred_cls) * true_cls)\n",
    "        \n",
    "        dice = 2 * tp / (2 * tp + fp + fn + 1e-6)\n",
    "        dice_scores.append(dice)\n",
    "        \n",
    "        sensitivity = tp / (tp + fn + 1e-6)\n",
    "        sensitivity_scores.append(sensitivity)\n",
    "        \n",
    "        precision = tp / (tp + fp + 1e-6)\n",
    "        precision_scores.append(precision)\n",
    "        \n",
    "        f1 = f1_score(true_cls.flatten(), pred_cls.flatten(), zero_division=0)\n",
    "        f1_scores.append(f1)\n",
    "        \n",
    "        if prob_maps is not None:\n",
    "            try:\n",
    "                auc = roc_auc_score(true_cls.flatten(), prob_maps[cls - class_offset].flatten())\n",
    "                auc_scores.append(auc)\n",
    "            except ValueError:\n",
    "                auc_scores.append(np.nan)\n",
    "        else:\n",
    "            auc_scores.append(np.nan)\n",
    "    \n",
    "    return dice_scores, sensitivity_scores, precision_scores, auc_scores, accuracy, f1_scores\n",
    "\n",
    "# Función para obtener el cuadro delimitador y centro del tumor\n",
    "def get_tumor_bbox_and_center(labels_np):\n",
    "    tumor_mask = (labels_np > 0).astype(np.uint8)  # Clases 1 (Infiltrado) y 2 (Vasogénico)\n",
    "    if np.sum(tumor_mask) == 0:\n",
    "        return None, None\n",
    "    indices = np.where(tumor_mask)\n",
    "    bbox = {\n",
    "        'x_min': np.min(indices[0]), 'x_max': np.max(indices[0]),\n",
    "        'y_min': np.min(indices[1]), 'y_max': np.max(indices[1]),\n",
    "        'z_min': np.min(indices[2]), 'z_max': np.max(indices[2])\n",
    "    }\n",
    "    center = np.mean(indices, axis=1)\n",
    "    return bbox, center\n",
    "\n",
    "# Función para dividir la región del tumor en cubos\n",
    "def get_tumor_cube_labels(segmentation_np, labels_np, bbox, center, grid_size=5):\n",
    "    # Dimensiones del cuadro delimitador\n",
    "    dims = [bbox['x_max'] - bbox['x_min'] + 1, \n",
    "            bbox['y_max'] - bbox['y_min'] + 1, \n",
    "            bbox['z_max'] - bbox['z_min'] + 1]\n",
    "    \n",
    "    # Tamaño del cubo (dinámico)\n",
    "    cube_size = [max(1, d // grid_size) for d in dims]\n",
    "    \n",
    "    # Ajustar el cuadro delimitador para centrarlo y cubrir la grilla\n",
    "    half_grid = np.array([grid_size * cs / 2 for cs in cube_size])\n",
    "    bbox_min = np.floor(center - half_grid).astype(int)\n",
    "    bbox_max = np.ceil(center + half_grid).astype(int)\n",
    "    \n",
    "    # Asegurar que no exceda los límites del volumen\n",
    "    bbox_min = np.maximum(bbox_min, 0)\n",
    "    bbox_max = np.minimum(bbox_max, np.array(segmentation_np.shape))\n",
    "    \n",
    "    # Extraer la región del tumor\n",
    "    tumor_pred = segmentation_np[bbox_min[0]:bbox_max[0], bbox_min[1]:bbox_max[1], bbox_min[2]:bbox_max[2]]\n",
    "    tumor_true = labels_np[bbox_min[0]:bbox_max[0], bbox_min[1]:bbox_max[1], bbox_min[2]:bbox_max[2]]\n",
    "    \n",
    "    # Ajustar para que coincida con la grilla\n",
    "    cube_labels_pred = np.zeros((grid_size, grid_size, grid_size), dtype=np.uint8)\n",
    "    cube_labels_true = np.zeros((grid_size, grid_size, grid_size), dtype=np.uint8)\n",
    "    cube_probs_pred = np.zeros((3, grid_size, grid_size, grid_size))  # Clases 0, 1, 2\n",
    "    \n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            for k in range(grid_size):\n",
    "                x_start = i * cube_size[0]\n",
    "                y_start = j * cube_size[1]\n",
    "                z_start = k * cube_size[2]\n",
    "                x_end = min(x_start + cube_size[0], tumor_pred.shape[0])\n",
    "                y_end = min(y_start + cube_size[1], tumor_pred.shape[1])\n",
    "                z_end = min(z_start + cube_size[2], tumor_pred.shape[2])\n",
    "                \n",
    "                if x_end <= x_start or y_end <= y_start or z_end <= z_start:\n",
    "                    continue\n",
    "                \n",
    "                cube_pred = tumor_pred[x_start:x_end, y_start:y_end, z_start:z_end]\n",
    "                cube_true = tumor_true[x_start:x_end, y_start:y_end, z_start:z_end]\n",
    "                \n",
    "                # Clase predominante (incluye clase 0)\n",
    "                cube_labels_pred[i, j, k] = stats.mode(cube_pred.flatten(), keepdims=True)[0][0]\n",
    "                cube_labels_true[i, j, k] = stats.mode(cube_true.flatten(), keepdims=True)[0][0]\n",
    "                \n",
    "                # Proporciones para clases 0, 1, 2\n",
    "                cube_probs_pred[0, i, j, k] = np.mean(cube_pred == 0)\n",
    "                cube_probs_pred[1, i, j, k] = np.mean(cube_pred == 1)\n",
    "                cube_probs_pred[2, i, j, k] = np.mean(cube_pred == 2)\n",
    "    \n",
    "    return cube_labels_pred, cube_labels_true, cube_probs_pred, bbox_min, bbox_max\n",
    "\n",
    "# Listas para métricas voxel-wise (todas las clases)\n",
    "all_dice = {0: [], 1: [], 2: []}\n",
    "all_sensitivity = {0: [], 1: [], 2: []}\n",
    "all_precision = {0: [], 1: [], 2: []}\n",
    "all_auc = {0: [], 1: [], 2: []}\n",
    "all_accuracy = []\n",
    "all_f1 = {0: [], 1: [], 2: []}\n",
    "\n",
    "# Listas para métricas cube-wise (todas las clases)\n",
    "all_dice_cube = {0: [], 1: [], 2: []}\n",
    "all_sensitivity_cube = {0: [], 1: [], 2: []}\n",
    "all_precision_cube = {0: [], 1: [], 2: []}\n",
    "all_auc_cube = {0: [], 1: [], 2: []}\n",
    "all_accuracy_cube = []\n",
    "all_f1_cube = {0: [], 1: [], 2: []}\n",
    "\n",
    "# Configuración de la grilla (5x5x5 o 3x3x3)\n",
    "grid_size = 5  # Cambia a 3 para una grilla 3x3x3\n",
    "\n",
    "# Procesar y combinar\n",
    "for idx, ((embeddings1, labels1), (embeddings2, labels2)) in enumerate(zip(loader_model1, loader_model2)):\n",
    "    # Generar mapas de probabilidad\n",
    "    prob_maps1 = generate_probability_maps(embeddings1, projection_head1, classifier1, device)\n",
    "    prob_maps2 = generate_probability_maps(embeddings2, projection_head2, classifier2, device)\n",
    "    \n",
    "    # Combinar mapas\n",
    "    combined_prob_maps = torch.zeros_like(prob_maps1)\n",
    "    combined_prob_maps[0] = prob_maps1[0]  # Fondo\n",
    "    combined_prob_maps[1] = torch.max(prob_maps1[1], prob_maps2[1])  # Infiltrado\n",
    "    combined_prob_maps[2] = prob_maps2[2]  # Vasogénico\n",
    "    \n",
    "    # Convertir a numpy\n",
    "    prob_maps_np = combined_prob_maps.cpu().numpy()\n",
    "    prob_maps_np_nifti = np.transpose(prob_maps_np, (1, 2, 3, 0))\n",
    "    \n",
    "    # Generar segmentación\n",
    "    segmentation = np.argmax(prob_maps_np, axis=0)\n",
    "    class_1_mask = prob_maps_np[1] > 0.4  # Umbral para Infiltrado\n",
    "    segmentation[class_1_mask] = 1\n",
    "    segmentation_np = segmentation.astype(np.uint8)\n",
    "    \n",
    "    # Etiquetas\n",
    "    labels = labels2.squeeze(0)\n",
    "    labels_np = labels.cpu().numpy().astype(np.uint8)\n",
    "    \n",
    "    # Calcular métricas voxel-wise\n",
    "    dice, sensitivity, precision, auc, accuracy, f1 = calculate_metrics(\n",
    "        segmentation_np, labels_np, prob_maps=prob_maps_np, num_classes=3, class_offset=0\n",
    "    )\n",
    "    \n",
    "    # Almacenar métricas voxel-wise\n",
    "    for cls in range(3):\n",
    "        all_dice[cls].append(dice[cls])\n",
    "        all_sensitivity[cls].append(sensitivity[cls])\n",
    "        all_precision[cls].append(precision[cls])\n",
    "        all_auc[cls].append(auc[cls])\n",
    "        all_f1[cls].append(f1[cls])\n",
    "    all_accuracy.append(accuracy)\n",
    "    \n",
    "    # Obtener cuadro delimitador y centro del tumor\n",
    "    bbox, center = get_tumor_bbox_and_center(labels_np)\n",
    "    if bbox is None:\n",
    "        print(f\"Caso {idx}: No se detectó tumor en el ground truth. Saltando evaluación cube-wise.\")\n",
    "        continue\n",
    "    \n",
    "    # Evaluación basada en cubos\n",
    "    cube_labels_pred, cube_labels_true, cube_probs_pred, bbox_min, bbox_max = get_tumor_cube_labels(\n",
    "        segmentation_np, labels_np, bbox, center, grid_size=grid_size\n",
    "    )\n",
    "    \n",
    "    # Calcular métricas cube-wise (clases 0, 1, 2)\n",
    "    dice_cube, sensitivity_cube, precision_cube, auc_cube, accuracy_cube, f1_cube = calculate_metrics(\n",
    "        cube_labels_pred, cube_labels_true, prob_maps=cube_probs_pred, num_classes=3, class_offset=0\n",
    "    )\n",
    "    \n",
    "    # Almacenar métricas cube-wise\n",
    "    for cls in range(3):\n",
    "        all_dice_cube[cls].append(dice_cube[cls])\n",
    "        all_sensitivity_cube[cls].append(sensitivity_cube[cls])\n",
    "        all_precision_cube[cls].append(precision_cube[cls])\n",
    "        all_auc_cube[cls].append(auc_cube[cls])\n",
    "        all_f1_cube[cls].append(f1_cube[cls])\n",
    "    all_accuracy_cube.append(accuracy_cube)\n",
    "    \n",
    "    # Mapa de coincidencias/discrepancias\n",
    "    match_map = (cube_labels_pred == cube_labels_true).astype(np.uint8)\n",
    "    mismatch_map = (cube_labels_pred != cube_labels_true).astype(np.uint8)\n",
    "    \n",
    "    # Guardar mapas de cubos\n",
    "    affine = np.eye(4)\n",
    "    affine[:3, 3] = bbox_min  # Ajustar origen al cuadro delimitador\n",
    "    nib.save(nib.Nifti1Image(cube_labels_pred, affine), \n",
    "             os.path.join(output_dir, f\"pred_cube_labels_case_{idx}.nii.gz\"))\n",
    "    nib.save(nib.Nifti1Image(cube_labels_true, affine), \n",
    "             os.path.join(output_dir, f\"true_cube_labels_case_{idx}.nii.gz\"))\n",
    "    nib.save(nib.Nifti1Image(match_map, affine), \n",
    "             os.path.join(output_dir, f\"match_map_case_{idx}.nii.gz\"))\n",
    "    nib.save(nib.Nifti1Image(mismatch_map, affine), \n",
    "             os.path.join(output_dir, f\"mismatch_map_case_{idx}.nii.gz\"))\n",
    "    \n",
    "    # Análisis espacial (centro de masa de Infiltrado)\n",
    "    infiltrado_pred = (cube_labels_pred == 1).astype(np.uint8)  # Clase 1: Infiltrado\n",
    "    infiltrado_true = (cube_labels_true == 1).astype(np.uint8)\n",
    "    pred_center = np.mean(np.where(infiltrado_pred), axis=1) if np.sum(infiltrado_pred) > 0 else np.array([np.nan] * 3)\n",
    "    true_center = np.mean(np.where(infiltrado_true), axis=1) if np.sum(infiltrado_true) > 0 else np.array([np.nan] * 3)\n",
    "    distance = np.linalg.norm(pred_center - true_center) if not np.any(np.isnan(pred_center)) and not np.any(np.isnan(true_center)) else np.nan\n",
    "    \n",
    "    # Visualización 3D\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    infiltrado_pred_pos = np.where(infiltrado_pred)\n",
    "    infiltrado_true_pos = np.where(infiltrado_true)\n",
    "    ax.scatter(infiltrado_pred_pos[0], infiltrado_pred_pos[1], infiltrado_pred_pos[2], \n",
    "               c='red', label='Infiltrado Predicho', alpha=0.5)\n",
    "    ax.scatter(infiltrado_true_pos[0], infiltrado_true_pos[1], infiltrado_true_pos[2], \n",
    "               c='blue', label='Infiltrado Verdadero', alpha=0.5)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.set_title(f'Infiltrado - Caso {idx} (Grilla {grid_size}x{grid_size}x{grid_size})')\n",
    "    ax.legend()\n",
    "    plt.savefig(os.path.join(output_dir, f\"infiltrado_scatter_case_{idx}.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    # Imprimir resultados\n",
    "    print(f\"Caso {idx} - Voxel-wise:\")\n",
    "    print(f\"  Dice: {dice}, Sensitivity: {sensitivity}, Precision: {precision}, AUC-ROC: {auc}, \"\n",
    "          f\"Accuracy: {accuracy:.4f}, F1 Score: {f1}\")\n",
    "    print(f\"Caso {idx} - Cube-wise (Grilla {grid_size}x{grid_size}x{grid_size}):\")\n",
    "    print(f\"  Dice: {dice_cube}, Sensitivity: {sensitivity_cube}, Precision: {precision_cube}, \"\n",
    "          f\"AUC-ROC: {auc_cube}, Accuracy: {accuracy_cube:.4f}, F1 Score: {f1_cube}\")\n",
    "    print(f\"  Centro de masa Infiltrado (Pred): {pred_center}\")\n",
    "    print(f\"  Centro de masa Infiltrado (True): {true_center}\")\n",
    "    print(f\"  Distancia entre centros: {distance:.2f}\")\n",
    "    \n",
    "    # Guardar mapas voxel-wise\n",
    "    nib.save(nib.Nifti1Image(prob_maps_np_nifti, np.eye(4)), \n",
    "             os.path.join(output_dir, f\"probability_maps_case_{idx}.nii.gz\"))\n",
    "    nib.save(nib.Nifti1Image(labels_np, np.eye(4)), \n",
    "             os.path.join(output_dir, f\"labels_case_{idx}.nii.gz\"))\n",
    "    nib.save(nib.Nifti1Image(segmentation_np, np.eye(4)), \n",
    "             os.path.join(output_dir, f\"segmentation_case_{idx}.nii.gz\"))\n",
    "\n",
    "# Calcular promedios y desviaciones estándar (voxel-wise)\n",
    "class_names = [\"Fondo\", \"Infiltrado\", \"Vasogénico\"]\n",
    "print(\"\\nResultados Voxel-wise:\")\n",
    "for cls in range(3):\n",
    "    dice_mean = np.nanmean(all_dice[cls])\n",
    "    dice_std = np.nanstd(all_dice[cls])\n",
    "    sens_mean = np.nanmean(all_sensitivity[cls])\n",
    "    sens_std = np.nanstd(all_sensitivity[cls])\n",
    "    prec_mean = np.nanmean(all_precision[cls])\n",
    "    prec_std = np.nanstd(all_precision[cls])\n",
    "    auc_mean = np.nanmean(all_auc[cls])\n",
    "    auc_std = np.nanstd(all_auc[cls])\n",
    "    f1_mean = np.nanmean(all_f1[cls])\n",
    "    f1_std = np.nanstd(all_f1[cls])\n",
    "    \n",
    "    print(f\"\\nClase {cls} ({class_names[cls]}):\")\n",
    "    print(f\"  Dice: {dice_mean:.4f} ± {dice_std:.4f}\")\n",
    "    print(f\"  Sensibilidad: {sens_mean:.4f} ± {sens_std:.4f}\")\n",
    "    print(f\"  Precisión: {prec_mean:.4f} ± {prec_std:.4f}\")\n",
    "    print(f\"  AUC-ROC: {auc_mean:.4f} ± {auc_std:.4f}\")\n",
    "    print(f\"  F1 Score: {f1_mean:.4f} ± {f1_std:.4f}\")\n",
    "\n",
    "accuracy_mean = np.nanmean(all_accuracy)\n",
    "accuracy_std = np.nanstd(all_accuracy)\n",
    "print(f\"\\nAccuracy Global: {accuracy_mean:.4f} ± {accuracy_std:.4f}\")\n",
    "\n",
    "# Calcular promedios y desviaciones estándar (cube-wise, clases 0, 1, 2)\n",
    "print(f\"\\nResultados Cube-wise (Grilla {grid_size}x{grid_size}x{grid_size}):\")\n",
    "for cls in range(3):\n",
    "    dice_mean = np.nanmean(all_dice_cube[cls])\n",
    "    dice_std = np.nanstd(all_dice_cube[cls])\n",
    "    sens_mean = np.nanmean(all_sensitivity_cube[cls])\n",
    "    sens_std = np.nanstd(all_sensitivity_cube[cls])\n",
    "    prec_mean = np.nanmean(all_precision_cube[cls])\n",
    "    prec_std = np.nanstd(all_precision_cube[cls])\n",
    "    auc_mean = np.nanmean(all_auc_cube[cls])\n",
    "    auc_std = np.nanstd(all_auc_cube[cls])\n",
    "    f1_mean = np.nanmean(all_f1_cube[cls])\n",
    "    f1_std = np.nanstd(all_f1_cube[cls])\n",
    "    \n",
    "    print(f\"\\nClase {cls} ({class_names[cls]}):\")\n",
    "    print(f\"  Dice: {dice_mean:.4f} ± {dice_std:.4f}\")\n",
    "    print(f\"  Sensibilidad: {sens_mean:.4f} ± {sens_std:.4f}\")\n",
    "    print(f\"  Precisión: {prec_mean:.4f} ± {prec_std:.4f}\")\n",
    "    print(f\"  AUC-ROC: {auc_mean:.4f} ± {auc_std:.4f}\")\n",
    "    print(f\"  F1 Score: {f1_mean:.4f} ± {f1_std:.4f}\")\n",
    "\n",
    "accuracy_mean_cube = np.nanmean(all_accuracy_cube)\n",
    "accuracy_std_cube = np.nanstd(all_accuracy_cube)\n",
    "print(f\"\\nAccuracy Global Cube-wise: {accuracy_mean_cube:.4f} ± {accuracy_std_cube:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
