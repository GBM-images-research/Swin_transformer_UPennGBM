{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading functions\n",
    "import os\n",
    "import time\n",
    "from monai.data import DataLoader, decollate_batch\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "\n",
    "from src.get_data import CustomDataset, CustomDatasetSeg\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from types import SimpleNamespace\n",
    "import wandb\n",
    "import logging\n",
    "\n",
    "#####\n",
    "import json\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai import transforms\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    Activations,\n",
    "    MapTransform,\n",
    "    Transform,\n",
    ")\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.utils.enums import MetricReduction\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai import data\n",
    "\n",
    "# from monai.data import decollate_batch\n",
    "from functools import partial\n",
    "from src.custom_transforms import (ConvertToMultiChannelBasedOnN_Froi, \n",
    "                                   ConvertToMultiChannelBasedOnAnotatedInfiltration, \n",
    "                                   masked, ConvertToMultiChannelBasedOnBratsClassesdI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trasnformaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = (128, 128, 128) # (220, 220, 155) (128, 128, 64)\n",
    "source_k=\"label\"\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        # ConvertToMultiChannelBasedOnN_Froi(keys=\"label\"),\n",
    "        # masked(keys=\"image\"),\n",
    "        ConvertToMultiChannelBasedOnAnotatedInfiltration(keys=\"label\"),\n",
    "        # ConvertToMultiChannelBasedOnBratsClassesdI(keys=\"label\"),\n",
    "        transforms.CropForegroundd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            source_key=source_k,\n",
    "            k_divisible=[roi[0], roi[1], roi[2]],\n",
    "        ),\n",
    "        transforms.RandSpatialCropd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            roi_size=[roi[0], roi[1], roi[2]],\n",
    "            random_size=False,\n",
    "        ),\n",
    "        transforms.NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "        \n",
    "    ]\n",
    ")\n",
    "val_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        # ConvertToMultiChannelBasedOnN_Froi(keys=\"label\"),\n",
    "        # masked(keys=\"image\"),\n",
    "        # ConvertToMultiChannelBasedOnAnotatedInfiltration(keys=\"label\"),\n",
    "        ConvertToMultiChannelBasedOnBratsClassesdI(keys=\"label\"),\n",
    "        transforms.RandSpatialCropd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            roi_size=[-1, -1, -1], #[224, 224, 128],\n",
    "            random_size=False,\n",
    "        ),\n",
    "        transforms.NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Crear el modelo\n",
    "######################\n",
    "\n",
    "### Hyperparameter\n",
    "roi = (128, 128, 128)  # (128, 128, 128)\n",
    "\n",
    "# Create Swin transformer\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def define_model(model_path):\n",
    "    model = SwinUNETR(\n",
    "        img_size=roi,\n",
    "        in_channels=11,\n",
    "        out_channels=2,  # mdificar con edema\n",
    "        feature_size=48, #48\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        dropout_path_rate=0.0,\n",
    "        use_checkpoint=True,\n",
    "    )\n",
    "\n",
    "    # # Load the best model\n",
    "    # model_path = \"artifacts/o9kppyr5_best_model:v0/model.pt\"\n",
    "\n",
    "    # Load the model on CPU\n",
    "    loaded_model = torch.load(model_path, map_location=torch.device('cuda:0'), weights_only=False)[\"state_dict\"]\n",
    "\n",
    "    # Load the state dictionary into the model\n",
    "    model.load_state_dict(loaded_model)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/training2/miniconda3/envs/monai_env/lib/python3.11/site-packages/monai/utils/deprecate_utils.py:221: FutureWarning: monai.networks.nets.swin_unetr SwinUNETR.__init__:img_size: Argument `img_size` has been deprecated since version 1.3. It will be removed in version 1.5. The img_size argument is not required anymore and checks on the input size are run during forward().\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SwinUNETR(\n",
       "  (swinViT): SwinTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv3d(11, 48, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (layers1): ModuleList(\n",
       "      (0): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=48, out_features=144, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=48, out_features=48, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (linear1): Linear(in_features=48, out_features=192, bias=True)\n",
       "              (linear2): Linear(in_features=192, out_features=48, bias=True)\n",
       "              (fn): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=384, out_features=96, bias=False)\n",
       "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layers2): ModuleList(\n",
       "      (0): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (linear1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (linear2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (fn): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=768, out_features=192, bias=False)\n",
       "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layers3): ModuleList(\n",
       "      (0): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (fn): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=1536, out_features=384, bias=False)\n",
       "          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layers4): ModuleList(\n",
       "      (0): BasicLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (linear1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (linear2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (fn): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (reduction): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder1): UnetrBasicBlock(\n",
       "    (layer): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(11, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(11, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder2): UnetrBasicBlock(\n",
       "    (layer): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder3): UnetrBasicBlock(\n",
       "    (layer): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder4): UnetrBasicBlock(\n",
       "    (layer): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder10): UnetrBasicBlock(\n",
       "    (layer): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(768, 768, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(768, 768, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(768, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder5): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(768, 384, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(768, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(768, 384, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder4): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(384, 192, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(384, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(384, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder3): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(192, 96, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(192, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(192, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder2): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(96, 48, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(96, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(96, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder1): UnetrUpBlock(\n",
       "    (transp_conv): Convolution(\n",
       "      (conv): ConvTranspose3d(48, 48, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)\n",
       "    )\n",
       "    (conv_block): UnetResBlock(\n",
       "      (conv1): Convolution(\n",
       "        (conv): Conv3d(96, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (conv2): Convolution(\n",
       "        (conv): Conv3d(48, 48, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (norm1): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (norm2): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (conv3): Convolution(\n",
       "        (conv): Conv3d(96, 48, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "      )\n",
       "      (norm3): InstanceNorm3d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (out): UnetOutBlock(\n",
       "    (conv): Convolution(\n",
       "      (conv): Conv3d(48, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo TC+Edema\n",
    "model1=define_model(\"artifacts/vtzpbajf_best_model:v0/model.pt\") # o9kppyr5\n",
    "model1.to(device)\n",
    "model1.eval()\n",
    "\n",
    "# Modelo Infitracion+Edema\n",
    "model2=define_model(\"artifacts/1dhzmigz_best_model:v0/model.pt\") # uixfayrn - rvu24jip\n",
    "model2.to(device)\n",
    "model2.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtener embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 images and 6 labels.\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 0\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 1\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 2\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 3\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 4\n",
      "Image torch.Size([1, 11, 128, 128, 128])\n",
      "label before squeeze torch.Size([1, 2, 128, 128, 128])\n",
      "label (128, 128, 128)\n",
      "decoder_features_model1: torch.Size([1, 48, 128, 128, 128])\n",
      "decoder_features_model2: torch.Size([1, 48, 128, 128, 128])\n",
      "Guardado embeddings y etiquetas para caso 5\n"
     ]
    }
   ],
   "source": [
    "# Create dataset data loader\n",
    "# dataset_path='./Dataset/Dataset_recurrence'\n",
    "dataset_path='./Dataset/Dataset_30_6'\n",
    "train_set=CustomDataset(dataset_path, section=\"test_6\", transform=train_transform) # v_transform\n",
    "train_loader = DataLoader(train_set, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "# Directorios para embeddings de cada modelo\n",
    "embedding_dir_model1 = \"Dataset/contrastive_voxel_wise/embeddings_model1\"\n",
    "embedding_dir_model2 = \"Dataset/contrastive_voxel_wise/embeddings_model2\"\n",
    "label_output_dir = \"Dataset/contrastive_voxel_wise/labels\"\n",
    "\n",
    "# Crear carpetas si no existen\n",
    "os.makedirs(embedding_dir_model1, exist_ok=True)\n",
    "os.makedirs(embedding_dir_model2, exist_ok=True)\n",
    "os.makedirs(label_output_dir, exist_ok=True)\n",
    "\n",
    "# Variables para las características de los decoders de ambos modelos\n",
    "decoder_features_model1 = None\n",
    "decoder_features_model2 = None\n",
    "\n",
    "# Funciones hook para cada modelo\n",
    "def decoder_hook_fn_model1(module, input, output):\n",
    "    global decoder_features_model1\n",
    "    decoder_features_model1 = output\n",
    "\n",
    "def decoder_hook_fn_model2(module, input, output):\n",
    "    global decoder_features_model2\n",
    "    decoder_features_model2 = output\n",
    "\n",
    "# Registrar los hooks en los decoders de ambos modelos\n",
    "hook_handle_decoder1 = model1.decoder1.conv_block.register_forward_hook(decoder_hook_fn_model1)\n",
    "hook_handle_decoder2 = model2.decoder1.conv_block.register_forward_hook(decoder_hook_fn_model2)\n",
    "\n",
    "# Extraer y guardar\n",
    "with torch.no_grad():\n",
    "    for idx, batch_data in enumerate(train_loader):\n",
    "        image, label = batch_data[\"image\"], batch_data[\"label\"]\n",
    "        print(\"Image\", image.shape)  # [1, 11, 128, 128, 128]\n",
    "        print(\"label before squeeze\", label.shape)  # [1, 2, 128, 128, 128]\n",
    "        \n",
    "        image = image.to(device)\n",
    "        label = label.squeeze(0)  # [2, 128, 128, 128]\n",
    "        \n",
    "        # Convertir one-hot a etiquetas únicas\n",
    "        label_sum = label.sum(dim=0)  # [128, 128, 128], suma de canales\n",
    "        label_class = torch.zeros_like(label_sum, dtype=torch.long)  # [128, 128, 128]\n",
    "        \n",
    "        # Asignar clases:\n",
    "        # - Fondo (0, 0) -> 0\n",
    "        # - Vasogénico (1, 0) -> 1\n",
    "        # - Infiltrado (0, 1) -> 2\n",
    "        label_class[label[1] == 1] = 2  # Infiltrado\n",
    "        label_class[(label[0] == 1) & (label[1] == 0)] = 1  # Vasogénico\n",
    "        # Donde label_sum == 0, ya es fondo (0)\n",
    "        \n",
    "        label = label_class.cpu().numpy()  # [128, 128, 128]\n",
    "        print(\"label\", label.shape)\n",
    "        \n",
    "        # Obtener embeddings de ambos modelos\n",
    "        _ = model1(image)  # Forward para model1\n",
    "        print(\"decoder_features_model1:\", decoder_features_model1.shape)  # [1, 48, 128, 128, 128]\n",
    "        \n",
    "        _ = model2(image)  # Forward para model2\n",
    "        print(\"decoder_features_model2:\", decoder_features_model2.shape)  # [1, 48, 128, 128, 128]\n",
    "        \n",
    "        # Guardar embeddings y etiquetas\n",
    "        np.save(f\"{embedding_dir_model1}/case_{idx}.npy\", decoder_features_model1.cpu().numpy())\n",
    "        np.save(f\"{embedding_dir_model2}/case_{idx}.npy\", decoder_features_model2.cpu().numpy())\n",
    "        np.save(f\"{label_output_dir}/case_{idx}.npy\", label)\n",
    "        \n",
    "        print(f\"Guardado embeddings y etiquetas para caso {idx}\")\n",
    "\n",
    "# Remover los hooks\n",
    "hook_handle_decoder1.remove()\n",
    "hook_handle_decoder2.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nibabel as nib\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dispositivo\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset (ya lo tienes)\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, embedding_dir, label_dir):\n",
    "        self.embedding_dir = embedding_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.case_files = [f for f in os.listdir(embedding_dir) if f.endswith(\".npy\")]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.case_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        embedding_path = os.path.join(self.embedding_dir, f\"case_{idx}.npy\")\n",
    "        label_path = os.path.join(self.label_dir, f\"case_{idx}.npy\")\n",
    "        \n",
    "        embeddings = np.load(embedding_path)  # [1, 48, 128, 128, 128]\n",
    "        labels = np.load(label_path)  # [128, 128, 128]\n",
    "        \n",
    "        embeddings = torch.tensor(embeddings, dtype=torch.float32).squeeze(0)  # [48, 128, 128, 128]\n",
    "        labels = torch.tensor(labels, dtype=torch.long)  # [128, 128, 128]\n",
    "        \n",
    "        return embeddings, labels\n",
    "\n",
    "# # Modelo de proyección\n",
    "# class ProjectionHead(nn.Module):\n",
    "#     def __init__(self, input_dim=48, hidden_dim=128, output_dim=128):\n",
    "#         super(ProjectionHead, self).__init__()\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.Linear(input_dim, hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim, output_dim)\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         return self.net(x)\n",
    "\n",
    "# Modelo de proyección (MLP más profundo)\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, input_dim=48, hidden_dim1=256, hidden_dim2=128, output_dim=128, dropout_p=0.3):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_dim2, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# # Clasificador supervisado\n",
    "# class Classifier(nn.Module):\n",
    "#     def __init__(self, input_dim=128, num_classes=3):\n",
    "#         super(Classifier, self).__init__()\n",
    "#         self.fc = nn.Linear(input_dim, num_classes)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         return self.fc(x)\n",
    "\n",
    "# Clasificador supervisado (MLP)\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim=128, hidden_dim1=256, hidden_dim2=128, num_classes=3, dropout_p=0.3):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_dim2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Configuración para ambos modelos\n",
    "embedding_dir_model1 = \"Dataset/contrastive_voxel_wise/embeddings_model1\"\n",
    "embedding_dir_model2 = \"Dataset/contrastive_voxel_wise/embeddings_model2\"\n",
    "label_dir = \"Dataset/contrastive_voxel_wise/labels\"\n",
    "batch_size = 1\n",
    "\n",
    "# Cargar datasets y DataLoaders para ambos modelos\n",
    "dataset_model1 = EmbeddingDataset(embedding_dir_model1, label_dir)\n",
    "dataset_model2 = EmbeddingDataset(embedding_dir_model2, label_dir)\n",
    "loader_model1 = DataLoader(dataset_model1, batch_size=batch_size, shuffle=False)\n",
    "loader_model2 = DataLoader(dataset_model2, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Cargar modelos contrastivos preentrenados\n",
    "projection_head1 = ProjectionHead(input_dim=48).to(device)\n",
    "projection_head1.load_state_dict(torch.load(\"trained_models/checkpoints_contrastive/contrastive_projection_head_final_new_pipe1_v01_m1.pth\", map_location=device))\n",
    "projection_head1.eval()\n",
    "\n",
    "projection_head2 = ProjectionHead(input_dim=48).to(device)\n",
    "projection_head2.load_state_dict(torch.load(\"trained_models/checkpoints_contrastive/contrastive_projection_head_final_new_pipe2_m1_1dhzmigz.pth\", map_location=device))\n",
    "projection_head2.eval()\n",
    "\n",
    "# Cargar clasificadores preentrenados\n",
    "classifier1 = Classifier(input_dim=128, num_classes=3).to(device)\n",
    "classifier1.load_state_dict(torch.load(\"trained_models/checkpoints/supervised_classifier_final_pipe1_v01_m1.pth\", map_location=device))\n",
    "classifier1.eval()\n",
    "\n",
    "classifier2 = Classifier(input_dim=128, num_classes=3).to(device)\n",
    "classifier2.load_state_dict(torch.load(\"trained_models/checkpoints/supervised_classifier_final_pipe2_m1_1dhzmigz.pth\", map_location=device))\n",
    "classifier2.eval()\n",
    "\n",
    "# Función para generar mapas de probabilidad\n",
    "def generate_probability_maps(embeddings, projection_head, classifier, device):\n",
    "    with torch.no_grad():\n",
    "        embeddings = embeddings.to(device).squeeze(0).permute(1, 2, 3, 0)\n",
    "        embeddings_flat = embeddings.reshape(-1, 48)\n",
    "        \n",
    "        z = projection_head(embeddings_flat)\n",
    "        z = F.normalize(z, dim=1)\n",
    "        \n",
    "        logits = classifier(z)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        \n",
    "        probs = probs.view(128, 128, 128, 3).permute(3, 0, 1, 2)\n",
    "        return probs\n",
    "\n",
    "# Funciones para calcular métricas\n",
    "def calculate_metrics(pred, true,prob_maps=None, num_classes=3):\n",
    "    dice_scores = []\n",
    "    sensitivity_scores = []\n",
    "    precision_scores = []\n",
    "    auc_scores = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    # Calcular Accuracy global\n",
    "    accuracy = accuracy_score(true.flatten(), pred.flatten())\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        pred_cls = (pred == cls).astype(np.uint8)\n",
    "        true_cls = (true == cls).astype(np.uint8)\n",
    "        \n",
    "        tp = np.sum(pred_cls * true_cls)\n",
    "        fp = np.sum(pred_cls * (1 - true_cls))\n",
    "        fn = np.sum((1 - pred_cls) * true_cls)\n",
    "        \n",
    "        dice = 2 * tp / (2 * tp + fp + fn + 1e-6)\n",
    "        dice_scores.append(dice)\n",
    "        \n",
    "        sensitivity = tp / (tp + fn + 1e-6)\n",
    "        sensitivity_scores.append(sensitivity)\n",
    "        \n",
    "        precision = tp / (tp + fp + 1e-6)\n",
    "        precision_scores.append(precision)\n",
    "\n",
    "        # F1 Score\n",
    "        f1 = f1_score(true_cls.flatten(), pred_cls.flatten(), zero_division=0)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        # AUC-ROC (requiere mapas de probabilidad)\n",
    "        if prob_maps is not None:\n",
    "            try:\n",
    "                auc = roc_auc_score(true_cls.flatten(), prob_maps[cls].flatten())\n",
    "                auc_scores.append(auc)\n",
    "            except ValueError:\n",
    "                auc_scores.append(np.nan)  # Manejar casos donde AUC no se puede calcular\n",
    "        else:\n",
    "            auc_scores.append(np.nan)  # Si no se proporcionan mapas de probabilidad\n",
    "    \n",
    "    return dice_scores, sensitivity_scores, precision_scores, auc_scores, accuracy, f1_scores\n",
    "\n",
    "# Directorio de salida\n",
    "output_dir = \"trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2_cube8\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecutar calculo y guardar mapas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardada curva ROC en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2/roc_curve_case_0.png\n",
      "Caso 0 - Dice: [0.9970035703989371, 0.520193908666654, 0.6986080252122276], Sensitivity: [0.9941729438903477, 0.4651015729859273, 0.8572294069472858], Precision: [0.9998503617499696, 0.5900915958885441, 0.5895229395928157], AUC-ROC: [0.9997574644423223, 0.9910906713472637, 0.9931217498754038], Accuracy: 0.9818, F1 Score: [0.9970035703991849, 0.5201939086743189, 0.698608025218861]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2/probability_maps_case_0.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2/labels_case_0.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2/segmentation_case_0.nii.gz\n",
      "Guardada curva ROC en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2/roc_curve_case_1.png\n",
      "Caso 1 - Dice: [0.9961940335339866, 0.7573532532253574, 0.7093981720864045], Sensitivity: [0.9927684989178468, 0.8805763915435673, 0.8734924409345383], Precision: [0.9996432895239844, 0.6643830515552622, 0.5972068985367515], AUC-ROC: [0.9991617904186789, 0.9987704127823962, 0.9967641799853623], Accuracy: 0.9909, F1 Score: [0.9961940335342289, 0.7573532532575018, 0.7093981720986376]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2/probability_maps_case_1.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2/labels_case_1.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2/segmentation_case_1.nii.gz\n",
      "Guardada curva ROC en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2/roc_curve_case_2.png\n",
      "Caso 2 - Dice: [0.9954207128587987, 0.7141549611409692, 0.8115478311414778], Sensitivity: [0.9918475049000461, 0.9249993951340334, 0.7700496806169659], Precision: [0.9990197594230913, 0.5815877941468294, 0.8577734344881823], AUC-ROC: [0.9994691909437659, 0.9964043270508957, 0.9960599556149978], Accuracy: 0.9798, F1 Score: [0.9954207128590542, 0.714154961147639, 0.8115478311456931]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2/probability_maps_case_2.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2/labels_case_2.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2/segmentation_case_2.nii.gz\n",
      "Guardada curva ROC en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2/roc_curve_case_3.png\n",
      "Caso 3 - Dice: [0.9946954256219234, 0.6495593571237637, 0.3871328671230227], Sensitivity: [0.9896221671657137, 0.5427375107503988, 0.9631785396731395], Precision: [0.9998209678212706, 0.8087349395850397, 0.2422506524014305], AUC-ROC: [0.9983193412576696, 0.9983511843142627, 0.9961697289113187], Accuracy: 0.9880, F1 Score: [0.9946954256221634, 0.649559357179886, 0.38713286713286715]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2/probability_maps_case_3.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2/labels_case_3.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2/segmentation_case_3.nii.gz\n",
      "Guardada curva ROC en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2/roc_curve_case_4.png\n",
      "Caso 4 - Dice: [0.9965279044150087, 0.7510474573280651, 0.6372424828180269], Sensitivity: [0.9930836863287986, 0.8135382059318024, 0.8549704811941695], Precision: [0.9999960961570087, 0.6974721529577604, 0.5078999574710252], AUC-ROC: [0.9996140716399262, 0.9982643383338923, 0.9963530494438688], Accuracy: 0.9905, F1 Score: [0.996527904415251, 0.7510474573486321, 0.6372424828321889]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2/probability_maps_case_4.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2/labels_case_4.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2/segmentation_case_4.nii.gz\n",
      "Guardada curva ROC en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2/roc_curve_case_5.png\n",
      "Caso 5 - Dice: [0.9969356400590003, 0.7873596864002423, 0.6544481470151394], Sensitivity: [0.9939036829084408, 0.7655546205819543, 0.8678748381525744], Precision: [0.9999861521116298, 0.8104433032694811, 0.5252735377407733], AUC-ROC: [0.9998924458565644, 0.9978815404296216, 0.996220551229559], Accuracy: 0.9882, F1 Score: [0.996935640059246, 0.7873596864102629, 0.6544481470261537]\n",
      "Guardado mapa de probabilidad en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2/probability_maps_case_5.nii.gz\n",
      "Guardadas etiquetas en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2/labels_case_5.nii.gz\n",
      "Guardada segmentación en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2/segmentation_case_5.nii.gz\n",
      "Guardada curva ROC promedio en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2/roc_curve_average.png\n",
      "\n",
      "Clase 0 (Fondo):\n",
      "  Dice: 0.9961 ± 0.0008\n",
      "  Sensibilidad: 0.9926 ± 0.0015\n",
      "  Precisión: 0.9997 ± 0.0003\n",
      "  AUC-ROC: 0.9994 ± 0.0005\n",
      "  F1 Score: 0.9961 ± 0.0008\n",
      "\n",
      "Clase 1 (Vasogénico):\n",
      "  Dice: 0.6966 ± 0.0899\n",
      "  Sensibilidad: 0.7321 ± 0.1704\n",
      "  Precisión: 0.6921 ± 0.0922\n",
      "  AUC-ROC: 0.9968 ± 0.0027\n",
      "  F1 Score: 0.6966 ± 0.0899\n",
      "\n",
      "Clase 2 (Infiltrado):\n",
      "  Dice: 0.6497 ± 0.1299\n",
      "  Sensibilidad: 0.8645 ± 0.0561\n",
      "  Precisión: 0.5533 ± 0.1805\n",
      "  AUC-ROC: 0.9958 ± 0.0012\n",
      "  F1 Score: 0.6497 ± 0.1299\n",
      "\n",
      "Accuracy Global: 0.9865 ± 0.0042\n"
     ]
    }
   ],
   "source": [
    "# Listas para métricas\n",
    "all_dice = {0: [], 1: [], 2: []}\n",
    "all_sensitivity = {0: [], 1: [], 2: []}\n",
    "all_precision = {0: [], 1: [], 2: []}\n",
    "all_auc = {0: [], 1: [], 2: []}  # Lista para AUC-ROC\n",
    "all_accuracy = []  # Lista para Accuracy global\n",
    "all_f1 = {0: [], 1: [], 2: []}  # Lista para F1 Score\n",
    "# Listas para almacenar FPR y TPR de todos los casos\n",
    "all_fpr = {0: [], 1: [], 2: []}\n",
    "all_tpr = {0: [], 1: [], 2: []}\n",
    "\n",
    "\n",
    "# Procesar y combinar\n",
    "for idx, ((embeddings1, labels1), (embeddings2, labels2)) in enumerate(zip(loader_model1, loader_model2)):\n",
    "    # Generar mapas de probabilidad para ambos modelos\n",
    "    prob_maps1 = generate_probability_maps(embeddings1, projection_head1, classifier1, device)  # [3, 128, 128, 128]\n",
    "    prob_maps2 = generate_probability_maps(embeddings2, projection_head2, classifier2, device)  # [3, 128, 128, 128]\n",
    "    \n",
    "    # Combinar mapas:\n",
    "    # - Clase 0: del modelo 1\n",
    "    # - Clase 1: máximo entre ambos modelos\n",
    "    # - Clase 2: del modelo 2\n",
    "    combined_prob_maps = torch.zeros_like(prob_maps1)  # [3, 128, 128, 128]\n",
    "    combined_prob_maps[0] = prob_maps1[0]  # Clase 0 del modelo 1\n",
    "    combined_prob_maps[1] = torch.max(prob_maps1[1], prob_maps2[1])  # Clase 1 máximo entre ambos\n",
    "    combined_prob_maps[2] = prob_maps2[2]  # Clase 2 del modelo 2\n",
    "    \n",
    "    # Normalizar probabilidades para que sumen 1 en cada vóxel\n",
    "    combined_prob_maps = combined_prob_maps / combined_prob_maps.sum(dim=0, keepdim=True)\n",
    "    \n",
    "    # Convertir a numpy\n",
    "    prob_maps_np = combined_prob_maps.cpu().numpy()  # [3, 128, 128, 128]\n",
    "    prob_maps_np_nifti = np.transpose(prob_maps_np, (1, 2, 3, 0))  # [128, 128, 128, 3]\n",
    "    \n",
    "    # Generar segmentación semántica\n",
    "    segmentation = np.argmax(prob_maps_np, axis=0)  # [128, 128, 128]\n",
    "    # segmentation_np = segmentation.astype(np.uint8)\n",
    "\n",
    "    # Aplicar umbral: asignar clase 1 si la probabilidad es > 0.4\n",
    "    # class_1_mask = prob_maps_np[1] > 0.3  # Máscara booleana para clase 1\n",
    "    # segmentation[class_1_mask] = 1  # Asignar clase 1 a los vóxeles que cumplen el criterio\n",
    "    segmentation_np = segmentation.astype(np.uint8)\n",
    "    \n",
    "    # Etiquetas (usamos las del modelo 2, asumiendo que son iguales)\n",
    "    labels = labels2.squeeze(0)  # [128, 128, 128]\n",
    "    labels_np = labels.cpu().numpy().astype(np.uint8)\n",
    "    \n",
    "    # Calcular métricas\n",
    "    dice, sensitivity, precision, auc, accuracy, f1 = calculate_metrics(segmentation_np, labels_np, prob_maps=prob_maps_np)\n",
    "    for cls in range(3):\n",
    "        all_dice[cls].append(dice[cls])\n",
    "        all_sensitivity[cls].append(sensitivity[cls])\n",
    "        all_precision[cls].append(precision[cls])\n",
    "        all_auc[cls].append(auc[cls])\n",
    "        all_f1[cls].append(f1[cls])\n",
    "    all_accuracy.append(accuracy)\n",
    "\n",
    "     # Graficar curvas ROC\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    class_names = [\"Fondo\", \"Vasogénico\", \"Infiltrado\"]\n",
    "    colors = ['blue', 'green', 'red']\n",
    "    \n",
    "    for cls in range(3):\n",
    "        # Etiquetas binarias para la clase actual\n",
    "        true_cls = (labels_np == cls).astype(np.uint8).flatten()\n",
    "        prob_cls = prob_maps_np[cls].flatten()\n",
    "        \n",
    "        # Calcular puntos de la curva ROC\n",
    "        fpr, tpr, _ = roc_curve(true_cls, prob_cls)\n",
    "        auc_value = auc[cls]  # Usar el AUC calculado previamente\n",
    "        all_fpr[cls].append(fpr)\n",
    "        all_tpr[cls].append(tpr)\n",
    "        \n",
    "        # Graficar\n",
    "        plt.plot(fpr, tpr, color=colors[cls], label=f'{class_names[cls]} (AUC = {auc_value:.4f})')\n",
    "    \n",
    "    # Configurar el gráfico\n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # Línea diagonal (clasificador aleatorio)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Tasa de Falsos Positivos (FPR)')\n",
    "    plt.ylabel('Tasa de Verdaderos Positivos (TPR)')\n",
    "    plt.title(f'Curva ROC - Caso {idx}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    # Guardar el gráfico\n",
    "    roc_output_path = os.path.join(output_dir, f\"roc_curve_case_{idx}.png\")\n",
    "    plt.savefig(roc_output_path)\n",
    "    plt.close()\n",
    "    print(f\"Guardada curva ROC en {roc_output_path}\")\n",
    "\n",
    "    print(f\"Caso {idx} - Dice: {dice}, Sensitivity: {sensitivity}, Precision: {precision}, \"\n",
    "          f\"AUC-ROC: {auc}, Accuracy: {accuracy:.4f}, F1 Score: {f1}\")\n",
    "    \n",
    "    # Crear imágenes NIfTI\n",
    "    affine = np.eye(4)\n",
    "    \n",
    "    # Guardar mapas de probabilidad combinados\n",
    "    nifti_prob_img = nib.Nifti1Image(prob_maps_np_nifti, affine)\n",
    "    prob_output_path = os.path.join(output_dir, f\"probability_maps_case_{idx}.nii.gz\")\n",
    "    nib.save(nifti_prob_img, prob_output_path)\n",
    "    print(f\"Guardado mapa de probabilidad en {prob_output_path}\")\n",
    "    \n",
    "    # Guardar etiquetas\n",
    "    nifti_label_img = nib.Nifti1Image(labels_np, affine)\n",
    "    label_output_path = os.path.join(output_dir, f\"labels_case_{idx}.nii.gz\")\n",
    "    nib.save(nifti_label_img, label_output_path)\n",
    "    print(f\"Guardadas etiquetas en {label_output_path}\")\n",
    "    \n",
    "    # Guardar segmentación semántica\n",
    "    nifti_seg_img = nib.Nifti1Image(segmentation_np, affine)\n",
    "    seg_output_path = os.path.join(output_dir, f\"segmentation_case_{idx}.nii.gz\")\n",
    "    nib.save(nifti_seg_img, seg_output_path)\n",
    "    print(f\"Guardada segmentación en {seg_output_path}\")\n",
    "\n",
    "# Curva ROC Promedio\n",
    "plt.figure(figsize=(8, 6))\n",
    "for cls in range(3):\n",
    "    # Interpolar FPR y TPR a una base común\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    tprs = []\n",
    "    for fpr, tpr in zip(all_fpr[cls], all_tpr[cls]):\n",
    "        tpr_interp = np.interp(mean_fpr, fpr, tpr)\n",
    "        tpr_interp[0] = 0.0  # Asegurar que comienza en 0\n",
    "        tprs.append(tpr_interp)\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0  # Asegurar que termina en 1\n",
    "    mean_auc = np.nanmean(all_auc[cls])\n",
    "    \n",
    "    plt.plot(mean_fpr, mean_tpr, color=colors[cls], label=f'{class_names[cls]} (AUC = {mean_auc:.4f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Tasa de Falsos Positivos (FPR)')\n",
    "plt.ylabel('Tasa de Verdaderos Positivos (TPR)')\n",
    "plt.title('Curva ROC Promedio')\n",
    "plt.legend(loc=\"lower right\")\n",
    "roc_avg_path = os.path.join(output_dir, \"roc_curve_average.png\")\n",
    "plt.savefig(roc_avg_path)\n",
    "plt.close()\n",
    "print(f\"Guardada curva ROC promedio en {roc_avg_path}\")\n",
    "\n",
    "# # Calcular promedios y desviaciones estándar\n",
    "# class_names = [\"Fondo\", \"Vasogénico\", \"Infiltrado\"]\n",
    "# for cls in range(3):\n",
    "#     dice_mean = np.mean(all_dice[cls])\n",
    "#     dice_std = np.std(all_dice[cls])\n",
    "#     sens_mean = np.mean(all_sensitivity[cls])\n",
    "#     sens_std = np.std(all_sensitivity[cls])\n",
    "#     prec_mean = np.mean(all_precision[cls])\n",
    "#     prec_std = np.std(all_precision[cls])\n",
    "    \n",
    "#     print(f\"\\nClase {cls} ({class_names[cls]}):\")\n",
    "#     print(f\"  Dice: {dice_mean:.4f} ± {dice_std:.4f}\")\n",
    "#     print(f\"  Sensibilidad: {sens_mean:.4f} ± {sens_std:.4f}\")\n",
    "#     print(f\"  Precisión: {prec_mean:.4f} ± {prec_std:.4f}\")\n",
    "\n",
    "# Calcular promedios y desviaciones estándar\n",
    "class_names = [\"Fondo\", \"Vasogénico\", \"Infiltrado\"]\n",
    "for cls in range(3):\n",
    "    dice_mean = np.nanmean(all_dice[cls])\n",
    "    dice_std = np.nanstd(all_dice[cls])\n",
    "    sens_mean = np.nanmean(all_sensitivity[cls])\n",
    "    sens_std = np.nanstd(all_sensitivity[cls])\n",
    "    prec_mean = np.nanmean(all_precision[cls])\n",
    "    prec_std = np.nanstd(all_precision[cls])\n",
    "    auc_mean = np.nanmean(all_auc[cls])\n",
    "    auc_std = np.nanstd(all_auc[cls])\n",
    "    f1_mean = np.nanmean(all_f1[cls])\n",
    "    f1_std = np.nanstd(all_f1[cls])\n",
    "    \n",
    "    print(f\"\\nClase {cls} ({class_names[cls]}):\")\n",
    "    print(f\"  Dice: {dice_mean:.4f} ± {dice_std:.4f}\")\n",
    "    print(f\"  Sensibilidad: {sens_mean:.4f} ± {sens_std:.4f}\")\n",
    "    print(f\"  Precisión: {prec_mean:.4f} ± {prec_std:.4f}\")\n",
    "    print(f\"  AUC-ROC: {auc_mean:.4f} ± {auc_std:.4f}\")\n",
    "    print(f\"  F1 Score: {f1_mean:.4f} ± {f1_std:.4f}\")\n",
    "\n",
    "# Accuracy global\n",
    "accuracy_mean = np.nanmean(all_accuracy)\n",
    "accuracy_std = np.nanstd(all_accuracy)\n",
    "print(f\"\\nAccuracy Global: {accuracy_mean:.4f} ± {accuracy_std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metricas basdas en regions (supervoxeles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardada curva ROC en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2_cube8/roc_curve_case_0.png\n",
      "Guardada curva ROC cube-wise en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2_cube8/roc_curve_cube_case_0.png\n",
      "Caso 0 - Voxel-wise:\n",
      "  Dice: [0.9970035703989371, 0.520193908666654, 0.6986080252122276], Sensitivity: [0.9941729438903477, 0.4651015729859273, 0.8572294069472858], Precision: [0.9998503617499696, 0.5900915958885441, 0.5895229395928157], AUC-ROC: [0.9997442857524647, 0.9908721550503753, 0.9926824584823732], Accuracy: 0.9818, F1 Score: [0.9970035703991849, 0.5201939086743189, 0.698608025218861]\n",
      "Caso 0 - Cube-wise (tamaño 8):\n",
      "  Dice: [0.9978418178243186, 0.539682535399345, 0.6560846526133087], Sensitivity: [0.9956929310879927, 0.4415584358239164, 0.8611110991512347], Precision: [0.9999999997455471, 0.6938775368596422, 0.529914525385346], AUC-ROC: [0.9998240104199434, 0.9316412624449448, 0.9923341893085929], Accuracy: 0.9829, F1 Score: [0.9978418179509966, 0.5396825396825397, 0.6560846560846562]\n",
      "  Centro de masa Infiltrado (Pred): [6.81632653 8.55102041 7.75510204]\n",
      "  Centro de masa Infiltrado (True): [7.50649351 7.45454545 6.74025974]\n",
      "Distancia entre centros: 1.645749881819\n",
      "Guardada curva ROC en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2_cube8/roc_curve_case_1.png\n",
      "Guardada curva ROC cube-wise en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2_cube8/roc_curve_cube_case_1.png\n",
      "Caso 1 - Voxel-wise:\n",
      "  Dice: [0.9961940335339866, 0.7573532532253574, 0.7093981720864045], Sensitivity: [0.9927684989178468, 0.8805763915435673, 0.8734924409345383], Precision: [0.9996432895239844, 0.6643830515552622, 0.5972068985367515], AUC-ROC: [0.9982288768831301, 0.9987986678355506, 0.9947559103374306], Accuracy: 0.9909, F1 Score: [0.9961940335342289, 0.7573532532575018, 0.7093981720986376]\n",
      "Caso 1 - Cube-wise (tamaño 8):\n",
      "  Dice: [0.9973955102322466, 0.8648648414901394, 0.7391304267485823], Sensitivity: [0.9952970294566097, 0.8888888395061756, 0.8947368185595574], Precision: [0.9995028583148141, 0.8421052188365674, 0.6296296179698219], AUC-ROC: [0.9992441654879773, 0.9997752166094491, 0.9984274078493425], Accuracy: 0.9939, F1 Score: [0.9973955103559469, 0.8648648648648649, 0.7391304347826088]\n",
      "  Centro de masa Infiltrado (Pred): [7.78947368 7.78947368 7.26315789]\n",
      "  Centro de masa Infiltrado (True): [8.05555556 8.         7.27777778]\n",
      "Distancia entre centros: 0.33960953001718336\n",
      "Guardada curva ROC en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2_cube8/roc_curve_case_2.png\n",
      "Guardada curva ROC cube-wise en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2_cube8/roc_curve_cube_case_2.png\n",
      "Caso 2 - Voxel-wise:\n",
      "  Dice: [0.9954207128587987, 0.7141549611409692, 0.8115478311414778], Sensitivity: [0.9918475049000461, 0.9249993951340334, 0.7700496806169659], Precision: [0.9990197594230913, 0.5815877941468294, 0.8577734344881823], AUC-ROC: [0.9992538722066036, 0.9960538811731321, 0.9950507396812097], Accuracy: 0.9798, F1 Score: [0.9954207128590542, 0.714154961147639, 0.8115478311456931]\n",
      "Caso 2 - Cube-wise (tamaño 8):\n",
      "  Dice: [0.9972502290169896, 0.7373737336496277, 0.8291316503385668], Sensitivity: [0.9955555552952796, 0.9605263031509698, 0.7589743550821828], Precision: [0.998950681794609, 0.5983606508331094, 0.913580241274196], AUC-ROC: [0.9995678074427804, 0.9972096753076722, 0.9817811343573969], Accuracy: 0.9836, F1 Score: [0.997250229147571, 0.7373737373737373, 0.8291316526610644]\n",
      "  Centro de masa Infiltrado (Pred): [9.64754098 5.71311475 7.29508197]\n",
      "  Centro de masa Infiltrado (True): [9.71052632 6.19736842 6.77631579]\n",
      "Distancia entre centros: 0.7124514812920449\n",
      "Guardada curva ROC en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2_cube8/roc_curve_case_3.png\n",
      "Guardada curva ROC cube-wise en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2_cube8/roc_curve_cube_case_3.png\n",
      "Caso 3 - Voxel-wise:\n",
      "  Dice: [0.9946954256219234, 0.6495593571237637, 0.3871328671230227], Sensitivity: [0.9896221671657137, 0.5427375107503988, 0.9631785396731395], Precision: [0.9998209678212706, 0.8087349395850397, 0.2422506524014305], AUC-ROC: [0.9982360359904578, 0.9980275841103045, 0.9913611417658387], Accuracy: 0.9880, F1 Score: [0.9946954256221634, 0.649559357179886, 0.38713286713286715]\n",
      "Caso 3 - Cube-wise (tamaño 8):\n",
      "  Dice: [0.9967988179023887, 0.6999999650000017, 0.3999999920000002], Sensitivity: [0.9936180655391218, 0.5833332847222263, 0.99999990000001], Precision: [0.9999999997529645, 0.8749998906250137, 0.24999999375000018], AUC-ROC: [0.9991911009952247, 0.9556909076069212, 0.9987273617229565], Accuracy: 0.9924, F1 Score: [0.9967988180251169, 0.7000000000000001, 0.4]\n",
      "  Centro de masa Infiltrado (Pred): [7.    9.    9.125]\n",
      "  Centro de masa Infiltrado (True): [6.83333333 8.83333333 8.58333333]\n",
      "Distancia entre centros: 0.5907269532815754\n",
      "Guardada curva ROC en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2_cube8/roc_curve_case_4.png\n",
      "Guardada curva ROC cube-wise en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2_cube8/roc_curve_cube_case_4.png\n",
      "Caso 4 - Voxel-wise:\n",
      "  Dice: [0.9965279044150087, 0.7510474573280651, 0.6372424828180269], Sensitivity: [0.9930836863287986, 0.8135382059318024, 0.8549704811941695], Precision: [0.9999960961570087, 0.6974721529577604, 0.5078999574710252], AUC-ROC: [0.9996016155347753, 0.9980592234123471, 0.9952250280934484], Accuracy: 0.9905, F1 Score: [0.996527904415251, 0.7510474573486321, 0.6372424828321889]\n",
      "Caso 4 - Cube-wise (tamaño 8):\n",
      "  Dice: [0.9980153806750166, 0.7428571322448981, 0.7333333211111114], Sensitivity: [0.9962852895006723, 0.8124999746093758, 0.8461538136094686], Precision: [0.9997514908052307, 0.6842105083102499, 0.6470588044982705], AUC-ROC: [0.9997779713412239, 0.9509258120078741, 0.9987006237006237], Accuracy: 0.9939, F1 Score: [0.9980153807988091, 0.742857142857143, 0.7333333333333334]\n",
      "  Centro de masa Infiltrado (Pred): [7.23684211 7.57894737 7.        ]\n",
      "  Centro de masa Infiltrado (True): [7.40625 8.09375 7.     ]\n",
      "Distancia entre centros: 0.541960131633111\n",
      "Guardada curva ROC en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2_cube8/roc_curve_case_5.png\n",
      "Guardada curva ROC cube-wise en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2_cube8/roc_curve_cube_case_5.png\n",
      "Caso 5 - Voxel-wise:\n",
      "  Dice: [0.9969356400590003, 0.7873596864002423, 0.6544481470151394], Sensitivity: [0.9939036829084408, 0.7655546205819543, 0.8678748381525744], Precision: [0.9999861521116298, 0.8104433032694811, 0.5252735377407733], AUC-ROC: [0.9998858128010235, 0.997705949643649, 0.9948220587097338], Accuracy: 0.9882, F1 Score: [0.996935640059246, 0.7873596864102629, 0.6544481470261537]\n",
      "Caso 5 - Cube-wise (tamaño 8):\n",
      "  Dice: [0.9973581581334308, 0.82993196714332, 0.7083333259548612], Sensitivity: [0.995479658213089, 0.7922077819193795, 0.9189188940832732], Precision: [0.999243760776596, 0.871428558979592, 0.5762711766733699], AUC-ROC: [0.9997874205856178, 0.991478787447934, 0.9960381667698741], Accuracy: 0.9910, F1 Score: [0.9973581582589005, 0.8299319727891157, 0.7083333333333333]\n",
      "  Centro de masa Infiltrado (Pred): [8.01428571 7.51428571 7.51428571]\n",
      "  Centro de masa Infiltrado (True): [7.80519481 7.49350649 7.15584416]\n",
      "Distancia entre centros: 0.4154890312615578\n",
      "\n",
      "Resultados Voxel-wise:\n",
      "\n",
      "Clase 0 (Fondo):\n",
      "  Dice: 0.9961 ± 0.0008\n",
      "  Sensibilidad: 0.9926 ± 0.0015\n",
      "  Precisión: 0.9997 ± 0.0003\n",
      "  AUC-ROC: 0.9992 ± 0.0007\n",
      "  F1 Score: 0.9961 ± 0.0008\n",
      "\n",
      "Clase 1 (Infiltrado):\n",
      "  Dice: 0.6966 ± 0.0899\n",
      "  Sensibilidad: 0.7321 ± 0.1704\n",
      "  Precisión: 0.6921 ± 0.0922\n",
      "  AUC-ROC: 0.9966 ± 0.0027\n",
      "  F1 Score: 0.6966 ± 0.0899\n",
      "\n",
      "Clase 2 (Vasogénico):\n",
      "  Dice: 0.6497 ± 0.1299\n",
      "  Sensibilidad: 0.8645 ± 0.0561\n",
      "  Precisión: 0.5533 ± 0.1805\n",
      "  AUC-ROC: 0.9940 ± 0.0014\n",
      "  F1 Score: 0.6497 ± 0.1299\n",
      "\n",
      "Accuracy Global: 0.9865 ± 0.0042\n",
      "Guardada curva ROC promedio en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2_cube8/roc_curve_average.png\n",
      "\n",
      "Resultados Cube-wise:\n",
      "\n",
      "Clase 0 (Fondo):\n",
      "  Dice: 0.9974 ± 0.0004\n",
      "  Sensibilidad: 0.9953 ± 0.0008\n",
      "  Precisión: 0.9996 ± 0.0004\n",
      "  AUC-ROC: 0.9996 ± 0.0003\n",
      "  F1 Score: 0.9974 ± 0.0004\n",
      "\n",
      "Clase 1 (Infiltrado):\n",
      "  Dice: 0.7358 ± 0.1043\n",
      "  Sensibilidad: 0.7465 ± 0.1790\n",
      "  Precisión: 0.7608 ± 0.1069\n",
      "  AUC-ROC: 0.9711 ± 0.0262\n",
      "  F1 Score: 0.7358 ± 0.1043\n",
      "\n",
      "Clase 2 (Vasogénico):\n",
      "  Dice: 0.6777 ± 0.1344\n",
      "  Sensibilidad: 0.8800 ± 0.0734\n",
      "  Precisión: 0.5911 ± 0.1953\n",
      "  AUC-ROC: 0.9943 ± 0.0060\n",
      "  F1 Score: 0.6777 ± 0.1344\n",
      "\n",
      "Generando curva ROC promedio para métricas cube-wise...\n",
      "Guardada curva ROC promedio cube-wise en trained_models/mapas_combinados_pipe1_v01_pipe2_1dhzmigz_test2_cube8/roc_curve_average_cube.png\n",
      "\n",
      "Accuracy Global Cube-wise: 0.9896 ± 0.0046\n",
      "\n",
      "Distancia entre centros Global: 0.7077 ± 0.4363\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, f1_score\n",
    "from scipy import stats\n",
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import torch\n",
    "\n",
    "# Función para calcular métricas (ya definida previamente)\n",
    "def calculate_metrics(pred, true, prob_maps=None, num_classes=3):\n",
    "    dice_scores = []\n",
    "    sensitivity_scores = []\n",
    "    precision_scores = []\n",
    "    auc_scores = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    accuracy = accuracy_score(true.flatten(), pred.flatten())\n",
    "    \n",
    "    for cls in range(num_classes):\n",
    "        pred_cls = (pred == cls).astype(np.uint8)\n",
    "        true_cls = (true == cls).astype(np.uint8)\n",
    "        \n",
    "        tp = np.sum(pred_cls * true_cls)\n",
    "        fp = np.sum(pred_cls * (1 - true_cls))\n",
    "        fn = np.sum((1 - pred_cls) * true_cls)\n",
    "        \n",
    "        dice = 2 * tp / (2 * tp + fp + fn + 1e-6)\n",
    "        dice_scores.append(dice)\n",
    "        \n",
    "        sensitivity = tp / (tp + fn + 1e-6)\n",
    "        sensitivity_scores.append(sensitivity)\n",
    "        \n",
    "        precision = tp / (tp + fp + 1e-6)\n",
    "        precision_scores.append(precision)\n",
    "        \n",
    "        f1 = f1_score(true_cls.flatten(), pred_cls.flatten(), zero_division=0)\n",
    "        f1_scores.append(f1)\n",
    "        \n",
    "        if prob_maps is not None:\n",
    "            try:\n",
    "                auc = roc_auc_score(true_cls.flatten(), prob_maps[cls].flatten())\n",
    "                auc_scores.append(auc)\n",
    "            except ValueError:\n",
    "                auc_scores.append(np.nan)\n",
    "        else:\n",
    "            auc_scores.append(np.nan)\n",
    "    \n",
    "    return dice_scores, sensitivity_scores, precision_scores, auc_scores, accuracy, f1_scores\n",
    "\n",
    "# Función para dividir en cubos y obtener clases predominantes\n",
    "def get_cube_labels(volume, cube_size, num_classes=3):\n",
    "    dims = volume.shape\n",
    "    assert dims[0] % cube_size == 0, \"El tamaño del cubo debe dividir exactamente el tamaño del volumen\"\n",
    "    num_cubes = dims[0] // cube_size\n",
    "    \n",
    "    cube_labels = np.zeros((num_cubes, num_cubes, num_cubes), dtype=np.uint8)\n",
    "    cube_probs = np.zeros((num_classes, num_cubes, num_cubes, num_cubes))\n",
    "    \n",
    "    for i in range(num_cubes):\n",
    "        for j in range(num_cubes):\n",
    "            for k in range(num_cubes):\n",
    "                cube = volume[i*cube_size:(i+1)*cube_size, \n",
    "                             j*cube_size:(j+1)*cube_size, \n",
    "                             k*cube_size:(k+1)*cube_size]\n",
    "                # Clase predominante (modo)\n",
    "                mode_value = stats.mode(cube.flatten(), keepdims=True)[0][0]\n",
    "                cube_labels[i, j, k] = mode_value\n",
    "                # Proporción de cada clase como \"probabilidad\" suavizada\n",
    "                for cls in range(num_classes):\n",
    "                    cube_probs[cls, i, j, k] = np.mean(cube == cls)\n",
    "    \n",
    "    return cube_labels, cube_probs\n",
    "\n",
    "# Listas para métricas voxel-wise\n",
    "all_dice = {0: [], 1: [], 2: []}\n",
    "all_sensitivity = {0: [], 1: [], 2: []}\n",
    "all_precision = {0: [], 1: [], 2: []}\n",
    "all_auc = {0: [], 1: [], 2: []}\n",
    "all_accuracy = []\n",
    "all_f1 = {0: [], 1: [], 2: []}\n",
    "# Listas para almacenar FPR y TPR de todos los casos\n",
    "all_fpr = {0: [], 1: [], 2: []}\n",
    "all_tpr = {0: [], 1: [], 2: []}\n",
    "\n",
    "# Listas para métricas cube-wise\n",
    "all_dice_cube = {0: [], 1: [], 2: []}\n",
    "all_sensitivity_cube = {0: [], 1: [], 2: []}\n",
    "all_precision_cube = {0: [], 1: [], 2: []}\n",
    "all_auc_cube = {0: [], 1: [], 2: []}\n",
    "all_accuracy_cube = []\n",
    "all_f1_cube = {0: [], 1: [], 2: []}\n",
    "all_center_distance=[]\n",
    "# Listas para almacenar FPR y TPR de todos los casos\n",
    "all_fpr_cube = {0: [], 1: [], 2: []}\n",
    "all_tpr_cube = {0: [], 1: [], 2: []}\n",
    "\n",
    "# Tamaño del cubo (ajusta según necesites)\n",
    "cube_size = 8  # 128 / 16 = 8 cubos por dimensión\n",
    "\n",
    "# Procesar y combinar\n",
    "for idx, ((embeddings1, labels1), (embeddings2, labels2)) in enumerate(zip(loader_model1, loader_model2)):\n",
    "    # Generar mapas de probabilidad para ambos modelos\n",
    "    prob_maps1 = generate_probability_maps(embeddings1, projection_head1, classifier1, device)  # [3, 128, 128, 128]\n",
    "    prob_maps2 = generate_probability_maps(embeddings2, projection_head2, classifier2, device)  # [3, 128, 128, 128]\n",
    "    \n",
    "    # Combinar mapas\n",
    "    combined_prob_maps = torch.zeros_like(prob_maps1)\n",
    "    combined_prob_maps[0] = prob_maps1[0]\n",
    "    combined_prob_maps[1] = torch.max(prob_maps1[1], prob_maps2[1])\n",
    "    combined_prob_maps[2] = prob_maps2[2]\n",
    "    \n",
    "    # Convertir a numpy\n",
    "    prob_maps_np = combined_prob_maps.cpu().numpy()  # [3, 128, 128, 128]\n",
    "    prob_maps_np_nifti = np.transpose(prob_maps_np, (1, 2, 3, 0))  # [128, 128, 128, 3]\n",
    "    \n",
    "    # Generar segmentación semántica\n",
    "    segmentation = np.argmax(prob_maps_np, axis=0)\n",
    "    # Colocar nuevo umbral para infiltracion\n",
    "    # class_1_mask = prob_maps_np[1] > 0.4\n",
    "    # segmentation[class_1_mask] = 1\n",
    "    segmentation_np = segmentation.astype(np.uint8)\n",
    "    \n",
    "    # Etiquetas\n",
    "    labels = labels2.squeeze(0)\n",
    "    labels_np = labels.cpu().numpy().astype(np.uint8)\n",
    "    \n",
    "    # Calcular métricas voxel-wise\n",
    "    dice, sensitivity, precision, auc, accuracy, f1 = calculate_metrics(segmentation_np, labels_np, prob_maps=prob_maps_np)\n",
    "    \n",
    "    # Almacenar métricas voxel-wise\n",
    "    for cls in range(3):\n",
    "        all_dice[cls].append(dice[cls])\n",
    "        all_sensitivity[cls].append(sensitivity[cls])\n",
    "        all_precision[cls].append(precision[cls])\n",
    "        all_auc[cls].append(auc[cls])\n",
    "        all_f1[cls].append(f1[cls])\n",
    "    all_accuracy.append(accuracy)\n",
    "\n",
    "    # Graficar curvas ROC\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    class_names = [\"Fondo\", \"Vasogénico\", \"Infiltrado\"]\n",
    "    colors = ['blue', 'green', 'red']\n",
    "    \n",
    "    for cls in range(3):\n",
    "        # Etiquetas binarias para la clase actual\n",
    "        true_cls = (labels_np == cls).astype(np.uint8).flatten()\n",
    "        prob_cls = prob_maps_np[cls].flatten()\n",
    "        \n",
    "        # Calcular puntos de la curva ROC\n",
    "        fpr, tpr, _ = roc_curve(true_cls, prob_cls)\n",
    "        auc_value = auc[cls]  # Usar el AUC calculado previamente\n",
    "        all_fpr[cls].append(fpr)\n",
    "        all_tpr[cls].append(tpr)\n",
    "        \n",
    "        # Graficar\n",
    "        plt.plot(fpr, tpr, color=colors[cls], label=f'{class_names[cls]} (AUC = {auc_value:.4f})')\n",
    "    \n",
    "    # Configurar el gráfico\n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # Línea diagonal (clasificador aleatorio)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Tasa de Falsos Positivos (FPR)')\n",
    "    plt.ylabel('Tasa de Verdaderos Positivos (TPR)')\n",
    "    plt.title(f'Curva ROC - Caso {idx}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    # Guardar el gráfico\n",
    "    roc_output_path = os.path.join(output_dir, f\"roc_curve_case_{idx}.png\")\n",
    "    plt.savefig(roc_output_path)\n",
    "    plt.close()\n",
    "    print(f\"Guardada curva ROC en {roc_output_path}\")\n",
    "    \n",
    "    ################################\n",
    "    # Evaluación basada en cubos\n",
    "    ####################################\n",
    "    pred_cube_labels, pred_cube_probs = get_cube_labels(segmentation_np, cube_size)\n",
    "    true_cube_labels, true_cube_probs = get_cube_labels(labels_np, cube_size)\n",
    "    \n",
    "    # Calcular métricas cube-wise\n",
    "    dice_cube, sensitivity_cube, precision_cube, auc_cube, accuracy_cube, f1_cube = calculate_metrics(\n",
    "        pred_cube_labels, true_cube_labels, prob_maps=pred_cube_probs\n",
    "    )\n",
    "    \n",
    "    # Almacenar métricas cube-wise\n",
    "    for cls in range(3):\n",
    "        all_dice_cube[cls].append(dice_cube[cls])\n",
    "        all_sensitivity_cube[cls].append(sensitivity_cube[cls])\n",
    "        all_precision_cube[cls].append(precision_cube[cls])\n",
    "        all_auc_cube[cls].append(auc_cube[cls])\n",
    "        all_f1_cube[cls].append(f1_cube[cls])\n",
    "    all_accuracy_cube.append(accuracy_cube)\n",
    "\n",
    "    # ***** NUEVO: Calcular y graficar curvas ROC para métricas cube-wise *****\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    class_names = [\"Fondo\", \"Vasogénico\", \"Infiltrado\"]\n",
    "    colors = ['blue', 'green', 'red']\n",
    "    \n",
    "    for cls in range(3):\n",
    "        # Etiquetas binarias para la clase actual (cube-wise)\n",
    "        true_cls_cube = (true_cube_labels == cls).astype(np.uint8).flatten()\n",
    "        prob_cls_cube = pred_cube_probs[cls].flatten()\n",
    "        \n",
    "        # Calcular puntos de la curva ROC\n",
    "        fpr_cube, tpr_cube, _ = roc_curve(true_cls_cube, prob_cls_cube)\n",
    "        auc_value_cube = auc_cube[cls]  # Usar el AUC calculado previamente\n",
    "        all_fpr_cube[cls].append(fpr_cube)\n",
    "        all_tpr_cube[cls].append(tpr_cube)\n",
    "        \n",
    "        # Graficar\n",
    "        plt.plot(fpr_cube, tpr_cube, color=colors[cls], label=f'{class_names[cls]} (AUC = {auc_value_cube:.4f})')\n",
    "    \n",
    "    # Configurar el gráfico\n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # Línea diagonal (clasificador aleatorio)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Tasa de Falsos Positivos (FPR)')\n",
    "    plt.ylabel('Tasa de Verdaderos Positivos (TPR)')\n",
    "    plt.title(f'Curva ROC Cube-wise - Caso {idx}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    # Guardar el gráfico\n",
    "    roc_cube_output_path = os.path.join(output_dir, f\"roc_curve_cube_case_{idx}.png\")\n",
    "    plt.savefig(roc_cube_output_path)\n",
    "    plt.close()\n",
    "    print(f\"Guardada curva ROC cube-wise en {roc_cube_output_path}\")\n",
    "    \n",
    "    # Mapa de coincidencias/discrepancias\n",
    "    match_map = (pred_cube_labels == true_cube_labels).astype(np.uint8)\n",
    "    mismatch_map = (pred_cube_labels != true_cube_labels).astype(np.uint8)\n",
    "    \n",
    "    # Guardar mapas de cubos y coincidencias\n",
    "    affine = np.eye(4) * cube_size  # Ajustar el affine para reflejar el tamaño del cubo\n",
    "    nib.save(nib.Nifti1Image(pred_cube_labels, affine), os.path.join(output_dir, f\"pred_cube_labels_case_{idx}.nii.gz\"))\n",
    "    nib.save(nib.Nifti1Image(true_cube_labels, affine), os.path.join(output_dir, f\"true_cube_labels_case_{idx}.nii.gz\"))\n",
    "    nib.save(nib.Nifti1Image(match_map, affine), os.path.join(output_dir, f\"match_map_case_{idx}.nii.gz\"))\n",
    "    nib.save(nib.Nifti1Image(mismatch_map, affine), os.path.join(output_dir, f\"mismatch_map_case_{idx}.nii.gz\"))\n",
    "    \n",
    "    # Análisis espacial (centro de masa de la clase Infiltrado)\n",
    "    infiltrado_pred = (pred_cube_labels == 1).astype(np.uint8)\n",
    "    infiltrado_true = (true_cube_labels == 1).astype(np.uint8)\n",
    "    if np.sum(infiltrado_pred) > 0:\n",
    "        pred_center = np.mean(np.where(infiltrado_pred), axis=1)\n",
    "    else:\n",
    "        pred_center = np.array([np.nan, np.nan, np.nan])\n",
    "    if np.sum(infiltrado_true) > 0:\n",
    "        true_center = np.mean(np.where(infiltrado_true), axis=1)\n",
    "    else:\n",
    "        true_center = np.array([np.nan, np.nan, np.nan])\n",
    "    distance = np.linalg.norm(pred_center - true_center) if not np.any(np.isnan(pred_center)) and not np.any(np.isnan(true_center)) else np.nan\n",
    "    all_center_distance.append(distance)\n",
    "\n",
    "    print(f\"Caso {idx} - Voxel-wise:\")\n",
    "    print(f\"  Dice: {dice}, Sensitivity: {sensitivity}, Precision: {precision}, AUC-ROC: {auc}, \"\n",
    "          f\"Accuracy: {accuracy:.4f}, F1 Score: {f1}\")\n",
    "    print(f\"Caso {idx} - Cube-wise (tamaño {cube_size}):\")\n",
    "    print(f\"  Dice: {dice_cube}, Sensitivity: {sensitivity_cube}, Precision: {precision_cube}, \"\n",
    "          f\"AUC-ROC: {auc_cube}, Accuracy: {accuracy_cube:.4f}, F1 Score: {f1_cube}\")\n",
    "    print(f\"  Centro de masa Infiltrado (Pred): {pred_center}\")\n",
    "    print(f\"  Centro de masa Infiltrado (True): {true_center}\")\n",
    "    print(f\"Distancia entre centros: {distance}\")\n",
    "    \n",
    "    # Guardar mapas de probabilidad y segmentaciones voxel-wise\n",
    "    nib.save(nib.Nifti1Image(prob_maps_np_nifti, np.eye(4)), os.path.join(output_dir, f\"probability_maps_case_{idx}.nii.gz\"))\n",
    "    nib.save(nib.Nifti1Image(labels_np, np.eye(4)), os.path.join(output_dir, f\"labels_case_{idx}.nii.gz\"))\n",
    "    nib.save(nib.Nifti1Image(segmentation_np, np.eye(4)), os.path.join(output_dir, f\"segmentation_case_{idx}.nii.gz\"))\n",
    "\n",
    "# Calcular promedios y desviaciones estándar (voxel-wise)\n",
    "class_names = [\"Fondo\", \"Infiltrado\", \"Vasogénico\", ]\n",
    "print(\"\\nResultados Voxel-wise:\")\n",
    "for cls in range(3):\n",
    "    dice_mean = np.nanmean(all_dice[cls])\n",
    "    dice_std = np.nanstd(all_dice[cls])\n",
    "    sens_mean = np.nanmean(all_sensitivity[cls])\n",
    "    sens_std = np.nanstd(all_sensitivity[cls])\n",
    "    prec_mean = np.nanmean(all_precision[cls])\n",
    "    prec_std = np.nanstd(all_precision[cls])\n",
    "    auc_mean = np.nanmean(all_auc[cls])\n",
    "    auc_std = np.nanstd(all_auc[cls])\n",
    "    f1_mean = np.nanmean(all_f1[cls])\n",
    "    f1_std = np.nanstd(all_f1[cls])\n",
    "    \n",
    "    print(f\"\\nClase {cls} ({class_names[cls]}):\")\n",
    "    print(f\"  Dice: {dice_mean:.4f} ± {dice_std:.4f}\")\n",
    "    print(f\"  Sensibilidad: {sens_mean:.4f} ± {sens_std:.4f}\")\n",
    "    print(f\"  Precisión: {prec_mean:.4f} ± {prec_std:.4f}\")\n",
    "    print(f\"  AUC-ROC: {auc_mean:.4f} ± {auc_std:.4f}\")\n",
    "    print(f\"  F1 Score: {f1_mean:.4f} ± {f1_std:.4f}\")\n",
    "\n",
    "accuracy_mean = np.nanmean(all_accuracy)\n",
    "accuracy_std = np.nanstd(all_accuracy)\n",
    "print(f\"\\nAccuracy Global: {accuracy_mean:.4f} ± {accuracy_std:.4f}\")\n",
    "\n",
    "# Curva ROC Promedio\n",
    "plt.figure(figsize=(8, 6))\n",
    "for cls in range(3):\n",
    "    # Interpolar FPR y TPR a una base común\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    tprs = []\n",
    "    for fpr, tpr in zip(all_fpr[cls], all_tpr[cls]):\n",
    "        tpr_interp = np.interp(mean_fpr, fpr, tpr)\n",
    "        tpr_interp[0] = 0.0  # Asegurar que comienza en 0\n",
    "        tprs.append(tpr_interp)\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0  # Asegurar que termina en 1\n",
    "    mean_auc = np.nanmean(all_auc[cls])\n",
    "    \n",
    "    plt.plot(mean_fpr, mean_tpr, color=colors[cls], label=f'{class_names[cls]} (AUC = {mean_auc:.4f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Tasa de Falsos Positivos (FPR)')\n",
    "plt.ylabel('Tasa de Verdaderos Positivos (TPR)')\n",
    "plt.title('Curva ROC Promedio')\n",
    "plt.legend(loc=\"lower right\")\n",
    "roc_avg_path = os.path.join(output_dir, \"roc_curve_average.png\")\n",
    "plt.savefig(roc_avg_path)\n",
    "plt.close()\n",
    "print(f\"Guardada curva ROC promedio en {roc_avg_path}\")\n",
    "\n",
    "# Calcular promedios y desviaciones estándar (cube-wise)\n",
    "print(\"\\nResultados Cube-wise:\")\n",
    "for cls in range(3):\n",
    "    dice_mean = np.nanmean(all_dice_cube[cls])\n",
    "    dice_std = np.nanstd(all_dice_cube[cls])\n",
    "    sens_mean = np.nanmean(all_sensitivity_cube[cls])\n",
    "    sens_std = np.nanstd(all_sensitivity_cube[cls])\n",
    "    prec_mean = np.nanmean(all_precision_cube[cls])\n",
    "    prec_std = np.nanstd(all_precision_cube[cls])\n",
    "    auc_mean = np.nanmean(all_auc_cube[cls])\n",
    "    auc_std = np.nanstd(all_auc_cube[cls])\n",
    "    f1_mean = np.nanmean(all_f1_cube[cls])\n",
    "    f1_std = np.nanstd(all_f1_cube[cls])\n",
    "    \n",
    "    print(f\"\\nClase {cls} ({class_names[cls]}):\")\n",
    "    print(f\"  Dice: {dice_mean:.4f} ± {dice_std:.4f}\")\n",
    "    print(f\"  Sensibilidad: {sens_mean:.4f} ± {sens_std:.4f}\")\n",
    "    print(f\"  Precisión: {prec_mean:.4f} ± {prec_std:.4f}\")\n",
    "    print(f\"  AUC-ROC: {auc_mean:.4f} ± {auc_std:.4f}\")\n",
    "    print(f\"  F1 Score: {f1_mean:.4f} ± {f1_std:.4f}\")\n",
    "\n",
    "# ***** NUEVO: Curva ROC promedio para métricas cube-wise *****\n",
    "print(\"\\nGenerando curva ROC promedio para métricas cube-wise...\")\n",
    "plt.figure(figsize=(8, 6))\n",
    "for cls in range(3):\n",
    "    # Interpolar FPR y TPR a una base común\n",
    "    mean_fpr_cube = np.linspace(0, 1, 100)\n",
    "    tprs_cube = []\n",
    "    for fpr, tpr in zip(all_fpr_cube[cls], all_tpr_cube[cls]):\n",
    "        tpr_interp = np.interp(mean_fpr_cube, fpr, tpr)\n",
    "        tpr_interp[0] = 0.0  # Asegurar que comienza en 0\n",
    "        tprs_cube.append(tpr_interp)\n",
    "    mean_tpr_cube = np.mean(tprs_cube, axis=0)\n",
    "    mean_tpr_cube[-1] = 1.0  # Asegurar que termina en 1\n",
    "    mean_auc_cube = np.nanmean(all_auc_cube[cls])\n",
    "    \n",
    "    plt.plot(mean_fpr_cube, mean_tpr_cube, color=colors[cls], label=f'{class_names[cls]} (AUC = {mean_auc_cube:.4f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Tasa de Falsos Positivos (FPR)')\n",
    "plt.ylabel('Tasa de Verdaderos Positivos (TPR)')\n",
    "plt.title('Curva ROC Promedio Cube-wise')\n",
    "plt.legend(loc=\"lower right\")\n",
    "roc_avg_cube_path = os.path.join(output_dir, \"roc_curve_average_cube.png\")\n",
    "plt.savefig(roc_avg_cube_path)\n",
    "plt.close()\n",
    "print(f\"Guardada curva ROC promedio cube-wise en {roc_avg_cube_path}\")\n",
    "\n",
    "accuracy_mean_cube = np.nanmean(all_accuracy_cube)\n",
    "accuracy_std_cube = np.nanstd(all_accuracy_cube)\n",
    "print(f\"\\nAccuracy Global Cube-wise: {accuracy_mean_cube:.4f} ± {accuracy_std_cube:.4f}\")\n",
    "\n",
    "dist_mean = np.nanmean(all_center_distance)\n",
    "dist_std = np.nanstd(all_center_distance)\n",
    "print(f\"\\nDistancia entre centros Global: {dist_mean:.4f} ± {dist_std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metricas basadas en regiones centrada en tumor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caso 0 - Voxel-wise:\n",
      "  Dice: [0.9970035703989371, 0.520193908666654, 0.6986080252122276], Sensitivity: [0.9941729438903477, 0.4651015729859273, 0.8572294069472858], Precision: [0.9998503617499696, 0.5900915958885441, 0.5895229395928157], AUC-ROC: [0.9997442857524647, 0.9908721550503753, 0.9926824584823732], Accuracy: 0.9818, F1 Score: [0.9970035703991849, 0.5201939086743189, 0.698608025218861]\n",
      "Caso 0 - Cube-wise (Grilla 5x5x5):\n",
      "  Dice: [0.9290322520707597, 0.5499999862500004, 0.6545454426446283], Sensitivity: [0.8780487697798931, 0.49999997727272827, 0.8571428163265326], Precision: [0.9863013563520362, 0.6111110771604957, 0.5294117491349486], AUC-ROC: [0.994895065229722, 0.823477493380406, 0.8811813186813187], Accuracy: 0.8080, F1 Score: [0.9290322580645162, 0.55, 0.6545454545454545]\n",
      "  Centro de masa Infiltrado (Pred): [1.72222222 2.77777778 1.83333333]\n",
      "  Centro de masa Infiltrado (True): [2.40909091 2.         1.54545455]\n",
      "  Distancia entre centros: 1.08\n",
      "Caso 1 - Voxel-wise:\n",
      "  Dice: [0.9961940335339866, 0.7573532532253574, 0.7093981720864045], Sensitivity: [0.9927684989178468, 0.8805763915435673, 0.8734924409345383], Precision: [0.9996432895239844, 0.6643830515552622, 0.5972068985367515], AUC-ROC: [0.9982288768831301, 0.9987986678355506, 0.9947559103374306], Accuracy: 0.9909, F1 Score: [0.9961940335342289, 0.7573532532575018, 0.7093981720986376]\n",
      "Caso 1 - Cube-wise (Grilla 5x5x5):\n",
      "  Dice: [0.9763033129085151, 0.8571427959183717, 0.7999999680000013], Sensitivity: [0.9537036948731139, 0.9999998333333611, 0.9090908264462886], Precision: [0.9999999902912623, 0.7499999062500118, 0.7142856632653098], AUC-ROC: [0.9929193899782134, 1.0, 0.9824561403508772], Accuracy: 0.9520, F1 Score: [0.976303317535545, 0.8571428571428571, 0.8]\n",
      "  Centro de masa Infiltrado (Pred): [2.75  3.    1.875]\n",
      "  Centro de masa Infiltrado (True): [3.         2.83333333 2.        ]\n",
      "  Distancia entre centros: 0.33\n",
      "Caso 2 - Voxel-wise:\n",
      "  Dice: [0.9954207128587987, 0.7141549611409692, 0.8115478311414778], Sensitivity: [0.9918475049000461, 0.9249993951340334, 0.7700496806169659], Precision: [0.9990197594230913, 0.5815877941468294, 0.8577734344881823], AUC-ROC: [0.9992538722066036, 0.9960538811731321, 0.9950507396812097], Accuracy: 0.9798, F1 Score: [0.9954207128590542, 0.714154961147639, 0.8115478311456931]\n",
      "Caso 2 - Cube-wise (Grilla 5x5x5):\n",
      "  Dice: [0.9902912573286833, 0.7058823114186875, 0.8148147846364894], Sensitivity: [0.9807692213387575, 0.857142734693895, 0.7857142295918408], Precision: [0.9999999901960785, 0.599999940000006, 0.8461537810650938], AUC-ROC: [0.9995421245421247, 0.9842615012106537, 0.9478764478764479], Accuracy: 0.9520, F1 Score: [0.9902912621359222, 0.7058823529411764, 0.8148148148148148]\n",
      "  Centro de masa Infiltrado (Pred): [2.6 1.6 2. ]\n",
      "  Centro de masa Infiltrado (True): [2.57142857 1.85714286 1.57142857]\n",
      "  Distancia entre centros: 0.50\n",
      "Caso 3 - Voxel-wise:\n",
      "  Dice: [0.9946954256219234, 0.6495593571237637, 0.3871328671230227], Sensitivity: [0.9896221671657137, 0.5427375107503988, 0.9631785396731395], Precision: [0.9998209678212706, 0.8087349395850397, 0.2422506524014305], AUC-ROC: [0.9982360359904578, 0.9980275841103045, 0.9913611417658387], Accuracy: 0.9880, F1 Score: [0.9946954256221634, 0.649559357179886, 0.38713286713286715]\n",
      "Caso 3 - Cube-wise (Grilla 5x5x5):\n",
      "  Dice: [0.9680365252601072, 0.599999940000006, 0.47619045351474026], Sensitivity: [0.938053089043778, 0.4285713673469475, 0.99999980000004], Precision: [0.9999999905660378, 0.9999996666667778, 0.3124999804687512], AUC-ROC: [0.9918879056047198, 0.891041162227603, 0.9866666666666667], Accuracy: 0.9120, F1 Score: [0.9680365296803652, 0.6, 0.47619047619047616]\n",
      "  Centro de masa Infiltrado (Pred): [2.33333333 3.         3.33333333]\n",
      "  Centro de masa Infiltrado (True): [1.85714286 2.71428571 3.        ]\n",
      "  Distancia entre centros: 0.65\n",
      "Caso 4 - Voxel-wise:\n",
      "  Dice: [0.9965279044150087, 0.7510474573280651, 0.6372424828180269], Sensitivity: [0.9930836863287986, 0.8135382059318024, 0.8549704811941695], Precision: [0.9999960961570087, 0.6974721529577604, 0.5078999574710252], AUC-ROC: [0.9996016155347753, 0.9980592234123471, 0.9952250280934484], Accuracy: 0.9905, F1 Score: [0.996527904415251, 0.7510474573486321, 0.6372424828321889]\n",
      "Caso 4 - Cube-wise (Grilla 5x5x5):\n",
      "  Dice: [0.9644670001803706, 0.8148147846364894, 0.7692307396449716], Sensitivity: [0.9313725398885045, 0.9166665902777842, 0.9090908264462886], Precision: [0.9999999894736844, 0.7333332844444478, 0.6666666222222252], AUC-ROC: [0.9978687127024722, 0.9874631268436578, 0.9880382775119617], Accuracy: 0.9280, F1 Score: [0.9644670050761421, 0.8148148148148148, 0.7692307692307692]\n",
      "  Centro de masa Infiltrado (Pred): [1.73333333 2.06666667 1.53333333]\n",
      "  Centro de masa Infiltrado (True): [2.         2.08333333 1.5       ]\n",
      "  Distancia entre centros: 0.27\n",
      "Caso 5 - Voxel-wise:\n",
      "  Dice: [0.9969356400590003, 0.7873596864002423, 0.6544481470151394], Sensitivity: [0.9939036829084408, 0.7655546205819543, 0.8678748381525744], Precision: [0.9999861521116298, 0.8104433032694811, 0.5252735377407733], AUC-ROC: [0.9998858128010235, 0.997705949643649, 0.9948220587097338], Accuracy: 0.9882, F1 Score: [0.996935640059246, 0.7873596864102629, 0.6544481470261537]\n",
      "Caso 5 - Cube-wise (Grilla 5x5x5):\n",
      "  Dice: [0.9585798759847345, 0.8461538298816571, 0.689655148632581], Sensitivity: [0.9204545349948349, 0.8148147846364894, 0.99999990000001], Precision: [0.9999999876543212, 0.8799999648000014, 0.5263157617728547], AUC-ROC: [0.9993857493857494, 0.9914965986394558, 0.9895652173913043], Accuracy: 0.9040, F1 Score: [0.9585798816568047, 0.8461538461538461, 0.6896551724137931]\n",
      "  Centro de masa Infiltrado (Pred): [2.56 1.92 2.2 ]\n",
      "  Centro de masa Infiltrado (True): [2.37037037 2.07407407 1.96296296]\n",
      "  Distancia entre centros: 0.34\n",
      "\n",
      "Resultados Voxel-wise:\n",
      "\n",
      "Clase 0 (Fondo):\n",
      "  Dice: 0.9961 ± 0.0008\n",
      "  Sensibilidad: 0.9926 ± 0.0015\n",
      "  Precisión: 0.9997 ± 0.0003\n",
      "  AUC-ROC: 0.9992 ± 0.0007\n",
      "  F1 Score: 0.9961 ± 0.0008\n",
      "\n",
      "Clase 1 (Vasogénico):\n",
      "  Dice: 0.6966 ± 0.0899\n",
      "  Sensibilidad: 0.7321 ± 0.1704\n",
      "  Precisión: 0.6921 ± 0.0922\n",
      "  AUC-ROC: 0.9966 ± 0.0027\n",
      "  F1 Score: 0.6966 ± 0.0899\n",
      "\n",
      "Clase 2 (Infiltrado):\n",
      "  Dice: 0.6497 ± 0.1299\n",
      "  Sensibilidad: 0.8645 ± 0.0561\n",
      "  Precisión: 0.5533 ± 0.1805\n",
      "  AUC-ROC: 0.9940 ± 0.0014\n",
      "  F1 Score: 0.6497 ± 0.1299\n",
      "\n",
      "Accuracy Global: 0.9865 ± 0.0042\n",
      "\n",
      "Resultados Cube-wise (Grilla 5x5x5):\n",
      "\n",
      "Clase 0 (Fondo):\n",
      "  Dice: 0.9645 ± 0.0188\n",
      "  Sensibilidad: 0.9337 ± 0.0314\n",
      "  Precisión: 0.9977 ± 0.0051\n",
      "  AUC-ROC: 0.9961 ± 0.0030\n",
      "  F1 Score: 0.9645 ± 0.0188\n",
      "\n",
      "Clase 1 (Vasogénico):\n",
      "  Dice: 0.7290 ± 0.1202\n",
      "  Sensibilidad: 0.7529 ± 0.2128\n",
      "  Precisión: 0.7624 ± 0.1417\n",
      "  AUC-ROC: 0.9463 ± 0.0661\n",
      "  F1 Score: 0.7290 ± 0.1202\n",
      "\n",
      "Clase 2 (Infiltrado):\n",
      "  Dice: 0.7007 ± 0.1156\n",
      "  Sensibilidad: 0.9102 ± 0.0758\n",
      "  Precisión: 0.5992 ± 0.1689\n",
      "  AUC-ROC: 0.9626 ± 0.0391\n",
      "  F1 Score: 0.7007 ± 0.1156\n",
      "\n",
      "Accuracy Global Cube-wise: 0.9093 ± 0.0488\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, f1_score\n",
    "from scipy import stats\n",
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import torch\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Función para calcular métricas (para todas las clases o un subconjunto)\n",
    "def calculate_metrics(pred, true, prob_maps=None, num_classes=3, class_offset=0):\n",
    "    dice_scores = []\n",
    "    sensitivity_scores = []\n",
    "    precision_scores = []\n",
    "    auc_scores = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    accuracy = accuracy_score(true.flatten(), pred.flatten())\n",
    "    \n",
    "    for cls in range(class_offset, class_offset + num_classes):\n",
    "        pred_cls = (pred == cls).astype(np.uint8)\n",
    "        true_cls = (true == cls).astype(np.uint8)\n",
    "        \n",
    "        tp = np.sum(pred_cls * true_cls)\n",
    "        fp = np.sum(pred_cls * (1 - true_cls))\n",
    "        fn = np.sum((1 - pred_cls) * true_cls)\n",
    "        \n",
    "        dice = 2 * tp / (2 * tp + fp + fn + 1e-6)\n",
    "        dice_scores.append(dice)\n",
    "        \n",
    "        sensitivity = tp / (tp + fn + 1e-6)\n",
    "        sensitivity_scores.append(sensitivity)\n",
    "        \n",
    "        precision = tp / (tp + fp + 1e-6)\n",
    "        precision_scores.append(precision)\n",
    "        \n",
    "        f1 = f1_score(true_cls.flatten(), pred_cls.flatten(), zero_division=0)\n",
    "        f1_scores.append(f1)\n",
    "        \n",
    "        if prob_maps is not None:\n",
    "            try:\n",
    "                auc = roc_auc_score(true_cls.flatten(), prob_maps[cls - class_offset].flatten())\n",
    "                auc_scores.append(auc)\n",
    "            except ValueError:\n",
    "                auc_scores.append(np.nan)\n",
    "        else:\n",
    "            auc_scores.append(np.nan)\n",
    "    \n",
    "    return dice_scores, sensitivity_scores, precision_scores, auc_scores, accuracy, f1_scores\n",
    "\n",
    "# Función para obtener el cuadro delimitador y centro del tumor\n",
    "def get_tumor_bbox_and_center(labels_np):\n",
    "    tumor_mask = (labels_np > 0).astype(np.uint8)  # Clases 1 (Infiltrado) y 2 (Vasogénico)\n",
    "    if np.sum(tumor_mask) == 0:\n",
    "        return None, None\n",
    "    indices = np.where(tumor_mask)\n",
    "    bbox = {\n",
    "        'x_min': np.min(indices[0]), 'x_max': np.max(indices[0]),\n",
    "        'y_min': np.min(indices[1]), 'y_max': np.max(indices[1]),\n",
    "        'z_min': np.min(indices[2]), 'z_max': np.max(indices[2])\n",
    "    }\n",
    "    center = np.mean(indices, axis=1)\n",
    "    return bbox, center\n",
    "\n",
    "# Función para dividir la región del tumor en cubos\n",
    "def get_tumor_cube_labels(segmentation_np, labels_np, bbox, center, grid_size=5):\n",
    "    # Dimensiones del cuadro delimitador\n",
    "    dims = [bbox['x_max'] - bbox['x_min'] + 1, \n",
    "            bbox['y_max'] - bbox['y_min'] + 1, \n",
    "            bbox['z_max'] - bbox['z_min'] + 1]\n",
    "    \n",
    "    # Tamaño del cubo (dinámico)\n",
    "    cube_size = [max(1, d // grid_size) for d in dims]\n",
    "    \n",
    "    # Ajustar el cuadro delimitador para centrarlo y cubrir la grilla\n",
    "    half_grid = np.array([grid_size * cs / 2 for cs in cube_size])\n",
    "    bbox_min = np.floor(center - half_grid).astype(int)\n",
    "    bbox_max = np.ceil(center + half_grid).astype(int)\n",
    "    \n",
    "    # Asegurar que no exceda los límites del volumen\n",
    "    bbox_min = np.maximum(bbox_min, 0)\n",
    "    bbox_max = np.minimum(bbox_max, np.array(segmentation_np.shape))\n",
    "    \n",
    "    # Extraer la región del tumor\n",
    "    tumor_pred = segmentation_np[bbox_min[0]:bbox_max[0], bbox_min[1]:bbox_max[1], bbox_min[2]:bbox_max[2]]\n",
    "    tumor_true = labels_np[bbox_min[0]:bbox_max[0], bbox_min[1]:bbox_max[1], bbox_min[2]:bbox_max[2]]\n",
    "    \n",
    "    # Ajustar para que coincida con la grilla\n",
    "    cube_labels_pred = np.zeros((grid_size, grid_size, grid_size), dtype=np.uint8)\n",
    "    cube_labels_true = np.zeros((grid_size, grid_size, grid_size), dtype=np.uint8)\n",
    "    cube_probs_pred = np.zeros((3, grid_size, grid_size, grid_size))  # Clases 0, 1, 2\n",
    "    \n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            for k in range(grid_size):\n",
    "                x_start = i * cube_size[0]\n",
    "                y_start = j * cube_size[1]\n",
    "                z_start = k * cube_size[2]\n",
    "                x_end = min(x_start + cube_size[0], tumor_pred.shape[0])\n",
    "                y_end = min(y_start + cube_size[1], tumor_pred.shape[1])\n",
    "                z_end = min(z_start + cube_size[2], tumor_pred.shape[2])\n",
    "                \n",
    "                if x_end <= x_start or y_end <= y_start or z_end <= z_start:\n",
    "                    continue\n",
    "                \n",
    "                cube_pred = tumor_pred[x_start:x_end, y_start:y_end, z_start:z_end]\n",
    "                cube_true = tumor_true[x_start:x_end, y_start:y_end, z_start:z_end]\n",
    "                \n",
    "                # Clase predominante (incluye clase 0)\n",
    "                cube_labels_pred[i, j, k] = stats.mode(cube_pred.flatten(), keepdims=True)[0][0]\n",
    "                cube_labels_true[i, j, k] = stats.mode(cube_true.flatten(), keepdims=True)[0][0]\n",
    "                \n",
    "                # Proporciones para clases 0, 1, 2\n",
    "                cube_probs_pred[0, i, j, k] = np.mean(cube_pred == 0)\n",
    "                cube_probs_pred[1, i, j, k] = np.mean(cube_pred == 1)\n",
    "                cube_probs_pred[2, i, j, k] = np.mean(cube_pred == 2)\n",
    "    \n",
    "    return cube_labels_pred, cube_labels_true, cube_probs_pred, bbox_min, bbox_max\n",
    "\n",
    "# Listas para métricas voxel-wise (todas las clases)\n",
    "all_dice = {0: [], 1: [], 2: []}\n",
    "all_sensitivity = {0: [], 1: [], 2: []}\n",
    "all_precision = {0: [], 1: [], 2: []}\n",
    "all_auc = {0: [], 1: [], 2: []}\n",
    "all_accuracy = []\n",
    "all_f1 = {0: [], 1: [], 2: []}\n",
    "\n",
    "# Listas para métricas cube-wise (todas las clases)\n",
    "all_dice_cube = {0: [], 1: [], 2: []}\n",
    "all_sensitivity_cube = {0: [], 1: [], 2: []}\n",
    "all_precision_cube = {0: [], 1: [], 2: []}\n",
    "all_auc_cube = {0: [], 1: [], 2: []}\n",
    "all_accuracy_cube = []\n",
    "all_f1_cube = {0: [], 1: [], 2: []}\n",
    "\n",
    "# Configuración de la grilla (5x5x5 o 3x3x3)\n",
    "grid_size = 5  # Cambia a 3 para una grilla 3x3x3\n",
    "\n",
    "# Procesar y combinar\n",
    "for idx, ((embeddings1, labels1), (embeddings2, labels2)) in enumerate(zip(loader_model1, loader_model2)):\n",
    "    # Generar mapas de probabilidad\n",
    "    prob_maps1 = generate_probability_maps(embeddings1, projection_head1, classifier1, device)\n",
    "    prob_maps2 = generate_probability_maps(embeddings2, projection_head2, classifier2, device)\n",
    "    \n",
    "    # Combinar mapas\n",
    "    combined_prob_maps = torch.zeros_like(prob_maps1)\n",
    "    combined_prob_maps[0] = prob_maps1[0]  # Fondo\n",
    "    combined_prob_maps[1] = torch.max(prob_maps1[1], prob_maps2[1])  # Infiltrado\n",
    "    combined_prob_maps[2] = prob_maps2[2]  # Vasogénico\n",
    "    \n",
    "    # Convertir a numpy\n",
    "    prob_maps_np = combined_prob_maps.cpu().numpy()\n",
    "    prob_maps_np_nifti = np.transpose(prob_maps_np, (1, 2, 3, 0))\n",
    "    \n",
    "    # Generar segmentación\n",
    "    segmentation = np.argmax(prob_maps_np, axis=0)\n",
    "    # class_1_mask = prob_maps_np[1] > 0.4  # Umbral para Infiltrado\n",
    "    # segmentation[class_1_mask] = 1\n",
    "    segmentation_np = segmentation.astype(np.uint8)\n",
    "    \n",
    "    # Etiquetas\n",
    "    labels = labels2.squeeze(0)\n",
    "    labels_np = labels.cpu().numpy().astype(np.uint8)\n",
    "    \n",
    "    # Calcular métricas voxel-wise\n",
    "    dice, sensitivity, precision, auc, accuracy, f1 = calculate_metrics(\n",
    "        segmentation_np, labels_np, prob_maps=prob_maps_np, num_classes=3, class_offset=0\n",
    "    )\n",
    "    \n",
    "    # Almacenar métricas voxel-wise\n",
    "    for cls in range(3):\n",
    "        all_dice[cls].append(dice[cls])\n",
    "        all_sensitivity[cls].append(sensitivity[cls])\n",
    "        all_precision[cls].append(precision[cls])\n",
    "        all_auc[cls].append(auc[cls])\n",
    "        all_f1[cls].append(f1[cls])\n",
    "    all_accuracy.append(accuracy)\n",
    "    \n",
    "    # Obtener cuadro delimitador y centro del tumor\n",
    "    bbox, center = get_tumor_bbox_and_center(labels_np)\n",
    "    if bbox is None:\n",
    "        print(f\"Caso {idx}: No se detectó tumor en el ground truth. Saltando evaluación cube-wise.\")\n",
    "        continue\n",
    "    \n",
    "    # Evaluación basada en cubos\n",
    "    cube_labels_pred, cube_labels_true, cube_probs_pred, bbox_min, bbox_max = get_tumor_cube_labels(\n",
    "        segmentation_np, labels_np, bbox, center, grid_size=grid_size\n",
    "    )\n",
    "    \n",
    "    # Calcular métricas cube-wise (clases 0, 1, 2)\n",
    "    dice_cube, sensitivity_cube, precision_cube, auc_cube, accuracy_cube, f1_cube = calculate_metrics(\n",
    "        cube_labels_pred, cube_labels_true, prob_maps=cube_probs_pred, num_classes=3, class_offset=0\n",
    "    )\n",
    "    \n",
    "    # Almacenar métricas cube-wise\n",
    "    for cls in range(3):\n",
    "        all_dice_cube[cls].append(dice_cube[cls])\n",
    "        all_sensitivity_cube[cls].append(sensitivity_cube[cls])\n",
    "        all_precision_cube[cls].append(precision_cube[cls])\n",
    "        all_auc_cube[cls].append(auc_cube[cls])\n",
    "        all_f1_cube[cls].append(f1_cube[cls])\n",
    "    all_accuracy_cube.append(accuracy_cube)\n",
    "    \n",
    "    # Mapa de coincidencias/discrepancias\n",
    "    match_map = (cube_labels_pred == cube_labels_true).astype(np.uint8)\n",
    "    mismatch_map = (cube_labels_pred != cube_labels_true).astype(np.uint8)\n",
    "    \n",
    "    # Guardar mapas de cubos\n",
    "    affine = np.eye(4)\n",
    "    affine[:3, 3] = bbox_min  # Ajustar origen al cuadro delimitador\n",
    "    nib.save(nib.Nifti1Image(cube_labels_pred, affine), \n",
    "             os.path.join(output_dir, f\"pred_cube_labels_case_{idx}.nii.gz\"))\n",
    "    nib.save(nib.Nifti1Image(cube_labels_true, affine), \n",
    "             os.path.join(output_dir, f\"true_cube_labels_case_{idx}.nii.gz\"))\n",
    "    nib.save(nib.Nifti1Image(match_map, affine), \n",
    "             os.path.join(output_dir, f\"match_map_case_{idx}.nii.gz\"))\n",
    "    nib.save(nib.Nifti1Image(mismatch_map, affine), \n",
    "             os.path.join(output_dir, f\"mismatch_map_case_{idx}.nii.gz\"))\n",
    "    \n",
    "    # Análisis espacial (centro de masa de Infiltrado)\n",
    "    infiltrado_pred = (cube_labels_pred == 1).astype(np.uint8)  # Clase 1: Infiltrado\n",
    "    infiltrado_true = (cube_labels_true == 1).astype(np.uint8)\n",
    "    pred_center = np.mean(np.where(infiltrado_pred), axis=1) if np.sum(infiltrado_pred) > 0 else np.array([np.nan] * 3)\n",
    "    true_center = np.mean(np.where(infiltrado_true), axis=1) if np.sum(infiltrado_true) > 0 else np.array([np.nan] * 3)\n",
    "    distance = np.linalg.norm(pred_center - true_center) if not np.any(np.isnan(pred_center)) and not np.any(np.isnan(true_center)) else np.nan\n",
    "    \n",
    "    # Visualización 3D\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    infiltrado_pred_pos = np.where(infiltrado_pred)\n",
    "    infiltrado_true_pos = np.where(infiltrado_true)\n",
    "    ax.scatter(infiltrado_pred_pos[0], infiltrado_pred_pos[1], infiltrado_pred_pos[2], \n",
    "               c='red', label='Infiltrado Predicho', alpha=0.5)\n",
    "    ax.scatter(infiltrado_true_pos[0], infiltrado_true_pos[1], infiltrado_true_pos[2], \n",
    "               c='blue', label='Infiltrado Verdadero', alpha=0.5)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.set_title(f'Infiltrado - Caso {idx} (Grilla {grid_size}x{grid_size}x{grid_size})')\n",
    "    ax.legend()\n",
    "    plt.savefig(os.path.join(output_dir, f\"infiltrado_scatter_case_{idx}.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    # Imprimir resultados\n",
    "    print(f\"Caso {idx} - Voxel-wise:\")\n",
    "    print(f\"  Dice: {dice}, Sensitivity: {sensitivity}, Precision: {precision}, AUC-ROC: {auc}, \"\n",
    "          f\"Accuracy: {accuracy:.4f}, F1 Score: {f1}\")\n",
    "    print(f\"Caso {idx} - Cube-wise (Grilla {grid_size}x{grid_size}x{grid_size}):\")\n",
    "    print(f\"  Dice: {dice_cube}, Sensitivity: {sensitivity_cube}, Precision: {precision_cube}, \"\n",
    "          f\"AUC-ROC: {auc_cube}, Accuracy: {accuracy_cube:.4f}, F1 Score: {f1_cube}\")\n",
    "    print(f\"  Centro de masa Infiltrado (Pred): {pred_center}\")\n",
    "    print(f\"  Centro de masa Infiltrado (True): {true_center}\")\n",
    "    print(f\"  Distancia entre centros: {distance:.2f}\")\n",
    "    \n",
    "    # Guardar mapas voxel-wise\n",
    "    nib.save(nib.Nifti1Image(prob_maps_np_nifti, np.eye(4)), \n",
    "             os.path.join(output_dir, f\"probability_maps_case_{idx}.nii.gz\"))\n",
    "    nib.save(nib.Nifti1Image(labels_np, np.eye(4)), \n",
    "             os.path.join(output_dir, f\"labels_case_{idx}.nii.gz\"))\n",
    "    nib.save(nib.Nifti1Image(segmentation_np, np.eye(4)), \n",
    "             os.path.join(output_dir, f\"segmentation_case_{idx}.nii.gz\"))\n",
    "\n",
    "# Calcular promedios y desviaciones estándar (voxel-wise)\n",
    "class_names = [\"Fondo\", \"Infiltrado\", \"Vasogénico\"]\n",
    "print(\"\\nResultados Voxel-wise:\")\n",
    "for cls in range(3):\n",
    "    dice_mean = np.nanmean(all_dice[cls])\n",
    "    dice_std = np.nanstd(all_dice[cls])\n",
    "    sens_mean = np.nanmean(all_sensitivity[cls])\n",
    "    sens_std = np.nanstd(all_sensitivity[cls])\n",
    "    prec_mean = np.nanmean(all_precision[cls])\n",
    "    prec_std = np.nanstd(all_precision[cls])\n",
    "    auc_mean = np.nanmean(all_auc[cls])\n",
    "    auc_std = np.nanstd(all_auc[cls])\n",
    "    f1_mean = np.nanmean(all_f1[cls])\n",
    "    f1_std = np.nanstd(all_f1[cls])\n",
    "    \n",
    "    print(f\"\\nClase {cls} ({class_names[cls]}):\")\n",
    "    print(f\"  Dice: {dice_mean:.4f} ± {dice_std:.4f}\")\n",
    "    print(f\"  Sensibilidad: {sens_mean:.4f} ± {sens_std:.4f}\")\n",
    "    print(f\"  Precisión: {prec_mean:.4f} ± {prec_std:.4f}\")\n",
    "    print(f\"  AUC-ROC: {auc_mean:.4f} ± {auc_std:.4f}\")\n",
    "    print(f\"  F1 Score: {f1_mean:.4f} ± {f1_std:.4f}\")\n",
    "\n",
    "accuracy_mean = np.nanmean(all_accuracy)\n",
    "accuracy_std = np.nanstd(all_accuracy)\n",
    "print(f\"\\nAccuracy Global: {accuracy_mean:.4f} ± {accuracy_std:.4f}\")\n",
    "\n",
    "# Calcular promedios y desviaciones estándar (cube-wise, clases 0, 1, 2)\n",
    "print(f\"\\nResultados Cube-wise (Grilla {grid_size}x{grid_size}x{grid_size}):\")\n",
    "for cls in range(3):\n",
    "    dice_mean = np.nanmean(all_dice_cube[cls])\n",
    "    dice_std = np.nanstd(all_dice_cube[cls])\n",
    "    sens_mean = np.nanmean(all_sensitivity_cube[cls])\n",
    "    sens_std = np.nanstd(all_sensitivity_cube[cls])\n",
    "    prec_mean = np.nanmean(all_precision_cube[cls])\n",
    "    prec_std = np.nanstd(all_precision_cube[cls])\n",
    "    auc_mean = np.nanmean(all_auc_cube[cls])\n",
    "    auc_std = np.nanstd(all_auc_cube[cls])\n",
    "    f1_mean = np.nanmean(all_f1_cube[cls])\n",
    "    f1_std = np.nanstd(all_f1_cube[cls])\n",
    "    \n",
    "    print(f\"\\nClase {cls} ({class_names[cls]}):\")\n",
    "    print(f\"  Dice: {dice_mean:.4f} ± {dice_std:.4f}\")\n",
    "    print(f\"  Sensibilidad: {sens_mean:.4f} ± {sens_std:.4f}\")\n",
    "    print(f\"  Precisión: {prec_mean:.4f} ± {prec_std:.4f}\")\n",
    "    print(f\"  AUC-ROC: {auc_mean:.4f} ± {auc_std:.4f}\")\n",
    "    print(f\"  F1 Score: {f1_mean:.4f} ± {f1_std:.4f}\")\n",
    "\n",
    "accuracy_mean_cube = np.nanmean(all_accuracy_cube)\n",
    "accuracy_std_cube = np.nanstd(all_accuracy_cube)\n",
    "print(f\"\\nAccuracy Global Cube-wise: {accuracy_mean_cube:.4f} ± {accuracy_std_cube:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardando rsultados e imagenes MRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 images and 6 labels.\n",
      "Caso 0: Vasogénico channel sum (6): 37953.0, Infiltrado channel sum (2): 42915.0\n",
      "Caso 0: Etiquetas únicas (0, 2, 6): [0 2 6]\n",
      "Guardadas etiquetas en Dataset/MRIs/combined_results/nifti_volumes/labels_case_0.nii.gz\n",
      "Guardado mapa de probabilidad en Dataset/MRIs/combined_results/probability_maps/probability_maps_case_0.nii.gz\n",
      "Guardada segmentación en Dataset/MRIs/combined_results/nifti_volumes/segmentation_case_0.nii.gz\n",
      "Caso 1: Vasogénico channel sum (6): 10132.0, Infiltrado channel sum (2): 23548.0\n",
      "Caso 1: Etiquetas únicas (0, 2, 6): [0 2 6]\n",
      "Guardadas etiquetas en Dataset/MRIs/combined_results/nifti_volumes/labels_case_1.nii.gz\n",
      "Guardado mapa de probabilidad en Dataset/MRIs/combined_results/probability_maps/probability_maps_case_1.nii.gz\n",
      "Guardada segmentación en Dataset/MRIs/combined_results/nifti_volumes/segmentation_case_1.nii.gz\n",
      "Caso 2: Vasogénico channel sum (6): 41333.0, Infiltrado channel sum (2): 101448.0\n",
      "Caso 2: Etiquetas únicas (0, 2, 6): [0 2 6]\n",
      "Guardadas etiquetas en Dataset/MRIs/combined_results/nifti_volumes/labels_case_2.nii.gz\n",
      "Guardado mapa de probabilidad en Dataset/MRIs/combined_results/probability_maps/probability_maps_case_2.nii.gz\n",
      "Guardada segmentación en Dataset/MRIs/combined_results/nifti_volumes/segmentation_case_2.nii.gz\n",
      "Caso 3: Vasogénico channel sum (6): 6926.0, Infiltrado channel sum (2): 7903.0\n",
      "Caso 3: Etiquetas únicas (0, 2, 6): [0 2 6]\n",
      "Guardadas etiquetas en Dataset/MRIs/combined_results/nifti_volumes/labels_case_3.nii.gz\n",
      "Guardado mapa de probabilidad en Dataset/MRIs/combined_results/probability_maps/probability_maps_case_3.nii.gz\n",
      "Guardada segmentación en Dataset/MRIs/combined_results/nifti_volumes/segmentation_case_3.nii.gz\n",
      "Caso 4: Vasogénico channel sum (6): 16856.0, Infiltrado channel sum (2): 16769.0\n",
      "Caso 4: Etiquetas únicas (0, 2, 6): [0 2 6]\n",
      "Guardadas etiquetas en Dataset/MRIs/combined_results/nifti_volumes/labels_case_4.nii.gz\n",
      "Guardado mapa de probabilidad en Dataset/MRIs/combined_results/probability_maps/probability_maps_case_4.nii.gz\n",
      "Guardada segmentación en Dataset/MRIs/combined_results/nifti_volumes/segmentation_case_4.nii.gz\n",
      "Caso 5: Vasogénico channel sum (6): 40406.0, Infiltrado channel sum (2): 22403.0\n",
      "Caso 5: Etiquetas únicas (0, 2, 6): [0 2 6]\n",
      "Guardadas etiquetas en Dataset/MRIs/combined_results/nifti_volumes/labels_case_5.nii.gz\n",
      "Guardado mapa de probabilidad en Dataset/MRIs/combined_results/probability_maps/probability_maps_case_5.nii.gz\n",
      "Guardada segmentación en Dataset/MRIs/combined_results/nifti_volumes/segmentation_case_5.nii.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from monai import transforms\n",
    "import nibabel as nib\n",
    "\n",
    "# Dispositivo\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Modelo de proyección (MLP más profundo)\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, input_dim=48, hidden_dim1=256, hidden_dim2=128, output_dim=128, dropout_p=0.3):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_dim2, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# Clasificador supervisado (MLP)\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim=128, hidden_dim1=256, hidden_dim2=128, num_classes=3, dropout_p=0.3):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_dim2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Función para generar mapas de probabilidad\n",
    "def generate_probability_maps(embeddings, projection_head, classifier, device, batch_size=8192):\n",
    "    projection_head.eval()\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        embeddings = embeddings.squeeze(0).permute(1, 2, 3, 0)  # [128, 128, 128, 48]\n",
    "        embeddings_flat = embeddings.reshape(-1, 48)  # [2097152, 48]\n",
    "        probs_flat = []\n",
    "        \n",
    "        for i in range(0, embeddings_flat.shape[0], batch_size):\n",
    "            batch = embeddings_flat[i:i+batch_size].to(device)\n",
    "            z = projection_head(batch)  # [batch_size, 128]\n",
    "            z = F.normalize(z, dim=1)\n",
    "            logits = classifier(z)  # [batch_size, 3]\n",
    "            probs = F.softmax(logits, dim=1)  # [batch_size, 3]\n",
    "            probs_flat.append(probs.cpu())\n",
    "        \n",
    "        probs_flat = torch.cat(probs_flat, dim=0)  # [2097152, 3]\n",
    "        probs = probs_flat.reshape(128, 128, 128, 3)  # [128, 128, 128, 3]\n",
    "        probs = probs.permute(3, 0, 1, 2)  # [3, 128, 128, 128]\n",
    "    \n",
    "    return probs  # Tensor [3, 128, 128, 128]\n",
    "\n",
    "# Transformaciones de MONAI\n",
    "roi = (128, 128, 128)\n",
    "source_k = \"label\"\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        # transforms.ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "        ConvertToMultiChannelBasedOnAnotatedInfiltration(keys=\"label\"),\n",
    "        transforms.CropForegroundd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            source_key=source_k,\n",
    "            k_divisible=[roi[0], roi[1], roi[2]],\n",
    "        ),\n",
    "        transforms.RandSpatialCropd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            roi_size=[roi[0], roi[1], roi[2]],\n",
    "            random_size=False,\n",
    "        ),\n",
    "        transforms.NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Dataset y DataLoader para imágenes MRI\n",
    "dataset_path = './Dataset/Dataset_30_6'\n",
    "train_set = CustomDataset(dataset_path, section=\"test_6\", transform=train_transform)\n",
    "train_loader = DataLoader(train_set, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "# DataLoader para embeddings (asumiendo que son los mismos que train_loader)\n",
    "loader_model1 = train_loader  # Reemplaza con tu loader real si es diferente\n",
    "loader_model2 = train_loader  # Reemplaza con tu loader real si es diferente\n",
    "\n",
    "# Directorios\n",
    "output_dir = \"Dataset/MRIs/combined_results\"\n",
    "nifti_output_dir = os.path.join(output_dir, \"nifti_volumes\")\n",
    "probs_output_dir = os.path.join(output_dir, \"probability_maps\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(nifti_output_dir, exist_ok=True)\n",
    "os.makedirs(probs_output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Cargar modelos contrastivos preentrenados\n",
    "projection_head1 = ProjectionHead(input_dim=48).to(device)\n",
    "projection_head1.load_state_dict(torch.load(\"trained_models/checkpoints_contrastive/contrastive_projection_head_final_new_pipe1_v01_m1.pth\", map_location=device))\n",
    "# projection_head1.eval()\n",
    "\n",
    "projection_head2 = ProjectionHead(input_dim=48).to(device)\n",
    "projection_head2.load_state_dict(torch.load(\"trained_models/checkpoints_contrastive/contrastive_projection_head_final_new_pipe2_m1_1dhzmigz.pth\", map_location=device))\n",
    "# projection_head2.eval()\n",
    "\n",
    "# Cargar clasificadores preentrenados\n",
    "classifier1 = Classifier(input_dim=128, num_classes=3).to(device)\n",
    "classifier1.load_state_dict(torch.load(\"trained_models/checkpoints/supervised_classifier_final_pipe1_v01_m1.pth\", map_location=device))\n",
    "# classifier1.eval()\n",
    "\n",
    "classifier2 = Classifier(input_dim=128, num_classes=3).to(device)\n",
    "classifier2.load_state_dict(torch.load(\"trained_models/checkpoints/supervised_classifier_final_pipe2_m1_1dhzmigz.pth\", map_location=device))\n",
    "# classifier2.eval()\n",
    "\n",
    "# Hooks para embeddings\n",
    "decoder_features_model1 = None\n",
    "decoder_features_model2 = None\n",
    "\n",
    "def decoder_hook_fn_model1(module, input, output):\n",
    "    global decoder_features_model1\n",
    "    decoder_features_model1 = output\n",
    "\n",
    "def decoder_hook_fn_model2(module, input, output):\n",
    "    global decoder_features_model2\n",
    "    decoder_features_model2 = output\n",
    "\n",
    "hook_handle_decoder1 = model1.decoder1.conv_block.register_forward_hook(decoder_hook_fn_model1)\n",
    "hook_handle_decoder2 = model2.decoder1.conv_block.register_forward_hook(decoder_hook_fn_model2)\n",
    "\n",
    "# Procesar y guardar\n",
    "# Procesar y combinar\n",
    "with torch.no_grad():\n",
    "    for idx, (batch_data, (embeddings1, labels1), (embeddings2, labels2)) in enumerate(zip(train_loader, loader_model1, loader_model2)):\n",
    "        # Obtener imágenes y metadatos\n",
    "        image, label = batch_data[\"image\"], batch_data[\"label\"]  # [1, 11, 128, 128, 128], [1, 2, 128, 128, 128]\n",
    "        image_meta = batch_data[\"image\"].meta\n",
    "        affine = image_meta.get(\"affine\", np.eye(4)).numpy()\n",
    "        \n",
    "        image = image.to(device)\n",
    "        label = label.squeeze(0)  # [2, 128, 128, 128]\n",
    "        \n",
    "        # Debug: Verificar canales one-hot\n",
    "        print(f\"Caso {idx}: Vasogénico channel sum (6): {label[0].sum().item()}, Infiltrado channel sum (2): {label[1].sum().item()}\")\n",
    "        \n",
    "        # Convertir one-hot a etiquetas únicas (0, 6, 2)\n",
    "        label_class = torch.zeros(label.shape[1:], dtype=torch.long)  # [128, 128, 128]\n",
    "        label_class[label[0] == 1] = 6  # Vasogénico\n",
    "        label_class[label[1] == 1] = 2  # Infiltrado\n",
    "        labels_np = label_class.cpu().numpy().astype(np.int16)  # [128, 128, 128]\n",
    "        \n",
    "        # Mapear etiquetas a 0, 1, 2 para métricas\n",
    "        labels_metrics = np.zeros_like(labels_np, dtype=np.int16)\n",
    "        labels_metrics[labels_np == 6] = 1  # Vasogénico -> 1\n",
    "        labels_metrics[labels_np == 2] = 2  # Infiltrado -> 2\n",
    "        # Fondo (0) ya está correcto\n",
    "        \n",
    "        # Verificar etiquetas\n",
    "        unique_labels = np.unique(labels_np)\n",
    "        print(f\"Caso {idx}: Etiquetas únicas (0, 2, 6): {unique_labels}\")\n",
    "        if not np.all(np.isin(unique_labels, [0, 2, 6])):\n",
    "            print(f\"Advertencia: Etiquetas inválidas en caso {idx}: {unique_labels}\")\n",
    "        if 6 not in unique_labels:\n",
    "            print(f\"Advertencia: Clase vasogénico (6) no presente en caso {idx}\")\n",
    "        \n",
    "        # Obtener embeddings\n",
    "        _ = model1(image)\n",
    "        _ = model2(image)\n",
    "        embeddings1 = decoder_features_model1\n",
    "        embeddings2 = decoder_features_model2\n",
    "        \n",
    "        # Generar mapas de probabilidad\n",
    "        prob_maps1 = generate_probability_maps(embeddings1, projection_head1, classifier1, device)  # [3, 128, 128, 128]\n",
    "        prob_maps2 = generate_probability_maps(embeddings2, projection_head2, classifier2, device)  # [3, 128, 128, 128]\n",
    "        \n",
    "        # Combinar mapas\n",
    "        combined_prob_maps = torch.zeros_like(prob_maps1)  # [3, 128, 128, 128]\n",
    "        combined_prob_maps[0] = prob_maps1[0]  # Fondo (0)\n",
    "        combined_prob_maps[1] = torch.max(prob_maps1[1], prob_maps2[1])  # Vasogénico (1)\n",
    "        combined_prob_maps[2] = prob_maps2[2]  # Infiltrado (2)\n",
    "        \n",
    "        # Normalizar probabilidades\n",
    "        combined_prob_maps = combined_prob_maps / (combined_prob_maps.sum(dim=0, keepdim=True) + 1e-6)\n",
    "        \n",
    "        # Convertir a numpy\n",
    "        prob_maps_np = combined_prob_maps.cpu().numpy()  # [3, 128, 128, 128]\n",
    "        prob_maps_np_nifti = np.transpose(prob_maps_np, (1, 2, 3, 0))  # [128, 128, 128, 3]\n",
    "        \n",
    "        # Generar segmentación semántica (clases 0, 1, 2)\n",
    "        segmentation = np.argmax(prob_maps_np, axis=0)  # [128, 128, 128]\n",
    "        segmentation_np = segmentation.astype(np.int16)  # [0, 1, 2]\n",
    "        \n",
    "        # Mapear segmentación a valores originales (0, 6, 2)\n",
    "        segmentation_np_orig = np.zeros_like(segmentation_np, dtype=np.int16)\n",
    "        segmentation_np_orig[segmentation_np == 1] = 6  # Vasogénico\n",
    "        segmentation_np_orig[segmentation_np == 2] = 2  # Infiltrado\n",
    "        # Fondo (0) ya está correcto\n",
    "                \n",
    "        # Guardar imágenes MRI en NIfTI\n",
    "        image_np = image.squeeze(0).cpu().numpy()  # [11, 128, 128, 128]\n",
    "        for channel in range(image_np.shape[0]):\n",
    "            nifti_img = nib.Nifti1Image(image_np[channel], affine)\n",
    "            nib.save(nifti_img, f\"{nifti_output_dir}/case_{idx}_modality_{channel}.nii.gz\")\n",
    "        \n",
    "        # Guardar etiquetas en NIfTI (valores 0, 2, 6)\n",
    "        nifti_label_img = nib.Nifti1Image(labels_np, affine)\n",
    "        label_output_path = os.path.join(nifti_output_dir, f\"labels_case_{idx}.nii.gz\")\n",
    "        nib.save(nifti_label_img, label_output_path)\n",
    "        print(f\"Guardadas etiquetas en {label_output_path}\")\n",
    "        \n",
    "        # Guardar mapas de probabilidad\n",
    "        nifti_prob_img = nib.Nifti1Image(prob_maps_np_nifti, affine)\n",
    "        prob_output_path = os.path.join(probs_output_dir, f\"probability_maps_case_{idx}.nii.gz\")\n",
    "        nib.save(nifti_prob_img, prob_output_path)\n",
    "        print(f\"Guardado mapa de probabilidad en {prob_output_path}\")\n",
    "        \n",
    "        # Guardar segmentación (valores 0, 6, 2)\n",
    "        nifti_seg_img = nib.Nifti1Image(segmentation_np_orig, affine)\n",
    "        seg_output_path = os.path.join(nifti_output_dir, f\"segmentation_case_{idx}.nii.gz\")\n",
    "        nib.save(nifti_seg_img, seg_output_path)\n",
    "        print(f\"Guardada segmentación en {seg_output_path}\")\n",
    "\n",
    "# Remover hooks\n",
    "hook_handle_decoder1.remove()\n",
    "hook_handle_decoder2.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardando resultados Pipeline1 + Pipeline2 con Recurrencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from monai import transforms\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import monai.utils\n",
    "from src.get_data import CustomDatasetRec\n",
    "\n",
    "# Dispositivo\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Modelo de proyección (MLP más profundo)\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, input_dim=48, hidden_dim1=256, hidden_dim2=128, output_dim=128, dropout_p=0.3):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_dim2, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# Clasificador supervisado (MLP)\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim=128, hidden_dim1=256, hidden_dim2=128, num_classes=3, dropout_p=0.3):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_dim2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Función para generar mapas de probabilidad\n",
    "def generate_probability_maps(embeddings, projection_head, classifier, device, batch_size=8192):\n",
    "    projection_head.eval()\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        embeddings = embeddings.squeeze(0).permute(1, 2, 3, 0)  # [128, 128, 128, 48]\n",
    "        embeddings_flat = embeddings.reshape(-1, 48)  # [2097152, 48]\n",
    "        probs_flat = []\n",
    "        \n",
    "        for i in range(0, embeddings_flat.shape[0], batch_size):\n",
    "            batch = embeddings_flat[i:i+batch_size].to(device)\n",
    "            z = projection_head(batch)  # [batch_size, 128]\n",
    "            z = F.normalize(z, dim=1)\n",
    "            logits = classifier(z)  # [batch_size, 3]\n",
    "            probs = F.softmax(logits, dim=1)  # [batch_size, 3]\n",
    "            probs_flat.append(probs.cpu())\n",
    "        \n",
    "        probs_flat = torch.cat(probs_flat, dim=0)  # [2097152, 3]\n",
    "        probs = probs_flat.reshape(128, 128, 128, 3)  # [128, 128, 128, 3]\n",
    "        probs = probs.permute(3, 0, 1, 2)  # [3, 128, 128, 128]\n",
    "    \n",
    "    return probs  # Tensor [3, 128, 128, 128]\n",
    "\n",
    "# Transformaciones de MONAI\n",
    "roi = (128, 128, 128)\n",
    "source_k = \"label\"\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        # transforms.ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "        ConvertToMultiChannelBasedOnAnotatedInfiltration(keys=\"label\"),\n",
    "        transforms.CropForegroundd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            source_key=source_k,\n",
    "            k_divisible=[roi[0], roi[1], roi[2]],\n",
    "        ),\n",
    "        transforms.RandSpatialCropd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            roi_size=[roi[0], roi[1], roi[2]],\n",
    "            random_size=False,\n",
    "        ),\n",
    "        transforms.NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "    ]\n",
    ")\n",
    "# Transformaciones combinadas para imágenes de entrada, etiquetas y recurrencia\n",
    "combined_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\", \"label\", \"recurrence\"]),\n",
    "        transforms.EnsureChannelFirstD(keys=\"recurrence\"),\n",
    "        ConvertToMultiChannelBasedOnAnotatedInfiltration(keys=\"label\"),\n",
    "        transforms.CropForegroundd(\n",
    "            keys=[\"image\", \"label\", \"recurrence\"],\n",
    "            source_key=\"label\",\n",
    "            k_divisible=[128, 128, 128],\n",
    "        ),\n",
    "        transforms.RandSpatialCropd(\n",
    "            keys=[\"image\", \"label\", \"recurrence\"],\n",
    "            roi_size=[128, 128, 128],\n",
    "            random_size=False,\n",
    "        ),\n",
    "        transforms.NormalizeIntensityd(keys=[\"image\", \"recurrence\"], nonzero=True, channel_wise=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Dataset y DataLoader\n",
    "dataset_path = './Dataset/Dataset_30_6'\n",
    "train_set = CustomDatasetRec(dataset_path, section=\"test_6\", transform=combined_transform)\n",
    "train_loader = DataLoader(train_set, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "# Directorios\n",
    "output_dir = \"Dataset/MRIs/contrastive_voxel_wise/combined_results\"\n",
    "nifti_output_dir = os.path.join(output_dir, \"nifti_volumes\")\n",
    "probs_output_dir = os.path.join(output_dir, \"probability_maps\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(nifti_output_dir, exist_ok=True)\n",
    "os.makedirs(probs_output_dir, exist_ok=True)\n",
    "\n",
    "# Cargar modelos contrastivos preentrenados\n",
    "projection_head1 = ProjectionHead(input_dim=48).to(device)\n",
    "projection_head1.load_state_dict(torch.load(\"trained_models/checkpoints_contrastive/contrastive_projection_head_final_new_pipe1_v01_m1.pth\", map_location=device))\n",
    "# projection_head1.eval()\n",
    "\n",
    "projection_head2 = ProjectionHead(input_dim=48).to(device)\n",
    "projection_head2.load_state_dict(torch.load(\"trained_models/checkpoints_contrastive/contrastive_projection_head_final_new_pipe2_m1_1dhzmigz.pth\", map_location=device))\n",
    "# projection_head2.eval()\n",
    "\n",
    "# Cargar clasificadores preentrenados\n",
    "classifier1 = Classifier(input_dim=128, num_classes=3).to(device)\n",
    "classifier1.load_state_dict(torch.load(\"trained_models/checkpoints/supervised_classifier_final_pipe1_v01_m1.pth\", map_location=device))\n",
    "# classifier1.eval()\n",
    "\n",
    "classifier2 = Classifier(input_dim=128, num_classes=3).to(device)\n",
    "classifier2.load_state_dict(torch.load(\"trained_models/checkpoints/supervised_classifier_final_pipe2_m1_1dhzmigz.pth\", map_location=device))\n",
    "# classifier2.eval()\n",
    "\n",
    "# Hooks para embeddings\n",
    "decoder_features_model1 = None\n",
    "decoder_features_model2 = None\n",
    "\n",
    "def decoder_hook_fn_model1(module, input, output):\n",
    "    global decoder_features_model1\n",
    "    decoder_features_model1 = output\n",
    "\n",
    "def decoder_hook_fn_model2(module, input, output):\n",
    "    global decoder_features_model2\n",
    "    decoder_features_model2 = output\n",
    "\n",
    "hook_handle_decoder1 = model1.decoder1.conv_block.register_forward_hook(decoder_hook_fn_model1)\n",
    "hook_handle_decoder2 = model2.decoder1.conv_block.register_forward_hook(decoder_hook_fn_model2)\n",
    "\n",
    "\n",
    "# Procesar y combinar\n",
    "with torch.no_grad():\n",
    "    for idx, batch_data in enumerate(train_loader):\n",
    "        monai.utils.set_determinism(seed=idx)\n",
    "        \n",
    "        print(f\"\\nCaso {idx}:\")\n",
    "        print(f\"image shape: {batch_data['image'].shape}\")\n",
    "        print(f\"label shape: {batch_data['label'].shape}\")\n",
    "        print(f\"recurrence shape: {batch_data['recurrence'].shape}\")\n",
    "        \n",
    "        # Extraer datos\n",
    "        image = batch_data[\"image\"].to(device)  # [1, 11, 128, 128, 128]\n",
    "        label = batch_data[\"label\"].to(device)  # [1, 2, 128, 128, 128]\n",
    "        recurrence = batch_data[\"recurrence\"].to(device)  # [1, 1, 128, 128, 128]\n",
    "                \n",
    "        # Convertir recurrencia a numpy\n",
    "        recurrence_np = recurrence.squeeze().cpu().numpy().astype(np.float32)  # [128, 128, 128]\n",
    "        print(f\"Caso {idx}: recurrence_np shape: {recurrence_np.shape}, min: {recurrence_np.min()}, max: {recurrence_np.max()}, mean: {recurrence_np.mean()}\")\n",
    "        \n",
    "        # Debug: Verificar canales one-hot\n",
    "        print(f\"Caso {idx}: Vasogénico channel sum (6): {label[0, 0].sum().item()}, Infiltrado channel sum (2): {label[0, 1].sum().item()}\")\n",
    "        \n",
    "        # Convertir one-hot a etiquetas únicas (0, 6, 2)\n",
    "        label_class = torch.zeros(label.shape[2:], dtype=torch.long, device=device)  # [128, 128, 128]\n",
    "        label_class[label[0, 0] == 1] = 6  # Vasogénico\n",
    "        label_class[label[0, 1] == 1] = 2  # Infiltrado\n",
    "        labels_np = label_class.cpu().numpy().astype(np.int16)  # [128, 128, 128]\n",
    "        \n",
    "        # Mapear etiquetas a 0, 1, 2 para métricas\n",
    "        labels_metrics = np.zeros_like(labels_np, dtype=np.int16)\n",
    "        labels_metrics[labels_np == 6] = 1  # Vasogénico -> 1\n",
    "        labels_metrics[labels_np == 2] = 2  # Infiltrado -> 2\n",
    "        \n",
    "        # Verificar etiquetas\n",
    "        unique_labels = np.unique(labels_np)\n",
    "        print(f\"Caso {idx}: Etiquetas únicas (0, 2, 6): {unique_labels}\")\n",
    "        if not np.all(np.isin(unique_labels, [0, 2, 6])):\n",
    "            print(f\"Advertencia: Etiquetas inválidas en caso {idx}: {unique_labels}\")\n",
    "        if 6 not in unique_labels:\n",
    "            print(f\"Advertencia: Clase vasogénico (6) no presente en caso {idx}\")\n",
    "        \n",
    "        # Obtener embeddings\n",
    "        _ = model1(image)\n",
    "        _ = model2(image)\n",
    "        embeddings1 = decoder_features_model1\n",
    "        embeddings2 = decoder_features_model2\n",
    "        \n",
    "        # Generar mapas de probabilidad\n",
    "        prob_maps1 = generate_probability_maps(embeddings1, projection_head1, classifier1, device)  # [3, 128, 128, 128]\n",
    "        prob_maps2 = generate_probability_maps(embeddings2, projection_head2, classifier2, device)  # [3, 128, 128, 128]\n",
    "        \n",
    "        # Combinar mapas\n",
    "        combined_prob_maps = torch.zeros_like(prob_maps1)  # [3, 128, 128, 128]\n",
    "        combined_prob_maps[0] = prob_maps1[0]  # Fondo (0)\n",
    "        combined_prob_maps[1] = torch.max(prob_maps1[1], prob_maps2[1])  # Vasogénico (1)\n",
    "        combined_prob_maps[2] = prob_maps2[2]  # Infiltrado (2)\n",
    "        \n",
    "        # Normalizar probabilidades\n",
    "        combined_prob_maps = combined_prob_maps / (combined_prob_maps.sum(dim=0, keepdim=True) + 1e-6)\n",
    "        \n",
    "        # Convertir a numpy\n",
    "        prob_maps_np = combined_prob_maps.cpu().numpy()  # [3, 128, 128, 128]\n",
    "        prob_maps_np_nifti = np.transpose(prob_maps_np, (1, 2, 3, 0))  # [128, 128, 128, 3]\n",
    "        \n",
    "        # Generar segmentación semántica (clases 0, 1, 2)\n",
    "        segmentation = np.argmax(prob_maps_np, axis=0)  # [128, 128, 128]\n",
    "        segmentation_np = segmentation.astype(np.int16)  # [0, 1, 2]\n",
    "        \n",
    "        # Mapear segmentación a valores originales (0, 6, 2)\n",
    "        segmentation_np_orig = np.zeros_like(segmentation_np, dtype=np.int16)\n",
    "        segmentation_np_orig[segmentation_np == 1] = 6  # Vasogénico\n",
    "        segmentation_np_orig[segmentation_np == 2] = 2  # Infiltrado\n",
    "        \n",
    "        # Guardar imágenes MRI en NIfTI\n",
    "        image_np = image.squeeze(0).cpu().numpy()  # [11, 128, 128, 128]\n",
    "        for channel in range(image_np.shape[0]):\n",
    "            nifti_img = nib.Nifti1Image(image_np[channel], affine, header=nib.Nifti1Header())\n",
    "            nib.save(nifti_img, os.path.join(nifti_output_dir, f\"case_{idx}_modality_{channel}.nii.gz\"))\n",
    "        \n",
    "        # Guardar imagen de recurrencia en NIfTI\n",
    "        nifti_recurrence_img = nib.Nifti1Image(recurrence_np, affine, header=nib.Nifti1Header())\n",
    "        recurrence_output_path = os.path.join(nifti_output_dir, f\"case_{idx}_recurrence.nii.gz\")\n",
    "        nib.save(nifti_recurrence_img, recurrence_output_path)\n",
    "        print(f\"Guardada imagen de recurrencia en {recurrence_output_path}\")\n",
    "        saved_nifti = nib.load(recurrence_output_path)\n",
    "        saved_data = saved_nifti.get_fdata()\n",
    "        print(f\"Saved recurrence NIfTI shape: {saved_data.shape}, min: {saved_data.min()}, max: {saved_data.max()}, mean: {saved_data.mean()}\")\n",
    "        file_size = os.path.getsize(recurrence_output_path) / 1024  # Size in kB\n",
    "        print(f\"Recurrence file size: {file_size:.2f} kB\")\n",
    "        \n",
    "        # Guardar etiquetas en NIfTI (valores 0, 2, 6)\n",
    "        nifti_label_img = nib.Nifti1Image(labels_np, affine, header=nib.Nifti1Header())\n",
    "        label_output_path = os.path.join(nifti_output_dir, f\"labels_case_{idx}.nii.gz\")\n",
    "        nib.save(nifti_label_img, label_output_path)\n",
    "        print(f\"Guardadas etiquetas en {label_output_path}\")\n",
    "        \n",
    "        # Guardar mapas de probabilidad\n",
    "        nifti_prob_img = nib.Nifti1Image(prob_maps_np_nifti, affine, header=nib.Nifti1Header())\n",
    "        prob_output_path = os.path.join(probs_output_dir, f\"probability_maps_case_{idx}.nii.gz\")\n",
    "        nib.save(nifti_prob_img, prob_output_path)\n",
    "        print(f\"Guardado mapa de probabilidad en {prob_output_path}\")\n",
    "        \n",
    "        # Guardar segmentación (valores 0, 6, 2)\n",
    "        nifti_seg_img = nib.Nifti1Image(segmentation_np_orig, affine, header=nib.Nifti1Header())\n",
    "        seg_output_path = os.path.join(nifti_output_dir, f\"segmentation_case_{idx}.nii.gz\")\n",
    "        nib.save(nifti_seg_img, seg_output_path)\n",
    "        print(f\"Guardada segmentación en {seg_output_path}\")\n",
    "        \n",
    "        # Restaurar aleatoriedad después de cada caso\n",
    "        monai.utils.set_determinism(seed=None)\n",
    "\n",
    "# Remover hooks\n",
    "hook_handle_decoder1.remove()\n",
    "hook_handle_decoder2.remove()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardando resultados Pipeline 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from monai import transforms\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import monai.utils\n",
    "from src.get_data import CustomDatasetRec\n",
    "\n",
    "# Dispositivo\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Modelo de proyección (MLP más profundo)\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, input_dim=48, hidden_dim1=256, hidden_dim2=128, output_dim=128, dropout_p=0.3):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_dim2, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# Clasificador supervisado (MLP)\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim=128, hidden_dim1=256, hidden_dim2=128, num_classes=3, dropout_p=0.3):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_dim2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Función para generar mapas de probabilidad\n",
    "def generate_probability_maps(embeddings, projection_head, classifier, device, batch_size=8192):\n",
    "    projection_head.eval()\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        embeddings = embeddings.squeeze(0).permute(1, 2, 3, 0)  # [128, 128, 128, 48]\n",
    "        embeddings_flat = embeddings.reshape(-1, 48)  # [2097152, 48]\n",
    "        probs_flat = []\n",
    "        \n",
    "        for i in range(0, embeddings_flat.shape[0], batch_size):\n",
    "            batch = embeddings_flat[i:i+batch_size].to(device)\n",
    "            z = projection_head(batch)  # [batch_size, 128]\n",
    "            z = F.normalize(z, dim=1)\n",
    "            logits = classifier(z)  # [batch_size, 3]\n",
    "            probs = F.softmax(logits, dim=1)  # [batch_size, 3]\n",
    "            probs_flat.append(probs.cpu())\n",
    "        \n",
    "        probs_flat = torch.cat(probs_flat, dim=0)  # [2097152, 3]\n",
    "        probs = probs_flat.reshape(128, 128, 128, 3)  # [128, 128, 128, 3]\n",
    "        probs = probs.permute(3, 0, 1, 2)  # [3, 128, 128, 128]\n",
    "    \n",
    "    return probs  # Tensor [3, 128, 128, 128]\n",
    "\n",
    "# Transformaciones de MONAI\n",
    "roi = (128, 128, 128)\n",
    "source_k = \"label\"\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        # transforms.ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "        ConvertToMultiChannelBasedOnAnotatedInfiltration(keys=\"label\"),\n",
    "        transforms.CropForegroundd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            source_key=source_k,\n",
    "            k_divisible=[roi[0], roi[1], roi[2]],\n",
    "        ),\n",
    "        transforms.RandSpatialCropd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            roi_size=[roi[0], roi[1], roi[2]],\n",
    "            random_size=False,\n",
    "        ),\n",
    "        transforms.NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "    ]\n",
    ")\n",
    "# Transformaciones combinadas para imágenes de entrada, etiquetas y recurrencia\n",
    "combined_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\", \"label\", \"recurrence\"]),\n",
    "        transforms.EnsureChannelFirstD(keys=\"recurrence\"),\n",
    "        ConvertToMultiChannelBasedOnAnotatedInfiltration(keys=\"label\"),\n",
    "        transforms.CropForegroundd(\n",
    "            keys=[\"image\", \"label\", \"recurrence\"],\n",
    "            source_key=\"label\",\n",
    "            k_divisible=[128, 128, 128],\n",
    "        ),\n",
    "        transforms.RandSpatialCropd(\n",
    "            keys=[\"image\", \"label\", \"recurrence\"],\n",
    "            roi_size=[128, 128, 128],\n",
    "            random_size=False,\n",
    "        ),\n",
    "        transforms.NormalizeIntensityd(keys=[\"image\", \"recurrence\"], nonzero=True, channel_wise=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Dataset y DataLoader\n",
    "dataset_path = './Dataset/Dataset_30_6'\n",
    "train_set = CustomDatasetRec(dataset_path, section=\"test_6\", transform=combined_transform)\n",
    "train_loader = DataLoader(train_set, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "# Directorios\n",
    "output_dir = \"Dataset/MRIs/contrastive_voxel_wise/combined_results-pipe2\"\n",
    "nifti_output_dir = os.path.join(output_dir, \"nifti_volumes\")\n",
    "probs_output_dir = os.path.join(output_dir, \"probability_maps\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(nifti_output_dir, exist_ok=True)\n",
    "os.makedirs(probs_output_dir, exist_ok=True)\n",
    "\n",
    "# # Cargar modelos contrastivos preentrenados\n",
    "# projection_head1 = ProjectionHead(input_dim=48).to(device)\n",
    "# projection_head1.load_state_dict(torch.load(\"trained_models/checkpoints_contrastive/contrastive_projection_head_final_new_pipe1_v01_m1.pth\", map_location=device))\n",
    "# # projection_head1.eval()\n",
    "\n",
    "projection_head2 = ProjectionHead(input_dim=48).to(device)\n",
    "projection_head2.load_state_dict(torch.load(\"trained_models/checkpoints_contrastive/contrastive_projection_head_final_new_pipe2_m1_1dhzmigz.pth\", map_location=device))\n",
    "# projection_head2.eval()\n",
    "\n",
    "# # Cargar clasificadores preentrenados\n",
    "# classifier1 = Classifier(input_dim=128, num_classes=3).to(device)\n",
    "# classifier1.load_state_dict(torch.load(\"trained_models/checkpoints/supervised_classifier_final_pipe1_v01_m1.pth\", map_location=device))\n",
    "# # classifier1.eval()\n",
    "\n",
    "classifier2 = Classifier(input_dim=128, num_classes=3).to(device)\n",
    "classifier2.load_state_dict(torch.load(\"trained_models/checkpoints/supervised_classifier_final_pipe2_m1_1dhzmigz.pth\", map_location=device))\n",
    "# classifier2.eval()\n",
    "\n",
    "# Hooks para embeddings\n",
    "# decoder_features_model1 = None\n",
    "decoder_features_model2 = None\n",
    "\n",
    "# def decoder_hook_fn_model1(module, input, output):\n",
    "#     global decoder_features_model1\n",
    "#     decoder_features_model1 = output\n",
    "\n",
    "def decoder_hook_fn_model2(module, input, output):\n",
    "    global decoder_features_model2\n",
    "    decoder_features_model2 = output\n",
    "\n",
    "# hook_handle_decoder1 = model1.decoder1.conv_block.register_forward_hook(decoder_hook_fn_model1)\n",
    "hook_handle_decoder2 = model2.decoder1.conv_block.register_forward_hook(decoder_hook_fn_model2)\n",
    "\n",
    "\n",
    "# Procesar y combinar\n",
    "with torch.no_grad():\n",
    "    for idx, batch_data in enumerate(train_loader):\n",
    "        monai.utils.set_determinism(seed=idx)\n",
    "        \n",
    "        print(f\"\\nCaso {idx}:\")\n",
    "        print(f\"image shape: {batch_data['image'].shape}\")\n",
    "        print(f\"label shape: {batch_data['label'].shape}\")\n",
    "        print(f\"recurrence shape: {batch_data['recurrence'].shape}\")\n",
    "        \n",
    "        # Extraer datos\n",
    "        image = batch_data[\"image\"].to(device)  # [1, 11, 128, 128, 128]\n",
    "        label = batch_data[\"label\"].to(device)  # [1, 2, 128, 128, 128]\n",
    "        recurrence = batch_data[\"recurrence\"].to(device)  # [1, 1, 128, 128, 128]\n",
    "                \n",
    "        # Convertir recurrencia a numpy\n",
    "        recurrence_np = recurrence.squeeze().cpu().numpy().astype(np.float32)  # [128, 128, 128]\n",
    "        print(f\"Caso {idx}: recurrence_np shape: {recurrence_np.shape}, min: {recurrence_np.min()}, max: {recurrence_np.max()}, mean: {recurrence_np.mean()}\")\n",
    "        \n",
    "        # Debug: Verificar canales one-hot\n",
    "        print(f\"Caso {idx}: Vasogénico channel sum (6): {label[0, 0].sum().item()}, Infiltrado channel sum (2): {label[0, 1].sum().item()}\")\n",
    "        \n",
    "        # Convertir one-hot a etiquetas únicas (0, 6, 2)\n",
    "        label_class = torch.zeros(label.shape[2:], dtype=torch.long, device=device)  # [128, 128, 128]\n",
    "        label_class[label[0, 0] == 1] = 6  # Vasogénico\n",
    "        label_class[label[0, 1] == 1] = 2  # Infiltrado\n",
    "        labels_np = label_class.cpu().numpy().astype(np.int16)  # [128, 128, 128]\n",
    "        \n",
    "        # Mapear etiquetas a 0, 1, 2 para métricas\n",
    "        labels_metrics = np.zeros_like(labels_np, dtype=np.int16)\n",
    "        labels_metrics[labels_np == 6] = 1  # Vasogénico -> 1\n",
    "        labels_metrics[labels_np == 2] = 2  # Infiltrado -> 2\n",
    "        \n",
    "        # Verificar etiquetas\n",
    "        unique_labels = np.unique(labels_np)\n",
    "        print(f\"Caso {idx}: Etiquetas únicas (0, 2, 6): {unique_labels}\")\n",
    "        if not np.all(np.isin(unique_labels, [0, 2, 6])):\n",
    "            print(f\"Advertencia: Etiquetas inválidas en caso {idx}: {unique_labels}\")\n",
    "        if 6 not in unique_labels:\n",
    "            print(f\"Advertencia: Clase vasogénico (6) no presente en caso {idx}\")\n",
    "        \n",
    "        # Obtener embeddings\n",
    "        # _ = model1(image)\n",
    "        _ = model2(image)\n",
    "        # embeddings1 = decoder_features_model1\n",
    "        embeddings2 = decoder_features_model2\n",
    "        \n",
    "        # Generar mapas de probabilidad\n",
    "        # prob_maps1 = generate_probability_maps(embeddings1, projection_head1, classifier1, device)  # [3, 128, 128, 128]\n",
    "        prob_maps2 = generate_probability_maps(embeddings2, projection_head2, classifier2, device)  # [3, 128, 128, 128]\n",
    "        \n",
    "        # # Combinar mapas\n",
    "        # combined_prob_maps = torch.zeros_like(prob_maps1)  # [3, 128, 128, 128]\n",
    "        # combined_prob_maps[0] = prob_maps1[0]  # Fondo (0)\n",
    "        # combined_prob_maps[1] = torch.max(prob_maps1[1], prob_maps2[1])  # Vasogénico (1)\n",
    "        # combined_prob_maps[2] = prob_maps2[2]  # Infiltrado (2)\n",
    "        \n",
    "        # Normalizar probabilidades\n",
    "        combined_prob_maps = prob_maps2 / (prob_maps2.sum(dim=0, keepdim=True) + 1e-6)\n",
    "        \n",
    "        # Convertir a numpy\n",
    "        prob_maps_np = combined_prob_maps.cpu().numpy()  # [3, 128, 128, 128]\n",
    "        prob_maps_np_nifti = np.transpose(prob_maps_np, (1, 2, 3, 0))  # [128, 128, 128, 3]\n",
    "        \n",
    "        # Generar segmentación semántica (clases 0, 1, 2)\n",
    "        segmentation = np.argmax(prob_maps_np, axis=0)  # [128, 128, 128]\n",
    "        segmentation_np = segmentation.astype(np.int16)  # [0, 1, 2]\n",
    "        \n",
    "        # Mapear segmentación a valores originales (0, 6, 2)\n",
    "        segmentation_np_orig = np.zeros_like(segmentation_np, dtype=np.int16)\n",
    "        segmentation_np_orig[segmentation_np == 1] = 6  # Vasogénico\n",
    "        segmentation_np_orig[segmentation_np == 2] = 2  # Infiltrado\n",
    "        \n",
    "        # Guardar imágenes MRI en NIfTI\n",
    "        image_np = image.squeeze(0).cpu().numpy()  # [11, 128, 128, 128]\n",
    "        for channel in range(image_np.shape[0]):\n",
    "            nifti_img = nib.Nifti1Image(image_np[channel], affine, header=nib.Nifti1Header())\n",
    "            nib.save(nifti_img, os.path.join(nifti_output_dir, f\"case_{idx}_modality_{channel}.nii.gz\"))\n",
    "        \n",
    "        # Guardar imagen de recurrencia en NIfTI\n",
    "        nifti_recurrence_img = nib.Nifti1Image(recurrence_np, affine, header=nib.Nifti1Header())\n",
    "        recurrence_output_path = os.path.join(nifti_output_dir, f\"case_{idx}_recurrence.nii.gz\")\n",
    "        nib.save(nifti_recurrence_img, recurrence_output_path)\n",
    "        print(f\"Guardada imagen de recurrencia en {recurrence_output_path}\")\n",
    "        saved_nifti = nib.load(recurrence_output_path)\n",
    "        saved_data = saved_nifti.get_fdata()\n",
    "        print(f\"Saved recurrence NIfTI shape: {saved_data.shape}, min: {saved_data.min()}, max: {saved_data.max()}, mean: {saved_data.mean()}\")\n",
    "        file_size = os.path.getsize(recurrence_output_path) / 1024  # Size in kB\n",
    "        print(f\"Recurrence file size: {file_size:.2f} kB\")\n",
    "        \n",
    "        # Guardar etiquetas en NIfTI (valores 0, 2, 6)\n",
    "        nifti_label_img = nib.Nifti1Image(labels_np, affine, header=nib.Nifti1Header())\n",
    "        label_output_path = os.path.join(nifti_output_dir, f\"labels_case_{idx}.nii.gz\")\n",
    "        nib.save(nifti_label_img, label_output_path)\n",
    "        print(f\"Guardadas etiquetas en {label_output_path}\")\n",
    "        \n",
    "        # Guardar mapas de probabilidad\n",
    "        nifti_prob_img = nib.Nifti1Image(prob_maps_np_nifti, affine, header=nib.Nifti1Header())\n",
    "        prob_output_path = os.path.join(probs_output_dir, f\"probability_maps_case_{idx}.nii.gz\")\n",
    "        nib.save(nifti_prob_img, prob_output_path)\n",
    "        print(f\"Guardado mapa de probabilidad en {prob_output_path}\")\n",
    "        \n",
    "        # Guardar segmentación (valores 0, 6, 2)\n",
    "        nifti_seg_img = nib.Nifti1Image(segmentation_np_orig, affine, header=nib.Nifti1Header())\n",
    "        seg_output_path = os.path.join(nifti_output_dir, f\"segmentation_case_{idx}.nii.gz\")\n",
    "        nib.save(nifti_seg_img, seg_output_path)\n",
    "        print(f\"Guardada segmentación en {seg_output_path}\")\n",
    "        \n",
    "        # Restaurar aleatoriedad después de cada caso\n",
    "        monai.utils.set_determinism(seed=None)\n",
    "\n",
    "# Remover hooks\n",
    "# hook_handle_decoder1.remove()\n",
    "hook_handle_decoder2.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardando resultados pipe 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 images, 6 labels, and 6 recurrence files.\n",
      "2025-06-20 13:11:46,329 - INFO - Apply pending transforms - lazy: None, pending: 0, upcoming 'Compose', transform.lazy: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/training2/miniconda3/envs/monai_env/lib/python3.11/site-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.croppad.dictionary CropForegroundd.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After transforms - image shape: torch.Size([11, 128, 128, 128]), label shape: torch.Size([2, 128, 128, 128]), recurrence shape: torch.Size([1, 128, 128, 128])\n",
      "\n",
      "Caso 0:\n",
      "image shape: torch.Size([1, 11, 128, 128, 128])\n",
      "label shape: torch.Size([1, 2, 128, 128, 128])\n",
      "recurrence shape: torch.Size([1, 1, 128, 128, 128])\n",
      "Caso 0: recurrence_np shape: (128, 128, 128), min: -2.4518356323242188, max: 10.343467712402344, mean: 1.5454133972525597e-08\n",
      "Caso 0: Vasogénico channel sum (6): 22305, Infiltrado channel sum (2): 40504\n",
      "Caso 0: Etiquetas únicas (0, 2, 6): [0 2 6]\n",
      "Guardada imagen de recurrencia en Dataset/MRIs/contrastive_voxel_wise/combined_results-pipe1/nifti_volumes/case_0_recurrence.nii.gz\n",
      "Saved recurrence NIfTI shape: (128, 128, 128), min: -2.4518356323242188, max: 10.343467712402344, mean: 1.4600035069811668e-08\n",
      "Recurrence file size: 3104.10 kB\n",
      "Guardadas etiquetas en Dataset/MRIs/contrastive_voxel_wise/combined_results-pipe1/nifti_volumes/labels_case_0.nii.gz\n",
      "Guardado mapa de probabilidad en Dataset/MRIs/contrastive_voxel_wise/combined_results-pipe1/probability_maps/probability_maps_case_0.nii.gz\n",
      "Guardada segmentación en Dataset/MRIs/contrastive_voxel_wise/combined_results-pipe1/nifti_volumes/segmentation_case_0.nii.gz\n",
      "2025-06-20 13:11:50,636 - INFO - Apply pending transforms - lazy: None, pending: 0, upcoming 'Compose', transform.lazy: False\n",
      "After transforms - image shape: torch.Size([11, 128, 128, 128]), label shape: torch.Size([2, 128, 128, 128]), recurrence shape: torch.Size([1, 128, 128, 128])\n",
      "\n",
      "Caso 1:\n",
      "image shape: torch.Size([1, 11, 128, 128, 128])\n",
      "label shape: torch.Size([1, 2, 128, 128, 128])\n",
      "recurrence shape: torch.Size([1, 1, 128, 128, 128])\n",
      "Caso 1: recurrence_np shape: (128, 128, 128), min: -2.740821123123169, max: 12.877437591552734, mean: -3.565219230949879e-08\n",
      "Caso 1: Vasogénico channel sum (6): 29228, Infiltrado channel sum (2): 113553\n",
      "Caso 1: Etiquetas únicas (0, 2, 6): [0 2 6]\n",
      "Guardada imagen de recurrencia en Dataset/MRIs/contrastive_voxel_wise/combined_results-pipe1/nifti_volumes/case_1_recurrence.nii.gz\n",
      "Saved recurrence NIfTI shape: (128, 128, 128), min: -2.740821123123169, max: 12.877437591552734, mean: -3.290565862845707e-08\n",
      "Recurrence file size: 3481.42 kB\n",
      "Guardadas etiquetas en Dataset/MRIs/contrastive_voxel_wise/combined_results-pipe1/nifti_volumes/labels_case_1.nii.gz\n",
      "Guardado mapa de probabilidad en Dataset/MRIs/contrastive_voxel_wise/combined_results-pipe1/probability_maps/probability_maps_case_1.nii.gz\n",
      "Guardada segmentación en Dataset/MRIs/contrastive_voxel_wise/combined_results-pipe1/nifti_volumes/segmentation_case_1.nii.gz\n",
      "2025-06-20 13:11:54,810 - INFO - Apply pending transforms - lazy: None, pending: 0, upcoming 'Compose', transform.lazy: False\n",
      "After transforms - image shape: torch.Size([11, 128, 128, 128]), label shape: torch.Size([2, 128, 128, 128]), recurrence shape: torch.Size([1, 128, 128, 128])\n",
      "\n",
      "Caso 2:\n",
      "image shape: torch.Size([1, 11, 128, 128, 128])\n",
      "label shape: torch.Size([1, 2, 128, 128, 128])\n",
      "recurrence shape: torch.Size([1, 1, 128, 128, 128])\n",
      "Caso 2: recurrence_np shape: (128, 128, 128), min: -3.149333953857422, max: 13.404967308044434, mean: 1.1228257790207863e-07\n",
      "Caso 2: Vasogénico channel sum (6): 6548, Infiltrado channel sum (2): 27132\n",
      "Caso 2: Etiquetas únicas (0, 2, 6): [0 2 6]\n",
      "Guardada imagen de recurrencia en Dataset/MRIs/contrastive_voxel_wise/combined_results-pipe1/nifti_volumes/case_2_recurrence.nii.gz\n",
      "Saved recurrence NIfTI shape: (128, 128, 128), min: -3.149333953857422, max: 13.404967308044434, mean: 1.1601991016171875e-07\n",
      "Recurrence file size: 3352.38 kB\n",
      "Guardadas etiquetas en Dataset/MRIs/contrastive_voxel_wise/combined_results-pipe1/nifti_volumes/labels_case_2.nii.gz\n",
      "Guardado mapa de probabilidad en Dataset/MRIs/contrastive_voxel_wise/combined_results-pipe1/probability_maps/probability_maps_case_2.nii.gz\n",
      "Guardada segmentación en Dataset/MRIs/contrastive_voxel_wise/combined_results-pipe1/nifti_volumes/segmentation_case_2.nii.gz\n",
      "2025-06-20 13:11:59,002 - INFO - Apply pending transforms - lazy: None, pending: 0, upcoming 'Compose', transform.lazy: False\n",
      "After transforms - image shape: torch.Size([11, 128, 128, 128]), label shape: torch.Size([2, 128, 128, 128]), recurrence shape: torch.Size([1, 128, 128, 128])\n",
      "\n",
      "Caso 3:\n",
      "image shape: torch.Size([1, 11, 128, 128, 128])\n",
      "label shape: torch.Size([1, 2, 128, 128, 128])\n",
      "recurrence shape: torch.Size([1, 1, 128, 128, 128])\n",
      "Caso 3: recurrence_np shape: (128, 128, 128), min: -2.4558207988739014, max: 9.173452377319336, mean: 1.0602525435388088e-07\n",
      "Caso 3: Vasogénico channel sum (6): 7641, Infiltrado channel sum (2): 73227\n",
      "Caso 3: Etiquetas únicas (0, 2, 6): [0 2 6]\n",
      "Guardada imagen de recurrencia en Dataset/MRIs/contrastive_voxel_wise/combined_results-pipe1/nifti_volumes/case_3_recurrence.nii.gz\n",
      "Saved recurrence NIfTI shape: (128, 128, 128), min: -2.4558207988739014, max: 9.173452377319336, mean: 1.0488573484908337e-07\n",
      "Recurrence file size: 3036.71 kB\n",
      "Guardadas etiquetas en Dataset/MRIs/contrastive_voxel_wise/combined_results-pipe1/nifti_volumes/labels_case_3.nii.gz\n",
      "Guardado mapa de probabilidad en Dataset/MRIs/contrastive_voxel_wise/combined_results-pipe1/probability_maps/probability_maps_case_3.nii.gz\n",
      "Guardada segmentación en Dataset/MRIs/contrastive_voxel_wise/combined_results-pipe1/nifti_volumes/segmentation_case_3.nii.gz\n",
      "2025-06-20 13:12:03,197 - INFO - Apply pending transforms - lazy: None, pending: 0, upcoming 'Compose', transform.lazy: False\n",
      "After transforms - image shape: torch.Size([11, 128, 128, 128]), label shape: torch.Size([2, 128, 128, 128]), recurrence shape: torch.Size([1, 128, 128, 128])\n",
      "\n",
      "Caso 4:\n",
      "image shape: torch.Size([1, 11, 128, 128, 128])\n",
      "label shape: torch.Size([1, 2, 128, 128, 128])\n",
      "recurrence shape: torch.Size([1, 1, 128, 128, 128])\n",
      "Caso 4: recurrence_np shape: (128, 128, 128), min: -2.65439772605896, max: 12.409589767456055, mean: 2.2140739019960165e-08\n",
      "Caso 4: Vasogénico channel sum (6): 10185, Infiltrado channel sum (2): 23440\n",
      "Caso 4: Etiquetas únicas (0, 2, 6): [0 2 6]\n",
      "Guardada imagen de recurrencia en Dataset/MRIs/contrastive_voxel_wise/combined_results-pipe1/nifti_volumes/case_4_recurrence.nii.gz\n",
      "Saved recurrence NIfTI shape: (128, 128, 128), min: -2.65439772605896, max: 12.409589767456055, mean: 1.906607699150853e-08\n",
      "Recurrence file size: 3580.78 kB\n",
      "Guardadas etiquetas en Dataset/MRIs/contrastive_voxel_wise/combined_results-pipe1/nifti_volumes/labels_case_4.nii.gz\n",
      "Guardado mapa de probabilidad en Dataset/MRIs/contrastive_voxel_wise/combined_results-pipe1/probability_maps/probability_maps_case_4.nii.gz\n",
      "Guardada segmentación en Dataset/MRIs/contrastive_voxel_wise/combined_results-pipe1/nifti_volumes/segmentation_case_4.nii.gz\n",
      "2025-06-20 13:12:07,572 - INFO - Apply pending transforms - lazy: None, pending: 0, upcoming 'Compose', transform.lazy: False\n",
      "After transforms - image shape: torch.Size([11, 128, 128, 128]), label shape: torch.Size([2, 128, 128, 128]), recurrence shape: torch.Size([1, 128, 128, 128])\n",
      "\n",
      "Caso 5:\n",
      "image shape: torch.Size([1, 11, 128, 128, 128])\n",
      "label shape: torch.Size([1, 2, 128, 128, 128])\n",
      "recurrence shape: torch.Size([1, 1, 128, 128, 128])\n",
      "Caso 5: recurrence_np shape: (128, 128, 128), min: -3.037285327911377, max: 11.260549545288086, mean: -7.253402145579457e-08\n",
      "Caso 5: Vasogénico channel sum (6): 2841, Infiltrado channel sum (2): 11988\n",
      "Caso 5: Etiquetas únicas (0, 2, 6): [0 2 6]\n",
      "Guardada imagen de recurrencia en Dataset/MRIs/contrastive_voxel_wise/combined_results-pipe1/nifti_volumes/case_5_recurrence.nii.gz\n",
      "Saved recurrence NIfTI shape: (128, 128, 128), min: -3.037285327911377, max: 11.260549545288086, mean: -7.070550909343831e-08\n",
      "Recurrence file size: 2770.12 kB\n",
      "Guardadas etiquetas en Dataset/MRIs/contrastive_voxel_wise/combined_results-pipe1/nifti_volumes/labels_case_5.nii.gz\n",
      "Guardado mapa de probabilidad en Dataset/MRIs/contrastive_voxel_wise/combined_results-pipe1/probability_maps/probability_maps_case_5.nii.gz\n",
      "Guardada segmentación en Dataset/MRIs/contrastive_voxel_wise/combined_results-pipe1/nifti_volumes/segmentation_case_5.nii.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from monai import transforms\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "import monai.utils\n",
    "from src.get_data import CustomDatasetRec\n",
    "\n",
    "# Dispositivo\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Modelo de proyección (MLP más profundo)\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, input_dim=48, hidden_dim1=256, hidden_dim2=128, output_dim=128, dropout_p=0.3):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_dim2, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# Clasificador supervisado (MLP)\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim=128, hidden_dim1=256, hidden_dim2=128, num_classes=3, dropout_p=0.3):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            # nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_dim2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Función para generar mapas de probabilidad\n",
    "def generate_probability_maps(embeddings, projection_head, classifier, device, batch_size=8192):\n",
    "    projection_head.eval()\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        embeddings = embeddings.squeeze(0).permute(1, 2, 3, 0)  # [128, 128, 128, 48]\n",
    "        embeddings_flat = embeddings.reshape(-1, 48)  # [2097152, 48]\n",
    "        probs_flat = []\n",
    "        \n",
    "        for i in range(0, embeddings_flat.shape[0], batch_size):\n",
    "            batch = embeddings_flat[i:i+batch_size].to(device)\n",
    "            z = projection_head(batch)  # [batch_size, 128]\n",
    "            z = F.normalize(z, dim=1)\n",
    "            logits = classifier(z)  # [batch_size, 3]\n",
    "            probs = F.softmax(logits, dim=1)  # [batch_size, 3]\n",
    "            probs_flat.append(probs.cpu())\n",
    "        \n",
    "        probs_flat = torch.cat(probs_flat, dim=0)  # [2097152, 3]\n",
    "        probs = probs_flat.reshape(128, 128, 128, 3)  # [128, 128, 128, 3]\n",
    "        probs = probs.permute(3, 0, 1, 2)  # [3, 128, 128, 128]\n",
    "    \n",
    "    return probs  # Tensor [3, 128, 128, 128]\n",
    "\n",
    "# Transformaciones de MONAI\n",
    "roi = (128, 128, 128)\n",
    "source_k = \"label\"\n",
    "# Transformaciones combinadas para imágenes de entrada, etiquetas y recurrencia\n",
    "combined_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\", \"label\", \"recurrence\"]),\n",
    "        transforms.EnsureChannelFirstD(keys=\"recurrence\"),\n",
    "        # ConvertToMultiChannelBasedOnAnotatedInfiltration(keys=\"label\"),\n",
    "        ConvertToMultiChannelBasedOnBratsClassesdI(keys=\"label\"),\n",
    "        transforms.CropForegroundd(\n",
    "            keys=[\"image\", \"label\", \"recurrence\"],\n",
    "            source_key=\"label\",\n",
    "            k_divisible=[128, 128, 128],\n",
    "        ),\n",
    "        transforms.RandSpatialCropd(\n",
    "            keys=[\"image\", \"label\", \"recurrence\"],\n",
    "            roi_size=[128, 128, 128],\n",
    "            random_size=False,\n",
    "        ),\n",
    "        transforms.NormalizeIntensityd(keys=[\"image\", \"recurrence\"], nonzero=True, channel_wise=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Dataset y DataLoader\n",
    "dataset_path = './Dataset/Dataset_30_6'\n",
    "train_set = CustomDatasetRec(dataset_path, section=\"test_6\", transform=combined_transform)\n",
    "train_loader = DataLoader(train_set, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "# Directorios\n",
    "output_dir = \"Dataset/MRIs/contrastive_voxel_wise/combined_results-pipe1\"\n",
    "nifti_output_dir = os.path.join(output_dir, \"nifti_volumes\")\n",
    "probs_output_dir = os.path.join(output_dir, \"probability_maps\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(nifti_output_dir, exist_ok=True)\n",
    "os.makedirs(probs_output_dir, exist_ok=True)\n",
    "\n",
    "# Cargar modelos contrastivos preentrenados\n",
    "projection_head1 = ProjectionHead(input_dim=48).to(device)\n",
    "projection_head1.load_state_dict(torch.load(\"trained_models/checkpoints_contrastive/contrastive_projection_head_final_new_pipe1_v01_m1.pth\", map_location=device))\n",
    "# projection_head1.eval()\n",
    "\n",
    "# projection_head2 = ProjectionHead(input_dim=48).to(device)\n",
    "# projection_head2.load_state_dict(torch.load(\"trained_models/checkpoints_contrastive/contrastive_projection_head_final_new_pipe2_m1_1dhzmigz.pth\", map_location=device))\n",
    "# # projection_head2.eval()\n",
    "\n",
    "# Cargar clasificadores preentrenados\n",
    "classifier1 = Classifier(input_dim=128, num_classes=3).to(device)\n",
    "classifier1.load_state_dict(torch.load(\"trained_models/checkpoints/supervised_classifier_final_pipe1_v01_m1.pth\", map_location=device))\n",
    "# classifier1.eval()\n",
    "\n",
    "# classifier2 = Classifier(input_dim=128, num_classes=3).to(device)\n",
    "# classifier2.load_state_dict(torch.load(\"trained_models/checkpoints/supervised_classifier_final_pipe2_m1_1dhzmigz.pth\", map_location=device))\n",
    "# # classifier2.eval()\n",
    "\n",
    "# Hooks para embeddings\n",
    "decoder_features_model1 = None\n",
    "# decoder_features_model2 = None\n",
    "\n",
    "def decoder_hook_fn_model1(module, input, output):\n",
    "    global decoder_features_model1\n",
    "    decoder_features_model1 = output\n",
    "\n",
    "# def decoder_hook_fn_model2(module, input, output):\n",
    "#     global decoder_features_model2\n",
    "#     decoder_features_model2 = output\n",
    "\n",
    "hook_handle_decoder1 = model1.decoder1.conv_block.register_forward_hook(decoder_hook_fn_model1)\n",
    "# hook_handle_decoder2 = model2.decoder1.conv_block.register_forward_hook(decoder_hook_fn_model2)\n",
    "\n",
    "\n",
    "# Procesar y combinar\n",
    "with torch.no_grad():\n",
    "    for idx, batch_data in enumerate(train_loader):\n",
    "        monai.utils.set_determinism(seed=idx)\n",
    "        \n",
    "        print(f\"\\nCaso {idx}:\")\n",
    "        print(f\"image shape: {batch_data['image'].shape}\")\n",
    "        print(f\"label shape: {batch_data['label'].shape}\")\n",
    "        print(f\"recurrence shape: {batch_data['recurrence'].shape}\")\n",
    "        \n",
    "        # Extraer datos\n",
    "        image = batch_data[\"image\"].to(device)  # [1, 11, 128, 128, 128]\n",
    "        label = batch_data[\"label\"].to(device)  # [1, 2, 128, 128, 128]\n",
    "        recurrence = batch_data[\"recurrence\"].to(device)  # [1, 1, 128, 128, 128]\n",
    "\n",
    "        image_meta = batch_data[\"image\"].meta\n",
    "        affine = image_meta.get(\"affine\", np.eye(4)).numpy()\n",
    "                \n",
    "        # Convertir recurrencia a numpy\n",
    "        recurrence_np = recurrence.squeeze().cpu().numpy().astype(np.float32)  # [128, 128, 128]\n",
    "        print(f\"Caso {idx}: recurrence_np shape: {recurrence_np.shape}, min: {recurrence_np.min()}, max: {recurrence_np.max()}, mean: {recurrence_np.mean()}\")\n",
    "        \n",
    "        # Debug: Verificar canales one-hot\n",
    "        print(f\"Caso {idx}: Vasogénico channel sum (6): {label[0, 0].sum().item()}, Infiltrado channel sum (2): {label[0, 1].sum().item()}\")\n",
    "        \n",
    "        # Convertir one-hot a etiquetas únicas (0, 6, 2)\n",
    "        label_class = torch.zeros(label.shape[2:], dtype=torch.long, device=device)  # [128, 128, 128]\n",
    "        label_class[label[0, 0] == 1] = 6  # Vasogénico\n",
    "        label_class[label[0, 1] == 1] = 2  # Infiltrado\n",
    "        labels_np = label_class.cpu().numpy().astype(np.int16)  # [128, 128, 128]\n",
    "        \n",
    "        # Mapear etiquetas a 0, 1, 2 para métricas\n",
    "        labels_metrics = np.zeros_like(labels_np, dtype=np.int16)\n",
    "        labels_metrics[labels_np == 6] = 1  # Vasogénico -> 1\n",
    "        labels_metrics[labels_np == 2] = 2  # Infiltrado -> 2\n",
    "        \n",
    "        # Verificar etiquetas\n",
    "        unique_labels = np.unique(labels_np)\n",
    "        print(f\"Caso {idx}: Etiquetas únicas (0, 2, 6): {unique_labels}\")\n",
    "        if not np.all(np.isin(unique_labels, [0, 2, 6])):\n",
    "            print(f\"Advertencia: Etiquetas inválidas en caso {idx}: {unique_labels}\")\n",
    "        if 6 not in unique_labels:\n",
    "            print(f\"Advertencia: Clase vasogénico (6) no presente en caso {idx}\")\n",
    "        \n",
    "        # Obtener embeddings\n",
    "        _ = model1(image)\n",
    "        # _ = model2(image)\n",
    "        embeddings1 = decoder_features_model1\n",
    "        # embeddings2 = decoder_features_model2\n",
    "        \n",
    "        # Generar mapas de probabilidad\n",
    "        prob_maps1 = generate_probability_maps(embeddings1, projection_head1, classifier1, device)  # [3, 128, 128, 128]\n",
    "        # prob_maps2 = generate_probability_maps(embeddings2, projection_head2, classifier2, device)  # [3, 128, 128, 128]\n",
    "        \n",
    "        # # Combinar mapas\n",
    "        # combined_prob_maps = torch.zeros_like(prob_maps1)  # [3, 128, 128, 128]\n",
    "        # combined_prob_maps[0] = prob_maps1[0]  # Fondo (0)\n",
    "        # combined_prob_maps[1] = torch.max(prob_maps1[1], prob_maps2[1])  # Vasogénico (1)\n",
    "        # combined_prob_maps[2] = prob_maps2[2]  # Infiltrado (2)\n",
    "        \n",
    "        # Normalizar probabilidades\n",
    "        combined_prob_maps = prob_maps1 / (prob_maps1.sum(dim=0, keepdim=True) + 1e-6)\n",
    "        \n",
    "        # Convertir a numpy\n",
    "        prob_maps_np = combined_prob_maps.cpu().numpy()  # [3, 128, 128, 128]\n",
    "        prob_maps_np_nifti = np.transpose(prob_maps_np, (1, 2, 3, 0))  # [128, 128, 128, 3]\n",
    "        \n",
    "        # Generar segmentación semántica (clases 0, 1, 2)\n",
    "        segmentation = np.argmax(prob_maps_np, axis=0)  # [128, 128, 128]\n",
    "        segmentation_np = segmentation.astype(np.int16)  # [0, 1, 2]\n",
    "        \n",
    "        # Mapear segmentación a valores originales (0, 6, 2)\n",
    "        segmentation_np_orig = np.zeros_like(segmentation_np, dtype=np.int16)\n",
    "        segmentation_np_orig[segmentation_np == 1] = 6  # Vasogénico\n",
    "        segmentation_np_orig[segmentation_np == 2] = 2  # Infiltrado\n",
    "        \n",
    "        # Guardar imágenes MRI en NIfTI\n",
    "        image_np = image.squeeze(0).cpu().numpy()  # [11, 128, 128, 128]\n",
    "        for channel in range(image_np.shape[0]):\n",
    "            nifti_img = nib.Nifti1Image(image_np[channel], affine, header=nib.Nifti1Header())\n",
    "            nib.save(nifti_img, os.path.join(nifti_output_dir, f\"case_{idx}_modality_{channel}.nii.gz\"))\n",
    "        \n",
    "        # Guardar imagen de recurrencia en NIfTI\n",
    "        nifti_recurrence_img = nib.Nifti1Image(recurrence_np, affine, header=nib.Nifti1Header())\n",
    "        recurrence_output_path = os.path.join(nifti_output_dir, f\"case_{idx}_recurrence.nii.gz\")\n",
    "        nib.save(nifti_recurrence_img, recurrence_output_path)\n",
    "        print(f\"Guardada imagen de recurrencia en {recurrence_output_path}\")\n",
    "        saved_nifti = nib.load(recurrence_output_path)\n",
    "        saved_data = saved_nifti.get_fdata()\n",
    "        print(f\"Saved recurrence NIfTI shape: {saved_data.shape}, min: {saved_data.min()}, max: {saved_data.max()}, mean: {saved_data.mean()}\")\n",
    "        file_size = os.path.getsize(recurrence_output_path) / 1024  # Size in kB\n",
    "        print(f\"Recurrence file size: {file_size:.2f} kB\")\n",
    "        \n",
    "        # Guardar etiquetas en NIfTI (valores 0, 2, 6)\n",
    "        nifti_label_img = nib.Nifti1Image(labels_np, affine, header=nib.Nifti1Header())\n",
    "        label_output_path = os.path.join(nifti_output_dir, f\"labels_case_{idx}.nii.gz\")\n",
    "        nib.save(nifti_label_img, label_output_path)\n",
    "        print(f\"Guardadas etiquetas en {label_output_path}\")\n",
    "        \n",
    "        # Guardar mapas de probabilidad\n",
    "        nifti_prob_img = nib.Nifti1Image(prob_maps_np_nifti, affine, header=nib.Nifti1Header())\n",
    "        prob_output_path = os.path.join(probs_output_dir, f\"probability_maps_case_{idx}.nii.gz\")\n",
    "        nib.save(nifti_prob_img, prob_output_path)\n",
    "        print(f\"Guardado mapa de probabilidad en {prob_output_path}\")\n",
    "        \n",
    "        # Guardar segmentación (valores 0, 6, 2)\n",
    "        nifti_seg_img = nib.Nifti1Image(segmentation_np_orig, affine, header=nib.Nifti1Header())\n",
    "        seg_output_path = os.path.join(nifti_output_dir, f\"segmentation_case_{idx}.nii.gz\")\n",
    "        nib.save(nifti_seg_img, seg_output_path)\n",
    "        print(f\"Guardada segmentación en {seg_output_path}\")\n",
    "        \n",
    "        # Restaurar aleatoriedad después de cada caso\n",
    "        monai.utils.set_determinism(seed=None)\n",
    "\n",
    "# Remover hooks\n",
    "hook_handle_decoder1.remove()\n",
    "# hook_handle_decoder2.remove()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
